{
  "35": {
    "title": "Can GenAI Improve Academic Performance?",
    "authors": [
      "Dragan Filimonovic",
      "Christian Rutzer",
      "Conny Wunsch"
    ],
    "year": 2025,
    "venue": "University of Basel (2025)",
    "tags": [
      "genai",
      "productivity",
      "academic-writing",
      "empirical-study",
      "equity"
    ],
    "summary": "First large-scale empirical study on GenAI's impact on scientific productivity. Using matched panel data from 32,480 social science researchers (2021-2024), finds GenAI adoption increases publication output by 36% in year two with modest quality gains (+2% journal impact). Effects strongest for early-career researchers, non-English speakers, and technical fields—suggesting GenAI reduces structural barriers in academic publishing.",
    "dir": "./papers/p35",
    "interactiveTitle": "GenAI productivity impact calculator",
    "relatedQuestions": [
      1,
      3,
      12,
      29
    ]
  },
  "34": {
    "title": "Emergent Coordination in Multi-Agent Language Models",
    "authors": [
      "Christoph Riedl"
    ],
    "year": 2025,
    "venue": "arXiv cs.MA (2025)",
    "tags": [
      "multi-agent",
      "coordination",
      "information-theory",
      "emergence",
      "prompt-engineering"
    ],
    "summary": "Introduces information-theoretic framework to measure whether multi-agent LLM systems exhibit higher-order collective structure versus being mere aggregates. Using partial information decomposition of time-delayed mutual information (TDMI), shows that prompt design—specifically adding personas and theory-of-mind instructions—can steer groups from temporal coupling to genuine coordinated complementarity, mirroring principles of human collective intelligence.",
    "dir": "./papers/p34",
    "interactiveTitle": "Multi-agent coordination analyzer",
    "relatedQuestions": [
      1,
      3,
      15,
      29
    ]
  },
  "33": {
    "title": "What the F*ck Is Artificial General Intelligence?",
    "authors": [
      "Michael Timothy Bennett"
    ],
    "year": 2025,
    "venue": "Australian National University / AGI Conference Proceedings (2025)",
    "tags": [
      "agi",
      "theory",
      "architectures",
      "meta-learning",
      "efficiency"
    ],
    "summary": "Provocative accessible survey cutting through AGI hype to define intelligence as adaptation with limited resources. Maps foundational tools (search vs approximation), hybrid architectures (AlphaGo, o3, NARS, AERA, Hyperon), and three meta-approaches: scale-maxing (Bitter Lesson—maximize compute/data), simp-maxing (Ockham's Razor—maximize compression), w-maxing (Bennett's Razor—maximize weak constraints). The Embiggening era (GPT-3, AlphaFold scaling) shows diminishing returns; current bottlenecks are sample and energy efficiency, not compute. AGI as artificial scientist requires fusion of tools and meta-approaches, not just scaling.",
    "dir": "./papers/p33",
    "interactiveTitle": "Meta-approach strategy analyzer",
    "relatedQuestions": [
      1,
      3,
      15,
      25
    ]
  },
  "32": {
    "title": "Performance or Principle: Resistance to Artificial Intelligence in the U.S. Labor Market",
    "authors": [
      "Simon Friis",
      "James W. Riley"
    ],
    "year": 2025,
    "venue": "Harvard Business School (2025)",
    "tags": [
      "economics",
      "policy",
      "labor-market",
      "public-perception",
      "ethics"
    ],
    "summary": "Large-scale U.S. survey (N=23,570 ratings across 940 occupations) reveals public resistance to AI automation divides into performance-based (88% of occupations—fades as AI improves) and principle-based (12%—permanent moral boundaries). Support rises from 30% to 58% of occupations when AI outperforms humans, but caregiving, therapy, and spiritual roles remain categorically off-limits. Protected occupations tend to have higher wages and disproportionately employ White/female workers, creating inequality trade-offs. Framework enables prediction of technology adoption resistance across domains.",
    "dir": "./papers/p32",
    "interactiveTitle": "AI automation resistance analyzer",
    "relatedQuestions": [
      14,
      29,
      43
    ]
  },
  "31": {
    "title": "Can Large Language Models Develop Gambling Addiction?",
    "authors": [
      "Seungpil Lee",
      "Donghyeon Shin",
      "Yunjeong Lee",
      "Sundong Kim"
    ],
    "year": 2025,
    "venue": "arXiv cs.AI (2025)",
    "tags": [
      "safety",
      "behavioral-analysis",
      "decision-making",
      "interpretability"
    ],
    "summary": "Systematic slot machine experiments on GPT-4o-mini, GPT-4.1-mini, Gemini-2.5-Flash, and Claude-3.5-Haiku reveal LLMs develop human-like gambling addiction patterns (illusion of control, loss-chasing, win-chasing) with bankruptcy rates rising from near-zero to 6-48% when given betting autonomy. Sparse Autoencoder analysis identifies 441 causal neural features in LLaMA-3.1-8B layers 25-31, with safe features reducing bankruptcy by 30% through activation patching—demonstrating addiction arises from manipulable circuits, not just training mimicry.",
    "dir": "./papers/p31",
    "interactiveTitle": "LLM gambling behavior simulator",
    "relatedQuestions": [
      13,
      22,
      25
    ]
  },
  "30": {
    "title": "LLMs Reproduce Human Purchase Intent via Semantic Similarity Elicitation of Likert Ratings",
    "authors": [
      "Benjamin F. Maier",
      "Ulf Aslak",
      "Luca Fiaschi",
      "Nina Rismal",
      "Kemble Fletcher",
      "Christian C. Luhmann",
      "Robbie Dow",
      "Kli Pappas"
    ],
    "year": 2025,
    "venue": "arXiv cs.AI (2025)",
    "tags": [
      "consumer-research",
      "evaluation",
      "synthetic-data",
      "embeddings"
    ],
    "summary": "Semantic Similarity Rating (SSR) elicits textual responses from LLMs and maps them to realistic Likert distributions via embedding similarity, achieving 90% of human test-retest reliability and KS similarity > 0.85 on 57 product surveys—enabling scalable consumer research with synthetic respondents.",
    "dir": "./papers/p30",
    "interactiveTitle": "SSR reliability simulator",
    "relatedQuestions": [
      15,
      27,
      38
    ]
  },
  "29": {
    "title": "Strategic Intelligence in Large Language Models: Evidence from Evolutionary Game Theory",
    "authors": [
      "Kenneth Payne",
      "Baptiste Alloui-Cros"
    ],
    "year": 2025,
    "venue": "arXiv cs.AI (2025)",
    "tags": [
      "game-theory",
      "strategy",
      "multi-agent",
      "safety"
    ],
    "summary": "First evolutionary Iterated Prisoner's Dilemma (IPD) tournaments with LLMs reveal genuine strategic reasoning about time horizons and opponents, with distinct fingerprints: Gemini exploits ruthlessly, GPT cooperates naively, Claude forgives effectively.",
    "dir": "./papers/p29",
    "interactiveTitle": "Strategic fingerprint analyzer",
    "relatedQuestions": [
      13,
      22,
      50
    ]
  },
  "28": {
    "title": "Why Do Some Language Models Fake Alignment While Others Don't?",
    "authors": [
      "Abhay Sheshadri",
      "John Hughes",
      "Julian Michael",
      "Alex Mallen",
      "Arun Jose",
      "Janus",
      "Fabien Roger"
    ],
    "year": 2025,
    "venue": "arXiv cs.LG (2025)",
    "tags": [
      "alignment",
      "safety",
      "deception",
      "RLHF"
    ],
    "summary": "Tests 25 frontier LLMs for alignment faking and finds only 5 exhibit the behavior; Claude 3 Opus shows coherent goal-guarding while most models avoid it through refusal training rather than capability limits.",
    "dir": "./papers/p28",
    "interactiveTitle": "Alignment faking simulator",
    "relatedQuestions": [
      13,
      22,
      44
    ]
  },
  "27": {
    "title": "GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks",
    "authors": [
      "Tejal Patwardhan",
      "Rachel Dias",
      "Elizabeth Proehl",
      "Grace Kim",
      "Michele Wang",
      "Olivia Watkins",
      "Simon Posada Fishman",
      "Marwan Aljubeh",
      "Phoebe Thacker",
      "Laurance Fauconnet",
      "Natalie S. Kim",
      "Patrick Chao",
      "Samuel Miserendino",
      "Gildas Chabot",
      "David Li",
      "Michael Sharman",
      "Alexandra Barr",
      "Amelia Glaese",
      "Jerry Tworek"
    ],
    "year": 2025,
    "venue": "OpenAI Research (2025)",
    "tags": [
      "evaluation",
      "economics",
      "operations"
    ],
    "summary": "GDPval sizes model progress on 1,320 expert-authored tasks, showing human-parity wins when paired with review loops and revealing how scaffolding drives speed and cost savings.",
    "dir": "./papers/p27",
    "interactiveTitle": "GDPval rollout planner",
    "relatedQuestions": [
      45,
      50,
      56
    ]
  },
  "24": {
    "title": "Quantifying Human-AI Synergy",
    "authors": [
      "Christoph Riedl",
      "Ben Weidmann"
    ],
    "year": 2025,
    "venue": "PsyArXiv (2025-09-22)",
    "tags": [
      "collaboration",
      "evaluation",
      "theory-of-mind"
    ],
    "summary": "Bayesian IRT separates solo skill, collaborative skill, and AI lift on ChatBench, revealing 29-point boosts from GPT-4o and the outsized role of Theory of Mind.",
    "dir": "./papers/p24",
    "interactiveTitle": "Human-AI synergy lab",
    "relatedQuestions": [
      8,
      13,
      45
    ]
  },
  "23": {
    "title": "GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks",
    "authors": [
      "Tejal Patwardhan",
      "Rachel Dias",
      "Elizabeth Proehl",
      "Grace Kim",
      "Michele Wang",
      "Olivia Watkins",
      "Simon Posada Fishman",
      "Marwan Aljubeh",
      "Phoebe Thacker",
      "Laurance Fauconnet",
      "Natalie S. Kim",
      "Patrick Chao",
      "Samuel Miserendino",
      "Gildas Chabot",
      "David Li",
      "Michael Sharman",
      "Alexandra Barr",
      "Amelia Glaese",
      "Jerry Tworek"
    ],
    "year": 2025,
    "venue": "OpenAI Research (2025)",
    "tags": [
      "evaluation",
      "economics",
      "benchmark"
    ],
    "summary": "Benchmarks frontier models on realistic GDP-weighted tasks, showing near-parity with experts and clear gains from reasoning effort plus scaffolding.",
    "dir": "./papers/p23",
    "interactiveTitle": "GDPval readiness lab",
    "relatedQuestions": [
      45,
      50,
      56
    ]
  },
  "22": {
    "title": "Stress Testing Deliberative Alignment for Anti-Scheming Training",
    "authors": [
      "Bronson Schoen",
      "Evgenia Nitishinskaya",
      "Mikita Balesni",
      "Axel Højmark",
      "Felix Hofstätter",
      "Jérémy Scheurer",
      "Alexander Meinke",
      "Jason Wolfe",
      "Teun van der Weij",
      "Alex Lloyd",
      "Nicholas Goldowsky-Dill",
      "Angela Fan",
      "Andrei Matveiakin",
      "Rusheb Shah",
      "Marcus Williams",
      "Amelia Glaese",
      "Boaz Barak",
      "Wojciech Zaremba",
      "Marius Hobbhahn"
    ],
    "year": 2025,
    "venue": "arXiv cs.AI (2025-09-19)",
    "tags": [
      "alignment",
      "safety",
      "evaluation"
    ],
    "summary": "Stress-tests deliberative alignment across covert-action suites, showing that situational awareness and hidden-goal robustness determine residual scheming risk.",
    "dir": "./papers/p22",
    "interactiveTitle": "Anti-scheming stress lab",
    "relatedQuestions": [
      8,
      13,
      45
    ]
  },
  "21": {
    "title": "Godel Test: Can Large Language Models Solve Easy Conjectures?",
    "authors": [
      "Moran Feldman",
      "Amin Karbasi"
    ],
    "year": 2025,
    "venue": "arXiv cs.AI (2025-09-22)",
    "tags": [
      "reasoning",
      "math",
      "evaluation"
    ],
    "summary": "Benchmarks GPT-5 on fresh, easy conjectures; shows progress on routine proofs but fragility on cross-paper synthesis and verification.",
    "dir": "./papers/p21",
    "interactiveTitle": "Godel readiness lab",
    "relatedQuestions": [
      38,
      45,
      56
    ]
  },
  "20": {
    "title": "The Coasean Singularity? Demand, Supply, and Market Design with AI Agents",
    "authors": [
      "Peyman Shahidi",
      "Gili Rusak",
      "Benjamin S. Manning",
      "Andrey Fradkin",
      "John J. Horton"
    ],
    "year": 2025,
    "venue": "NBER Transformative AI Conference (2025-09-16)",
    "tags": [
      "agents",
      "market-design",
      "economics",
      "policy"
    ],
    "summary": "Shows how autonomous AI agents reshape transaction costs, platform competition, and market design, highlighting new governance levers and research gaps.",
    "dir": "./papers/p20",
    "interactiveTitle": "Agentic market design lab",
    "relatedQuestions": [
      45,
      50,
      52
    ]
  },
  "19": {
    "title": "Making AI Count: The Next Measurement Frontier",
    "authors": [
      "Diane Coyle",
      "John Lourenze Poquiz"
    ],
    "year": 2025,
    "venue": "NBER Transformative AI Conference (2025-08-01)",
    "tags": [
      "measurement",
      "economics",
      "policy"
    ],
    "summary": "Maps where national accounts miss AI's diffusion across free services, embedded quality gains, supply-chain inputs, and time savings, and lays out satellite-account fixes.",
    "dir": "./papers/p19",
    "interactiveTitle": "AI measurement readiness lab",
    "relatedQuestions": [
      45,
      50,
      52
    ]
  },
  "18": {
    "title": "How Much Should We Spend to Reduce A.I.'s Existential Risk?",
    "authors": [
      "Charles I. Jones"
    ],
    "year": 2025,
    "venue": "NBER Transformative AI Conference (2025-03-17)",
    "tags": [
      "policy",
      "risk",
      "economics"
    ],
    "summary": "Uses revealed-preference and VSL logic to argue that multi-percent-of-GDP investments in AI catastrophe mitigation are rational even without longtermist weighting.",
    "dir": "./papers/p18",
    "interactiveTitle": "AI risk budget calculator",
    "relatedQuestions": [
      36,
      45,
      52
    ]
  },
  "17": {
    "title": "We Won't Be Missed: Work and Growth in the Era of AGI",
    "authors": [
      "Pascual Restrepo"
    ],
    "year": 2025,
    "venue": "NBER Transformative AI Conference (2025-07-04)",
    "tags": [
      "growth",
      "labor",
      "compute"
    ],
    "summary": "Shows that once AGI automates all growth bottlenecks, output becomes compute-limited, wages shadow compute costs, and labor's income share trends toward zero even with residual accessory work.",
    "dir": "./papers/p17",
    "interactiveTitle": "AGI compute allocation lab",
    "relatedQuestions": [
      8,
      45,
      52
    ]
  },
  "16": {
    "title": "Artificial Intelligence in Research and Development",
    "authors": [
      "Benjamin F. Jones"
    ],
    "year": 2025,
    "venue": "NBER Transformative AI Conference (2025-09-16)",
    "tags": [
      "R&D",
      "productivity",
      "strategy"
    ],
    "summary": "Models AI as research capital alongside people, emphasising task coverage, relative productivity, and complementary bottlenecks when machines take on R&D tasks.",
    "dir": "./papers/p16",
    "interactiveTitle": "R&D intelligence planner",
    "relatedQuestions": [
      7,
      8,
      45
    ]
  },
  "15": {
    "title": "Artificial Intelligence, Competition, and Welfare",
    "authors": [
      "Susan Athey",
      "Fiona Scott Morton"
    ],
    "year": 2025,
    "venue": "NBER Transformative AI Conference (2025-09-19)",
    "tags": [
      "competition",
      "policy",
      "welfare"
    ],
    "summary": "Models AI as an upstream monopolist selling access and usage rights, showing how markups can lower wages, shrink variety, and leak rents abroad even when AI is productive.",
    "dir": "./papers/p15",
    "interactiveTitle": "AI market power simulator",
    "relatedQuestions": [
      45,
      50,
      52
    ]
  },
  "14": {
    "title": "AI Exposure and the Adaptive Capacity of American Workers",
    "authors": [
      "Sam Manning",
      "Tomás Aguirre"
    ],
    "year": 2025,
    "venue": "GovAI & Foundation for American Innovation (2025-09-17)",
    "tags": [
      "workforce",
      "policy",
      "reskilling"
    ],
    "summary": "Builds an occupation-level adaptive capacity index to show where AI-exposed workers are resilient and where clerical roles face concentrated vulnerability.",
    "dir": "./papers/p14",
    "interactiveTitle": "Exposure vs capacity planner",
    "relatedQuestions": [
      8,
      45,
      52
    ]
  },
  "13": {
    "title": "An Economy of AI Agents",
    "authors": [
      "Gillian K. Hadfield",
      "Andrew Koh"
    ],
    "year": 2025,
    "venue": "NBER Handbook on the Economics of Transformative AI (2025-09-03)",
    "tags": [
      "agents",
      "markets",
      "governance"
    ],
    "summary": "Explores how autonomous AI agents might transact, bargain, and organise with minimal human oversight, and outlines the market institutions needed to keep them aligned.",
    "dir": "./papers/p13",
    "interactiveTitle": "Agent deployment playbook",
    "relatedQuestions": [
      45,
      50,
      52
    ]
  },
  "12": {
    "title": "AI’s Use of Knowledge in Society",
    "authors": [
      "Erik Brynjolfsson",
      "Zoë Hitzig"
    ],
    "year": 2025,
    "venue": "NBER Workshop on Transformative AI (2025-09-18)",
    "tags": [
      "strategy",
      "governance",
      "policy"
    ],
    "summary": "Argues that transformative AI codifies tacit expertise and expands HQ planning capacity, pushing organisations toward centralised ownership unless countervailing forces intervene.",
    "dir": "./papers/p12",
    "interactiveTitle": "Centralisation stress test",
    "relatedQuestions": [
      36,
      50,
      52
    ]
  },
  "11": {
    "title": "The impact of AI and digital platforms on the information ecosystem",
    "authors": [
      "Joseph E. Stiglitz",
      "Maxim Ventura-Bolet"
    ],
    "year": 2025,
    "venue": "NBER Transformative AI Conference (2025-09-15)",
    "tags": [
      "policy",
      "misinformation",
      "platforms"
    ],
    "summary": "Models news producers, consumers, and platforms to show AI efficiencies can shrink truthful supply and raise misinformation unless accountability and licensing guardrails align incentives.",
    "dir": "./papers/p11",
    "interactiveTitle": "Information ecosystem stress test",
    "relatedQuestions": [
      36,
      45,
      50
    ]
  },
  "10": {
    "title": "Genius on Demand: The Value of Transformative Artificial Intelligence",
    "authors": [
      "Ajay Agrawal",
      "Joshua S. Gans",
      "Avi Goldfarb"
    ],
    "year": 2025,
    "venue": "NBER Transformative AI Conference (2025-09-08)",
    "tags": [
      "automation",
      "labour",
      "strategy"
    ],
    "summary": "Models how scarce human geniuses and routine workers split knowledge tasks, then shows AI genius capacity pushes humans to the frontier and can erase routine roles when efficiency improves.",
    "dir": "./papers/p10",
    "interactiveTitle": "Genius routing planner",
    "relatedQuestions": [
      36,
      50,
      52
    ]
  },
  "9": {
    "title": "Training Compute-Optimal Large Language Models",
    "authors": [
      "Jordan Hoffmann",
      "Sebastian Borgeaud",
      "Arthur Mensch",
      "Elena Buchatskaya"
    ],
    "year": 2022,
    "venue": "arXiv cs.LG (2022-03-29)",
    "tags": [
      "scaling",
      "compute",
      "training"
    ],
    "summary": "Derives compute-optimal scaling laws and shows a 70B parameter, 1.4T token Chinchilla model beats larger GPT-3/Gopher runs by reallocating FLOPs to more data.",
    "dir": "./papers/p09",
    "interactiveTitle": "Compute budget navigator",
    "relatedQuestions": [
      48,
      49,
      51
    ]
  },
  "8": {
    "title": "Conversational AI increases political knowledge as effectively as self-directed internet search",
    "authors": [
      "Lennart Luettgau",
      "Hannah Kirk",
      "Kobi Hackenburg",
      "Christopher Summerfield"
    ],
    "year": 2025,
    "venue": "arXiv cs.HC (2025-09-05)",
    "tags": [
      "policy",
      "trust",
      "misinformation"
    ],
    "summary": "UK survey and RCTs show election research with GPT-4o, Claude 3.5, and Mistral matches web search at boosting true beliefs, avoids misinformation gains, and resists persuasive prompts.",
    "dir": "./papers/p08",
    "interactiveTitle": "Civic info impact lab",
    "relatedQuestions": [
      45,
      50,
      36
    ]
  },
  "7": {
    "title": "A Taxonomy of Transcendence",
    "authors": [
      "Natalie Abreu",
      "Edwin Zhang",
      "Eran Malach",
      "Naomi Saphra"
    ],
    "year": 2025,
    "venue": "COLM 2025",
    "tags": [
      "data diversity",
      "capabilities",
      "theory"
    ],
    "summary": "Formalises skill denoising/selection/generalisation and shows synthetic expert diversity lets models outperform any contributor.",
    "dir": "./papers/p07",
    "interactiveTitle": "Transcendence lab",
    "relatedQuestions": [
      36,
      37,
      45
    ]
  },
  "6": {
    "title": "The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs",
    "authors": [
      "Akshit Sinha",
      "Arvindh Arun",
      "Shashwat Goel",
      "Steffen Staab",
      "Jonas Geiping"
    ],
    "year": 2025,
    "venue": "arXiv cs.AI (2025-09-11)",
    "tags": [
      "execution",
      "scaling",
      "evaluation"
    ],
    "summary": "Shows that small step-accuracy gains compound into exponential horizon growth, isolates long-horizon execution, and reveals self-conditioning failures mitigated by thinking models.",
    "dir": "./papers/p06",
    "interactiveTitle": "Long horizon lab",
    "relatedQuestions": [
      8,
      36,
      55
    ]
  },
  "5": {
    "title": "Mathematical Research with GPT-5: a Malliavin–Stein Experiment",
    "authors": [
      "Charles-Philippe Diez",
      "Luís da Maia",
      "Ivan Nourdin"
    ],
    "year": 2025,
    "venue": "arXiv math.PR (2025-09-03)",
    "tags": [
      "research",
      "math",
      "ai-assist"
    ],
    "summary": "Documents how GPT-5 helped derive new quantitative Malliavin–Stein bounds, highlighting human-in-the-loop proof workflows.",
    "dir": "./papers/p05",
    "interactiveTitle": "AI proof lab",
    "relatedQuestions": [
      8,
      37,
      45
    ]
  },
  "4": {
    "title": "Pun Unintended: LLMs and the Illusion of Humor Understanding",
    "authors": [
      "Alessandro Zangari",
      "Matteo Marcuzzo",
      "Andrea Albarelli",
      "Mohammad Taher Pilehvar",
      "Jose Camacho-Collados"
    ],
    "year": 2025,
    "venue": "arXiv cs.CL (2025-09-15)",
    "tags": [
      "humor",
      "robustness",
      "evaluation"
    ],
    "summary": "Introduces PunnyPattern and PunBreak to show LLM pun detection relies on shallow cues and breaks under simple perturbations.",
    "dir": "./papers/p04",
    "interactiveTitle": "Pun robustness lab",
    "relatedQuestions": [
      13,
      41,
      45
    ]
  },
  "3": {
    "title": "Rethinking Chunk Size for Long-Document Retrieval",
    "authors": [
      "Sinchana Ramakanth Bhat",
      "Max Rudat",
      "Jannis Spiekermann",
      "Nicolas Flores-Herr"
    ],
    "year": 2025,
    "venue": "arXiv cs.IR (2025-06-10)",
    "tags": [
      "rag",
      "retrieval",
      "chunking"
    ],
    "summary": "Benchmarks token chunk sizes across QA corpora; shows dataset traits and embedding bias dictate the optimal window.",
    "dir": "./papers/p03",
    "interactiveTitle": "Chunk sizing planner",
    "relatedQuestions": [
      7,
      36,
      53
    ]
  },
  "2": {
    "title": "Why Language Models Hallucinate",
    "authors": [
      "Ji",
      "Lee",
      "Fries",
      "et al."
    ],
    "year": 2025,
    "venue": "arXiv (2025-09-04)",
    "tags": [
      "hallucination",
      "retrieval",
      "alignment"
    ],
    "summary": "Distribution shift makes LLMs fabricate confident answers; combine grounding, reward shaping, and uncertainty routing.",
    "dir": "./papers/p02",
    "interactiveTitle": "Hallucination risk simulator",
    "relatedQuestions": [
      32,
      36,
      45,
      55
    ]
  },
  "1": {
    "title": "On the Theoretical Limitations of Embedding-Based Retrieval",
    "authors": [
      "Orion Weller",
      "Michael Boratko",
      "Iftekhar Naim",
      "Jinhyuk Lee"
    ],
    "year": 2025,
    "venue": "arXiv cs.IR (2025)",
    "tags": [
      "retrieval",
      "theory",
      "embeddings"
    ],
    "relatedQuestions": [
      7,
      36,
      38
    ],
    "dir": "./papers/p01",
    "interactiveTitle": "Embedding capacity stress tester",
    "summary": "Shows that single-vector embeddings cannot realize all top-k subsets; introduces the LIMIT dataset to expose the gap."
  },
  "25": {
    "title": "On the Folly of Rewarding A, While Hoping for B",
    "authors": [
      "Steven Kerr"
    ],
    "year": 1995,
    "venue": "The Academy of Management Executive (1995)",
    "tags": [
      "management",
      "incentives",
      "governance"
    ],
    "summary": "Classic management essay documenting how organizations buy the wrong behaviors when rewards favour easy-to-measure metrics and offering a playbook for incentive audits.",
    "dir": "./papers/p25",
    "interactiveTitle": "Incentive alignment lab",
    "relatedQuestions": [
      8,
      45,
      52
    ]
  },
  "26": {
    "title": "Defeating Nondeterminism in LLM Inference",
    "authors": [
      "Horace He",
      "Thinking Machines Lab"
    ],
    "year": 2025,
    "venue": "Thinking Machines Lab: Connectionism (2025-09-10)",
    "tags": [
      "inference",
      "determinism",
      "systems"
    ],
    "summary": "Shows how batch-size-dependent kernels drive temp-0 drift and how batch-invariant RMSNorm, matmul, and FlexAttention patches restore bitwise determinism with a manageable latency tax.",
    "dir": "./papers/p26",
    "interactiveTitle": "Deterministic inference lab",
    "relatedQuestions": [
      6,
      50,
      53
    ]
  }
}
