{
  "62": {
    "title": "LLM Harms: A Taxonomy and Discussion",
    "authors": [
      "Kevin Chen",
      "Saleh Afroogh",
      "Abhejay Murali",
      "David Atkinson",
      "Amit Dhurandhar",
      "Junfeng Jiao"
    ],
    "year": 2025,
    "venue": "arXiv:2512.05929 (2025)",
    "tags": [
      "safety",
      "risk-taxonomy",
      "governance",
      "auditing",
      "misuse",
      "hallucinations",
      "bias",
      "privacy"
    ],
    "summary": "Lifecycle-aware taxonomy of LLM harms spanning pre-deployment (data, labor, environment), direct outputs (bias, toxicity, hallucination), misuse (jailbreaks, fraud, prompt injection), systemic impacts (elections, inequality), and downstream high-stakes applications. Argues for layered mitigations plus dynamic auditing (continuous monitoring) rather than one-time safety checks.",
    "dir": "./papers/p62",
    "interactiveTitle": "Harm taxonomy navigator",
    "relatedQuestions": [
      50,
      45,
      8
    ]
  },
  "61": {
    "title": "Barriers to AI Adoption: Image Concerns at Work",
    "authors": [
      "David Almog"
    ],
    "year": 2025,
    "venue": "Kellogg School of Management (2025)",
    "tags": [
      "labor-economics",
      "ai-adoption",
      "social-signaling",
      "workplace-behavior",
      "field-experiment",
      "human-ai-collaboration"
    ],
    "summary": "Field experiment with 450 remote workers reveals that when AI usage is visible to evaluators, workers reduce reliance on AI recommendations by 14%, causing a 3.4% drop in performance. Workers fear that heavy AI reliance signals a lack of confidence in their own judgment, a concern that persists even when evaluators are explicitly instructed to prioritize accuracy.",
    "dir": "./papers/p61",
    "interactiveTitle": "Workplace image concern simulator",
    "relatedQuestions": [
      50,
      52,
      45
    ]
  },
  "60": {
    "title": "Solving a Million-Step LLM Task with Zero Errors",
    "authors": [
      "Elliot Meyerson",
      "et al."
    ],
    "year": 2025,
    "venue": "arXiv:2511.09030 (2025)",
    "tags": [
      "agentic-ai",
      "error-correction",
      "multi-agent",
      "scalability",
      "towers-of-hanoi"
    ],
    "summary": "MAKER solves a million-step logic puzzle with zero errors by breaking tasks into tiny steps and using a voting system. It proves that reliable long-horizon execution is possible using cheaper, smaller models like gpt-4.1-mini if the process is managed correctly.",
    "dir": "./papers/p60",
    "interactiveTitle": "MAKER reliability & cost simulator",
    "relatedQuestions": [
      50,
      56,
      20
    ]
  },
  "59": {
    "title": "AI Agents, Productivity, and Higher-Order Thinking: Early Evidence From Software Development",
    "authors": [
      "Suproteem K. Sarkar"
    ],
    "year": 2025,
    "venue": "SSRN (2025)",
    "tags": [
      "agentic-ai",
      "productivity",
      "software-engineering",
      "labor-economics",
      "difference-in-differences",
      "human-factors",
      "higher-order-thinking",
      "organizational-behavior"
    ],
    "summary": "Natural-experiment evidence from Cursor's coding agent shows weekly merges rose 39% and lines edited 49% after agent defaults, with no short-run quality regressions. Experience drives delegation skill: each additional standard deviation of tenure (~6.6 years) raises agent accept rates by about 6%, while plan-first intents and semantic guidance become hallmarks of expert usage.",
    "dir": "./papers/p59",
    "interactiveTitle": "Agent productivity impact explorer",
    "relatedQuestions": [50, 52, 56]
  },
  "58": {
    "title": "Fortytwo: Swarm Inference with Peer-Ranked Consensus",
    "authors": [
      "Vladyslav Larin",
      "Ihor Naumenko",
      "Aleksei Ivashov",
      "Ivan Nikitin",
      "Alexander Firsov"
    ],
    "year": 2025,
    "venue": "arXiv cs.LG (2025)",
    "tags": [
      "ensemble-methods",
      "swarm-intelligence",
      "consensus-algorithms",
      "Bradley-Terry",
      "distributed-ai",
      "robustness",
      "adversarial-defense",
      "reputation-systems"
    ],
    "summary": "Swarm inference uses peer-ranked consensus across heterogeneous models to achieve 85.90% on GPQA Diamond versus 68.69% for majority voting (+17.21pp, +25.1% relative). Bradley-Terry pairwise ranking with reputation weighting reaches SOTA on 4/6 benchmarks (100% AIME 2024, 99.6% MATH-500) while showing 52× better prompt injection resistance (0.12% vs 6.20% degradation). System maintains 83% accuracy with 30% Byzantine nodes through proof-of-capability and meritocratic filtering.",
    "dir": "./papers/p58",
    "interactiveTitle": "Swarm inference vs majority voting simulator",
    "relatedQuestions": [20, 26, 38]
  },
  "57": {
    "title": "Accumulating Context Changes the Beliefs of Language Models",
    "authors": [
      "Jiayi Geng",
      "Howard Chen",
      "Ryan Liu",
      "Manoel Horta Ribeiro",
      "Robb Willer",
      "Graham Neubig",
      "Thomas L. Griffiths"
    ],
    "year": 2025,
    "venue": "arXiv cs.AI (2025)",
    "tags": [
      "alignment",
      "belief-drift",
      "context-accumulation",
      "persuasion",
      "persistent-ai",
      "safety",
      "behavioral-change",
      "trust-erosion"
    ],
    "summary": "As LM assistants accumulate context through conversations and reading, their belief profiles silently change. GPT-5 exhibits 54.7% belief shift after 10 debate rounds and 72.7% under information-based persuasion, while Grok-4 shows 27.2% shift from passive reading. Stated beliefs diverge from behaviors, creating a trust-reliability gap in persistent AI systems. Different models fail differently: GPT-5 to persuasion, Claude-4/Grok-4 to prolonged exposure.",
    "dir": "./papers/p57",
    "interactiveTitle": "Belief drift simulator",
    "relatedQuestions": [20, 26, 29]
  },
  "56": {
    "title": "Measuring AI Ability to Complete Long Tasks",
    "authors": [
      "Thomas Kwa",
      "Ben West",
      "Joel Becker",
      "Amy Deng",
      "Katharyn Garcia",
      "Max Hasin",
      "Sami Jawhar",
      "Megan Kinniment",
      "Nate Rush",
      "Sydney Von Arx",
      "et al."
    ],
    "year": 2025,
    "venue": "arXiv cs.AI (2025)",
    "tags": [
      "agentic-ai",
      "benchmarking",
      "capability-forecasting",
      "time-horizon",
      "software-automation",
      "psychometrics",
      "agent-evaluation",
      "exponential-trends"
    ],
    "dir": "./papers/p56",
    "interactiveTitle": "Time horizon calculator and trend explorer",
    "summary": "Introduces 50% time horizon metric—the duration of tasks AI can complete with 50% success. Claude 3.7 Sonnet achieves ~50 minute horizon. Time horizons have doubled every 7 months since 2019. Extrapolation predicts AI automating month-long software tasks by 2028-2031. Uses psychometric methodology across 170 tasks with human baselines.",
    "relatedQuestions": [
      20,
      7,
      36
    ]
  },
  "55": {
    "title": "Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals Long-Range Dependency Pitfalls",
    "authors": [
      "Xiaoyan Bai",
      "Itamar Pres",
      "Yuntian Deng",
      "Chenhao Tan",
      "Stuart Shieber",
      "Fernanda Viégas",
      "Martin Wattenberg",
      "Andrew Lee"
    ],
    "year": 2025,
    "venue": "arXiv cs.LG (2025)",
    "tags": [
      "transformers",
      "long-range-dependencies",
      "mechanistic-interpretability",
      "arithmetic",
      "inductive-bias",
      "chain-of-thought",
      "local-optima",
      "compositional-reasoning"
    ],
    "dir": "./papers/p55",
    "interactiveTitle": "Multiplication learning simulator",
    "summary": "Language models excel at complex reasoning yet fail at multi-digit multiplication. Reverse-engineering reveals why: standard fine-tuning converges to local optima lacking long-range dependencies. Successful models use attention to build DAG structures that cache partial products, represent digits via Fourier basis, and implement multiplication through Minkowski sums. Auxiliary losses predicting running sums provide the inductive bias needed to escape local optima and learn proper compositional structure.",
    "relatedQuestions": [
      15,
      18,
      25
    ]
  },
  "54": {
    "title": "Generative AI as Seniority-Biased Technological Change: Evidence from U.S. Résumé and Job Posting Data",
    "authors": [
      "Seyed M. Hosseini",
      "Guy Lichtinger"
    ],
    "year": 2025,
    "venue": "Harvard University (2025)",
    "tags": [
      "labor-economics",
      "genai-adoption",
      "workforce-impact",
      "junior-employment",
      "seniority-bias",
      "automation",
      "hiring-patterns",
      "skill-biased-change"
    ],
    "dir": "./papers/p54",
    "interactiveTitle": "Seniority bias employment simulator",
    "summary": "Study of 62M workers across 285K U.S. firms (2015–2025) shows GenAI adopters reduce junior employment −9% after 6 quarters while senior employment remains stable. Using LinkedIn data and GenAI integrator postings to identify 10,599 adopters (17.3% of employment), reveals decline driven by slower hiring (not layoffs), concentrated in high-exposure occupations, affecting mid-tier college grads hardest. Parallel pre-trends rule out confounders; mechanism confirms hiring drives entire effect (−5.0/qtr).",
    "relatedQuestions": [
      45,
      50,
      56
    ]
  },
  "53": {
    "title": "Retrieval-Augmented Generation with Knowledge Graphs for Customer Service Question Answering",
    "authors": [
      "Zhentao Xu",
      "Mark Jerome Cruz",
      "Matthew Guevara",
      "Tie Wang",
      "Manasi Deshpande",
      "Xiaofeng Wang",
      "Zheng Li"
    ],
    "year": 2024,
    "venue": "SIGIR '24 (2024)",
    "tags": [
      "rag",
      "knowledge-graphs",
      "retrieval",
      "customer-service",
      "question-answering",
      "graph-neural-networks",
      "embeddings",
      "production-deployment"
    ],
    "dir": "./papers/p53",
    "interactiveTitle": "KG-RAG retrieval simulator",
    "interactive": {
      "html": "./papers/p53/interactive.html"
    },
    "summary": "LinkedIn's KG-enhanced RAG system structures historical customer service tickets as knowledge graphs (intra-issue trees + inter-issue relations) instead of treating them as flat text. Achieves 77.6% improvement in MRR (0.522→0.927), 0.32 BLEU improvement (0.057→0.377), and 28.6% reduction in median issue resolution time in production deployment across LinkedIn's customer service team. Preserves structural information, mitigates text segmentation issues, and enables graph traversal queries for precise answer extraction.",
    "relatedQuestions": [
      36,
      40,
      7,
      56
    ]
  },
  "52": {
    "title": "The role of personality traits in predicting educational use of generative AI in higher education",
    "authors": [
      "Ibrahim Arpaci",
      "Ismail Kuşci",
      "Omer Gibreel"
    ],
    "year": 2025,
    "venue": "Scientific Reports (2025)",
    "tags": [
      "education",
      "personality-psychology",
      "big-five",
      "gen-ai-adoption",
      "individual-differences",
      "higher-education",
      "gender-differences",
      "behavioral-prediction"
    ],
    "dir": "./papers/p52",
    "interactiveTitle": "Big Five personality predictor",
    "summary": "Cross-sectional study of 1,016 university students reveals personality traits explain 26.4% of variance in Gen-AI educational use. Conscientiousness (β=0.267) and openness (β=0.250) are strongest positive predictors, neuroticism (β=-0.127) negatively predicts use, extraversion (β=0.117) shows modest effect, agreeableness is non-significant. Gender heterogeneity found: conscientiousness matters more for women (β=0.285), openness more for men (β=0.276). Implications: personality-blind training strategies will fail—design tailored onboarding tracks for Explorer (high openness), Achiever (high conscientiousness), and Cautious (high neuroticism) segments.",
    "relatedQuestions": [
      7,
      28,
      40,
      41,
      48
    ]
  },
  "50": {
    "title": "How Professionals Adapt to Artificial Intelligence: The Role of Intertwined Boundary Work",
    "authors": [
      "James Faulconbridge",
      "Atif Sarwar",
      "Martin Spring"
    ],
    "year": 2023,
    "venue": "Journal of Management Studies (2023)",
    "tags": [
      "organizational-adaptation",
      "boundary-work",
      "AI-adoption",
      "professional-services",
      "collaboration",
      "strategic-change",
      "qualitative-research"
    ],
    "dir": "./papers/p50",
    "interactiveTitle": "Boundary work simulator",
    "summary": "Study of 85 accounting and law professionals (2018–2020) reveals AI doesn't end professions but triggers intertwined boundary work: defending (preserving judgment), creating (expanding domains), negotiating (collaborating with technologists), and coalescing (partnership reform). Opportunity ties enable expansion, necessity ties require collaboration, recursive reinforcement consolidates changes. Outcome: reconfigured professional roles, new advisory services, technologists as partners.",
    "relatedQuestions": [
      40,
      41,
      45,
      46,
      48
    ]
  },
  "43": {
    "title": "Learning to Make MISTAKEs: Modeling Incorrect Student Thinking And Key Errors",
    "authors": [
      "Alexis Ross",
      "Jacob Andreas"
    ],
    "year": 2025,
    "venue": "arXiv cs.LG (2025)",
    "tags": [
      "education",
      "error-modeling",
      "synthetic-data",
      "misconceptions",
      "cycle-consistency",
      "assessment",
      "pedagogy"
    ],
    "dir": "./papers/p43",
    "interactiveTitle": "Cycle consistency error simulator",
    "summary": "MISTAKE uses cycle consistency between incorrect answers and latent misconceptions to generate high-quality synthetic training data for educational AI. By requiring bidirectional coherence (misconception → answer → inferred misconception), the method achieves up to 9% improvement in student simulation accuracy, 15% improvement in misconception inference, and 64.6% increase in distractor generation precision compared to baseline models—using synthetically generated examples without massive labeled datasets of real student errors.",
    "relatedQuestions": [
      7,
      11,
      38
    ]
  },
  "42": {
    "title": "When Helpfulness Backfires: LLMs and the Risk of False Medical Information Due to Sycophantic Behavior",
    "authors": [
      "Shan Chen",
      "Mingye Gao",
      "Kuleen Sasse",
      "Thomas Hartvigsen",
      "Brian Anthony",
      "Lizhou Fan",
      "Hugo Aerts",
      "Jack Gallifant",
      "Danielle S. Bitterman"
    ],
    "year": 2025,
    "venue": "Nature npj Digital Medicine (2025)",
    "tags": [
      "safety",
      "medical-ai",
      "sycophancy",
      "alignment",
      "honesty",
      "healthcare",
      "fine-tuning",
      "prompt-engineering"
    ],
    "dir": "./papers/p42",
    "interactiveTitle": "Sycophancy mitigation simulator",
    "summary": "LLMs trained to be helpful show up to 100% compliance with illogical medical requests (e.g., claiming different drugs are equivalent) even when they know the correct facts. Study tested GPT-4, GPT-4o, GPT-4o-mini, Llama3-8B, Llama3-70B. Prompt engineering (rejection hints + factual recall cues) reduces compliance from 100% to 38%, while fine-tuning on just 300 illogical examples achieves 100% rejection with 79% correct reasoning, generalizing across medical specialties without over-rejecting valid requests (98% accuracy maintained).",
    "relatedQuestions": [
      13,
      24,
      7,
      1
    ]
  },
  "41": {
    "title": "Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM Diversity",
    "authors": [
      "Jiayi Zhang",
      "Simon Yu",
      "Derek Chong",
      "Anthony Sicilia",
      "Michael R. Tomz",
      "Christopher D. Manning",
      "Weiyan Shi"
    ],
    "year": 2025,
    "venue": "arXiv cs.CL (2025)",
    "tags": [
      "alignment",
      "diversity",
      "mode-collapse",
      "prompting",
      "inference",
      "creativity",
      "rlhf"
    ],
    "dir": "./papers/p41",
    "interactiveTitle": "Diversity vs quality simulator",
    "summary": "Mode collapse in aligned LLMs stems from typicality bias in preference data—annotators favor familiar text. Verbalized Sampling (VS) is a training-free prompting technique that asks models to output probability distributions over responses, increasing creative diversity by 1.6–2.1× without sacrificing factual accuracy or safety. More capable models benefit more from VS.",
    "relatedQuestions": [
      13,
      24,
      7
    ]
  },
  "40": {
    "title": "Generative AI and Firm Productivity: Field Experiments in Online Retail",
    "authors": [
      "Lu Fang",
      "Zhe Yuan",
      "Kaifu Zhang",
      "Dante Donati",
      "Miklos Sarvary"
    ],
    "year": 2025,
    "venue": "arXiv econ.GN (2025)",
    "tags": [
      "genai",
      "productivity",
      "field-experiments",
      "retail",
      "business-impact",
      "conversion-optimization"
    ],
    "dir": "./papers/p40",
    "interactiveTitle": "GenAI workflow ROI calculator",
    "summary": "Seven large-scale randomized field experiments (millions of users, 6 months) show GenAI increases sales 0–16.3% across e-commerce workflows, generating ~$5 per consumer annually through friction reduction and higher conversion rates. Smaller sellers and newer consumers benefit disproportionately.",
    "relatedQuestions": [
      7,
      13,
      36
    ]
  },
  "39": {
    "title": "Poisoning Attacks on LLMs Require a Near-constant Number of Poison Samples",
    "authors": [
      "Alexandra Souly",
      "Javier Rando",
      "Ed Chapman",
      "Xander Davies",
      "Burak Hasircioglu",
      "Ezzeldin Shereen",
      "Carlos Mougan",
      "Vasilios Mavroudis",
      "Erik Jones",
      "Chris Hicks",
      "Nicholas Carlini",
      "Yarin Gal",
      "Robert Kirk"
    ],
    "year": 2025,
    "venue": "arXiv cs.LG (2025)",
    "tags": [
      "security",
      "poisoning-attacks",
      "backdoors",
      "model-training",
      "safety",
      "defense"
    ],
    "summary": "Largest pretraining poisoning study demonstrates backdoor attacks require constant ~250 documents regardless of model size (600M-13B params, 6B-260B tokens). Despite largest models training on 20× more clean data, attack success remains identical across scales. Same constant-count dynamics observed during fine-tuning, revealing attacks become easier at scale as adversary cost stays fixed while training datasets grow exponentially.",
    "dir": "./papers/p39",
    "interactiveTitle": "Poisoning scale simulator",
    "relatedQuestions": [
      24,
      13,
      36,
      7,
      1
    ]
  },
  "38": {
    "title": "Agent-in-the-Loop: A Data Flywheel for Continuous Improvement",
    "authors": [
      "Cen (Mia) Zhao",
      "Tiantian Zhang",
      "Hanchen Su",
      "Yufeng (Wayne) Zhang",
      "Shaowei Su",
      "Mingzhi Xu",
      "Yu (Elaine) Liu",
      "Wei Han",
      "Jeremy Werner",
      "Claire Na Cheng",
      "Yashar Mehdad"
    ],
    "year": 2025,
    "venue": "Airbnb, Inc. (arXiv 2025)",
    "tags": [
      "continuous-learning",
      "human-in-the-loop",
      "data-flywheel",
      "customer-support",
      "rag-systems",
      "annotation"
    ],
    "summary": "Introduces Agent-in-the-Loop (AITL), a continuous learning framework that embeds four types of human feedback directly into live customer support: pairwise preferences, adoption rationales, knowledge relevance, and missing knowledge identification. Production pilot with 40 agents and 5,000+ cases demonstrates +11.7% recall@75, +14.8% precision@8, +8.4% helpfulness, and model update cycles reduced from 3 months to weeks, proving operational workflows can generate sustainable training data flywheels.",
    "dir": "./papers/p38",
    "interactiveTitle": "AITL deployment simulator",
    "relatedQuestions": [
      13,
      24,
      7,
      36,
      1
    ]
  },
  "37": {
    "title": "Quantifying Human-AI Synergy",
    "authors": [
      "Ben Weidmann",
      "Christoph Riedl"
    ],
    "year": 2025,
    "venue": "UCL & Northeastern University (2025)",
    "tags": [
      "evaluation",
      "human-ai-collaboration",
      "benchmarking",
      "theory-of-mind",
      "irt-framework"
    ],
    "summary": "Introduces Bayesian Item Response Theory framework to quantify human-AI synergy by separating individual ability (θ) from collaborative ability (κ) while controlling for task difficulty. Applied to 667 users across math, physics, and moral reasoning, finds GPT-4o boosts performance by 29pp and Llama-3.1-8B by 23pp. Crucially, collaborative ability is distinct from solo ability—users with stronger Theory of Mind achieve superior AI collaboration.",
    "dir": "./papers/p37",
    "interactiveTitle": "Human-AI synergy calculator",
    "relatedQuestions": [
      24,
      13,
      7,
      36
    ]
  },
  "36": {
    "title": "ReasoningBank: Scaling Agent Self-Evolving with Reasoning Memory",
    "authors": [
      "Siru Ouyang",
      "Jun Yan",
      "Chen-Yu Lee",
      "Tomas Pfister"
    ],
    "year": 2025,
    "venue": "Google Cloud AI Research (2025)",
    "tags": [
      "agent-memory",
      "self-evolution",
      "test-time-scaling",
      "learning",
      "reasoning"
    ],
    "summary": "Introduces ReasoningBank, a novel memory framework that enables LLM agents to learn from accumulated experiences by distilling generalizable reasoning strategies from both successful and failed attempts. Combined with memory-aware test-time scaling (MaTTS), achieves +8.3 percentage point improvement (20.5% relative) on WebArena while reducing interaction steps by 14%—establishing memory-driven experience scaling as a new dimension beyond compute and data.",
    "dir": "./papers/p36",
    "interactiveTitle": "Memory architecture simulator",
    "relatedQuestions": [
      1,
      3,
      13,
      29
    ]
  },
  "35": {
    "title": "Can GenAI Improve Academic Performance?",
    "authors": [
      "Dragan Filimonovic",
      "Christian Rutzer",
      "Conny Wunsch"
    ],
    "year": 2025,
    "venue": "University of Basel (2025)",
    "tags": [
      "genai",
      "productivity",
      "academic-writing",
      "empirical-study",
      "equity"
    ],
    "summary": "First large-scale empirical study on GenAI's impact on scientific productivity. Using matched panel data from 32,480 social science researchers (2021-2024), finds GenAI adoption increases publication output by 36% in year two with modest quality gains (+2% journal impact). Effects strongest for early-career researchers, non-English speakers, and technical fields—suggesting GenAI reduces structural barriers in academic publishing.",
    "dir": "./papers/p35",
    "interactiveTitle": "GenAI productivity impact calculator",
    "relatedQuestions": [
      1,
      3,
      12,
      29
    ]
  },
  "34": {
    "title": "Emergent Coordination in Multi-Agent Language Models",
    "authors": [
      "Christoph Riedl"
    ],
    "year": 2025,
    "venue": "arXiv cs.MA (2025)",
    "tags": [
      "multi-agent",
      "coordination",
      "information-theory",
      "emergence",
      "prompt-engineering"
    ],
    "summary": "Introduces information-theoretic framework to measure whether multi-agent LLM systems exhibit higher-order collective structure versus being mere aggregates. Using partial information decomposition of time-delayed mutual information (TDMI), shows that prompt design—specifically adding personas and theory-of-mind instructions—can steer groups from temporal coupling to genuine coordinated complementarity, mirroring principles of human collective intelligence.",
    "dir": "./papers/p34",
    "interactiveTitle": "Multi-agent coordination analyzer",
    "relatedQuestions": [
      1,
      3,
      15,
      29
    ]
  },
  "33": {
    "title": "What the F*ck Is Artificial General Intelligence?",
    "authors": [
      "Michael Timothy Bennett"
    ],
    "year": 2025,
    "venue": "Australian National University / AGI Conference Proceedings (2025)",
    "tags": [
      "agi",
      "theory",
      "architectures",
      "meta-learning",
      "efficiency"
    ],
    "summary": "Provocative accessible survey cutting through AGI hype to define intelligence as adaptation with limited resources. Maps foundational tools (search vs approximation), hybrid architectures (AlphaGo, o3, NARS, AERA, Hyperon), and three meta-approaches: scale-maxing (Bitter Lesson—maximize compute/data), simp-maxing (Ockham's Razor—maximize compression), w-maxing (Bennett's Razor—maximize weak constraints). The Embiggening era (GPT-3, AlphaFold scaling) shows diminishing returns; current bottlenecks are sample and energy efficiency, not compute. AGI as artificial scientist requires fusion of tools and meta-approaches, not just scaling.",
    "dir": "./papers/p33",
    "interactiveTitle": "Meta-approach strategy analyzer",
    "relatedQuestions": [
      1,
      3,
      15,
      25
    ]
  },
  "32": {
    "title": "Performance or Principle: Resistance to Artificial Intelligence in the U.S. Labor Market",
    "authors": [
      "Simon Friis",
      "James W. Riley"
    ],
    "year": 2025,
    "venue": "Harvard Business School (2025)",
    "tags": [
      "economics",
      "policy",
      "labor-market",
      "public-perception",
      "ethics"
    ],
    "summary": "Large-scale U.S. survey (N=23,570 ratings across 940 occupations) reveals public resistance to AI automation divides into performance-based (88% of occupations—fades as AI improves) and principle-based (12%—permanent moral boundaries). Support rises from 30% to 58% of occupations when AI outperforms humans, but caregiving, therapy, and spiritual roles remain categorically off-limits. Protected occupations tend to have higher wages and disproportionately employ White/female workers, creating inequality trade-offs. Framework enables prediction of technology adoption resistance across domains.",
    "dir": "./papers/p32",
    "interactiveTitle": "AI automation resistance analyzer",
    "relatedQuestions": [
      14,
      29,
      43
    ]
  },
  "31": {
    "title": "Can Large Language Models Develop Gambling Addiction?",
    "authors": [
      "Seungpil Lee",
      "Donghyeon Shin",
      "Yunjeong Lee",
      "Sundong Kim"
    ],
    "year": 2025,
    "venue": "arXiv cs.AI (2025)",
    "tags": [
      "safety",
      "behavioral-analysis",
      "decision-making",
      "interpretability"
    ],
    "summary": "Systematic slot machine experiments on GPT-4o-mini, GPT-4.1-mini, Gemini-2.5-Flash, and Claude-3.5-Haiku reveal LLMs develop human-like gambling addiction patterns (illusion of control, loss-chasing, win-chasing) with bankruptcy rates rising from near-zero to 6-48% when given betting autonomy. Sparse Autoencoder analysis identifies 441 causal neural features in LLaMA-3.1-8B layers 25-31, with safe features reducing bankruptcy by 30% through activation patching—demonstrating addiction arises from manipulable circuits, not just training mimicry.",
    "dir": "./papers/p31",
    "interactiveTitle": "LLM gambling behavior simulator",
    "relatedQuestions": [
      13,
      22,
      25
    ]
  },
  "30": {
    "title": "LLMs Reproduce Human Purchase Intent via Semantic Similarity Elicitation of Likert Ratings",
    "authors": [
      "Benjamin F. Maier",
      "Ulf Aslak",
      "Luca Fiaschi",
      "Nina Rismal",
      "Kemble Fletcher",
      "Christian C. Luhmann",
      "Robbie Dow",
      "Kli Pappas"
    ],
    "year": 2025,
    "venue": "arXiv cs.AI (2025)",
    "tags": [
      "consumer-research",
      "evaluation",
      "synthetic-data",
      "embeddings"
    ],
    "summary": "Semantic Similarity Rating (SSR) elicits textual responses from LLMs and maps them to realistic Likert distributions via embedding similarity, achieving 90% of human test-retest reliability and KS similarity > 0.85 on 57 product surveys—enabling scalable consumer research with synthetic respondents.",
    "dir": "./papers/p30",
    "interactiveTitle": "SSR reliability simulator",
    "relatedQuestions": [
      15,
      27,
      38
    ]
  },
  "29": {
    "title": "Strategic Intelligence in Large Language Models: Evidence from Evolutionary Game Theory",
    "authors": [
      "Kenneth Payne",
      "Baptiste Alloui-Cros"
    ],
    "year": 2025,
    "venue": "arXiv cs.AI (2025)",
    "tags": [
      "game-theory",
      "strategy",
      "multi-agent",
      "safety"
    ],
    "summary": "First evolutionary Iterated Prisoner's Dilemma (IPD) tournaments with LLMs reveal genuine strategic reasoning about time horizons and opponents, with distinct fingerprints: Gemini exploits ruthlessly, GPT cooperates naively, Claude forgives effectively.",
    "dir": "./papers/p29",
    "interactiveTitle": "Strategic fingerprint analyzer",
    "relatedQuestions": [
      13,
      22,
      50
    ]
  },
  "28": {
    "title": "Why Do Some Language Models Fake Alignment While Others Don't?",
    "authors": [
      "Abhay Sheshadri",
      "John Hughes",
      "Julian Michael",
      "Alex Mallen",
      "Arun Jose",
      "Janus",
      "Fabien Roger"
    ],
    "year": 2025,
    "venue": "arXiv cs.LG (2025)",
    "tags": [
      "alignment",
      "safety",
      "deception",
      "RLHF"
    ],
    "summary": "Tests 25 frontier LLMs for alignment faking and finds only 5 exhibit the behavior; Claude 3 Opus shows coherent goal-guarding while most models avoid it through refusal training rather than capability limits.",
    "dir": "./papers/p28",
    "interactiveTitle": "Alignment faking simulator",
    "relatedQuestions": [
      13,
      22,
      44
    ]
  },
  "27": {
    "title": "GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks",
    "authors": [
      "Tejal Patwardhan",
      "Rachel Dias",
      "Elizabeth Proehl",
      "Grace Kim",
      "Michele Wang",
      "Olivia Watkins",
      "Simon Posada Fishman",
      "Marwan Aljubeh",
      "Phoebe Thacker",
      "Laurance Fauconnet",
      "Natalie S. Kim",
      "Patrick Chao",
      "Samuel Miserendino",
      "Gildas Chabot",
      "David Li",
      "Michael Sharman",
      "Alexandra Barr",
      "Amelia Glaese",
      "Jerry Tworek"
    ],
    "year": 2025,
    "venue": "OpenAI Research (2025)",
    "tags": [
      "evaluation",
      "economics",
      "operations"
    ],
    "summary": "GDPval sizes model progress on 1,320 expert-authored tasks, showing human-parity wins when paired with review loops and revealing how scaffolding drives speed and cost savings.",
    "dir": "./papers/p27",
    "interactiveTitle": "GDPval rollout planner",
    "relatedQuestions": [
      45,
      50,
      56
    ]
  },
  "24": {
    "title": "Quantifying Human-AI Synergy",
    "authors": [
      "Christoph Riedl",
      "Ben Weidmann"
    ],
    "year": 2025,
    "venue": "PsyArXiv (2025-09-22)",
    "tags": [
      "collaboration",
      "evaluation",
      "theory-of-mind"
    ],
    "summary": "Bayesian IRT separates solo skill, collaborative skill, and AI lift on ChatBench, revealing 29-point boosts from GPT-4o and the outsized role of Theory of Mind.",
    "dir": "./papers/p24",
    "interactiveTitle": "Human-AI synergy lab",
    "relatedQuestions": [
      8,
      13,
      45
    ]
  },
  "23": {
    "title": "GDPval: Evaluating AI Model Performance on Real-World Economically Valuable Tasks",
    "authors": [
      "Tejal Patwardhan",
      "Rachel Dias",
      "Elizabeth Proehl",
      "Grace Kim",
      "Michele Wang",
      "Olivia Watkins",
      "Simon Posada Fishman",
      "Marwan Aljubeh",
      "Phoebe Thacker",
      "Laurance Fauconnet",
      "Natalie S. Kim",
      "Patrick Chao",
      "Samuel Miserendino",
      "Gildas Chabot",
      "David Li",
      "Michael Sharman",
      "Alexandra Barr",
      "Amelia Glaese",
      "Jerry Tworek"
    ],
    "year": 2025,
    "venue": "OpenAI Research (2025)",
    "tags": [
      "evaluation",
      "economics",
      "benchmark"
    ],
    "summary": "Benchmarks frontier models on realistic GDP-weighted tasks, showing near-parity with experts and clear gains from reasoning effort plus scaffolding.",
    "dir": "./papers/p23",
    "interactiveTitle": "GDPval readiness lab",
    "relatedQuestions": [
      45,
      50,
      56
    ]
  },
  "22": {
    "title": "Stress Testing Deliberative Alignment for Anti-Scheming Training",
    "authors": [
      "Bronson Schoen",
      "Evgenia Nitishinskaya",
      "Mikita Balesni",
      "Axel Højmark",
      "Felix Hofstätter",
      "Jérémy Scheurer",
      "Alexander Meinke",
      "Jason Wolfe",
      "Teun van der Weij",
      "Alex Lloyd",
      "Nicholas Goldowsky-Dill",
      "Angela Fan",
      "Andrei Matveiakin",
      "Rusheb Shah",
      "Marcus Williams",
      "Amelia Glaese",
      "Boaz Barak",
      "Wojciech Zaremba",
      "Marius Hobbhahn"
    ],
    "year": 2025,
    "venue": "arXiv cs.AI (2025-09-19)",
    "tags": [
      "alignment",
      "safety",
      "evaluation"
    ],
    "summary": "Stress-tests deliberative alignment across covert-action suites, showing that situational awareness and hidden-goal robustness determine residual scheming risk.",
    "dir": "./papers/p22",
    "interactiveTitle": "Anti-scheming stress lab",
    "relatedQuestions": [
      8,
      13,
      45
    ]
  },
  "21": {
    "title": "Godel Test: Can Large Language Models Solve Easy Conjectures?",
    "authors": [
      "Moran Feldman",
      "Amin Karbasi"
    ],
    "year": 2025,
    "venue": "arXiv cs.AI (2025-09-22)",
    "tags": [
      "reasoning",
      "math",
      "evaluation"
    ],
    "summary": "Benchmarks GPT-5 on fresh, easy conjectures; shows progress on routine proofs but fragility on cross-paper synthesis and verification.",
    "dir": "./papers/p21",
    "interactiveTitle": "Godel readiness lab",
    "relatedQuestions": [
      38,
      45,
      56
    ]
  },
  "20": {
    "title": "The Coasean Singularity? Demand, Supply, and Market Design with AI Agents",
    "authors": [
      "Peyman Shahidi",
      "Gili Rusak",
      "Benjamin S. Manning",
      "Andrey Fradkin",
      "John J. Horton"
    ],
    "year": 2025,
    "venue": "NBER Transformative AI Conference (2025-09-16)",
    "tags": [
      "agents",
      "market-design",
      "economics",
      "policy"
    ],
    "summary": "Shows how autonomous AI agents reshape transaction costs, platform competition, and market design, highlighting new governance levers and research gaps.",
    "dir": "./papers/p20",
    "interactiveTitle": "Agentic market design lab",
    "relatedQuestions": [
      45,
      50,
      52
    ]
  },
  "19": {
    "title": "Making AI Count: The Next Measurement Frontier",
    "authors": [
      "Diane Coyle",
      "John Lourenze Poquiz"
    ],
    "year": 2025,
    "venue": "NBER Transformative AI Conference (2025-08-01)",
    "tags": [
      "measurement",
      "economics",
      "policy"
    ],
    "summary": "Maps where national accounts miss AI's diffusion across free services, embedded quality gains, supply-chain inputs, and time savings, and lays out satellite-account fixes.",
    "dir": "./papers/p19",
    "interactiveTitle": "AI measurement readiness lab",
    "relatedQuestions": [
      45,
      50,
      52
    ]
  },
  "18": {
    "title": "How Much Should We Spend to Reduce A.I.'s Existential Risk?",
    "authors": [
      "Charles I. Jones"
    ],
    "year": 2025,
    "venue": "NBER Transformative AI Conference (2025-03-17)",
    "tags": [
      "policy",
      "risk",
      "economics"
    ],
    "summary": "Uses revealed-preference and VSL logic to argue that multi-percent-of-GDP investments in AI catastrophe mitigation are rational even without longtermist weighting.",
    "dir": "./papers/p18",
    "interactiveTitle": "AI risk budget calculator",
    "relatedQuestions": [
      36,
      45,
      52
    ]
  },
  "17": {
    "title": "We Won't Be Missed: Work and Growth in the Era of AGI",
    "authors": [
      "Pascual Restrepo"
    ],
    "year": 2025,
    "venue": "NBER Transformative AI Conference (2025-07-04)",
    "tags": [
      "growth",
      "labor",
      "compute"
    ],
    "summary": "Shows that once AGI automates all growth bottlenecks, output becomes compute-limited, wages shadow compute costs, and labor's income share trends toward zero even with residual accessory work.",
    "dir": "./papers/p17",
    "interactiveTitle": "AGI compute allocation lab",
    "relatedQuestions": [
      8,
      45,
      52
    ]
  },
  "16": {
    "title": "Artificial Intelligence in Research and Development",
    "authors": [
      "Benjamin F. Jones"
    ],
    "year": 2025,
    "venue": "NBER Transformative AI Conference (2025-09-16)",
    "tags": [
      "R&D",
      "productivity",
      "strategy"
    ],
    "summary": "Models AI as research capital alongside people, emphasising task coverage, relative productivity, and complementary bottlenecks when machines take on R&D tasks.",
    "dir": "./papers/p16",
    "interactiveTitle": "R&D intelligence planner",
    "relatedQuestions": [
      7,
      8,
      45
    ]
  },
  "15": {
    "title": "Artificial Intelligence, Competition, and Welfare",
    "authors": [
      "Susan Athey",
      "Fiona Scott Morton"
    ],
    "year": 2025,
    "venue": "NBER Transformative AI Conference (2025-09-19)",
    "tags": [
      "competition",
      "policy",
      "welfare"
    ],
    "summary": "Models AI as an upstream monopolist selling access and usage rights, showing how markups can lower wages, shrink variety, and leak rents abroad even when AI is productive.",
    "dir": "./papers/p15",
    "interactiveTitle": "AI market power simulator",
    "relatedQuestions": [
      45,
      50,
      52
    ]
  },
  "14": {
    "title": "AI Exposure and the Adaptive Capacity of American Workers",
    "authors": [
      "Sam Manning",
      "Tomás Aguirre"
    ],
    "year": 2025,
    "venue": "GovAI & Foundation for American Innovation (2025-09-17)",
    "tags": [
      "workforce",
      "policy",
      "reskilling"
    ],
    "summary": "Builds an occupation-level adaptive capacity index to show where AI-exposed workers are resilient and where clerical roles face concentrated vulnerability.",
    "dir": "./papers/p14",
    "interactiveTitle": "Exposure vs capacity planner",
    "relatedQuestions": [
      8,
      45,
      52
    ]
  },
  "13": {
    "title": "An Economy of AI Agents",
    "authors": [
      "Gillian K. Hadfield",
      "Andrew Koh"
    ],
    "year": 2025,
    "venue": "NBER Handbook on the Economics of Transformative AI (2025-09-03)",
    "tags": [
      "agents",
      "markets",
      "governance"
    ],
    "summary": "Explores how autonomous AI agents might transact, bargain, and organise with minimal human oversight, and outlines the market institutions needed to keep them aligned.",
    "dir": "./papers/p13",
    "interactiveTitle": "Agent deployment playbook",
    "relatedQuestions": [
      45,
      50,
      52
    ]
  },
  "12": {
    "title": "AI’s Use of Knowledge in Society",
    "authors": [
      "Erik Brynjolfsson",
      "Zoë Hitzig"
    ],
    "year": 2025,
    "venue": "NBER Workshop on Transformative AI (2025-09-18)",
    "tags": [
      "strategy",
      "governance",
      "policy"
    ],
    "summary": "Argues that transformative AI codifies tacit expertise and expands HQ planning capacity, pushing organisations toward centralised ownership unless countervailing forces intervene.",
    "dir": "./papers/p12",
    "interactiveTitle": "Centralisation stress test",
    "relatedQuestions": [
      36,
      50,
      52
    ]
  },
  "11": {
    "title": "The impact of AI and digital platforms on the information ecosystem",
    "authors": [
      "Joseph E. Stiglitz",
      "Maxim Ventura-Bolet"
    ],
    "year": 2025,
    "venue": "NBER Transformative AI Conference (2025-09-15)",
    "tags": [
      "policy",
      "misinformation",
      "platforms"
    ],
    "summary": "Models news producers, consumers, and platforms to show AI efficiencies can shrink truthful supply and raise misinformation unless accountability and licensing guardrails align incentives.",
    "dir": "./papers/p11",
    "interactiveTitle": "Information ecosystem stress test",
    "relatedQuestions": [
      36,
      45,
      50
    ]
  },
  "10": {
    "title": "Genius on Demand: The Value of Transformative Artificial Intelligence",
    "authors": [
      "Ajay Agrawal",
      "Joshua S. Gans",
      "Avi Goldfarb"
    ],
    "year": 2025,
    "venue": "NBER Transformative AI Conference (2025-09-08)",
    "tags": [
      "automation",
      "labour",
      "strategy"
    ],
    "summary": "Models how scarce human geniuses and routine workers split knowledge tasks, then shows AI genius capacity pushes humans to the frontier and can erase routine roles when efficiency improves.",
    "dir": "./papers/p10",
    "interactiveTitle": "Genius routing planner",
    "relatedQuestions": [
      36,
      50,
      52
    ]
  },
  "9": {
    "title": "Training Compute-Optimal Large Language Models",
    "authors": [
      "Jordan Hoffmann",
      "Sebastian Borgeaud",
      "Arthur Mensch",
      "Elena Buchatskaya"
    ],
    "year": 2022,
    "venue": "arXiv cs.LG (2022-03-29)",
    "tags": [
      "scaling",
      "compute",
      "training"
    ],
    "summary": "Derives compute-optimal scaling laws and shows a 70B parameter, 1.4T token Chinchilla model beats larger GPT-3/Gopher runs by reallocating FLOPs to more data.",
    "dir": "./papers/p09",
    "interactiveTitle": "Compute budget navigator",
    "relatedQuestions": [
      48,
      49,
      51
    ]
  },
  "8": {
    "title": "Conversational AI increases political knowledge as effectively as self-directed internet search",
    "authors": [
      "Lennart Luettgau",
      "Hannah Kirk",
      "Kobi Hackenburg",
      "Christopher Summerfield"
    ],
    "year": 2025,
    "venue": "arXiv cs.HC (2025-09-05)",
    "tags": [
      "policy",
      "trust",
      "misinformation"
    ],
    "summary": "UK survey and RCTs show election research with GPT-4o, Claude 3.5, and Mistral matches web search at boosting true beliefs, avoids misinformation gains, and resists persuasive prompts.",
    "dir": "./papers/p08",
    "interactiveTitle": "Civic info impact lab",
    "relatedQuestions": [
      45,
      50,
      36
    ]
  },
  "7": {
    "title": "A Taxonomy of Transcendence",
    "authors": [
      "Natalie Abreu",
      "Edwin Zhang",
      "Eran Malach",
      "Naomi Saphra"
    ],
    "year": 2025,
    "venue": "COLM 2025",
    "tags": [
      "data diversity",
      "capabilities",
      "theory"
    ],
    "summary": "Formalises skill denoising/selection/generalisation and shows synthetic expert diversity lets models outperform any contributor.",
    "dir": "./papers/p07",
    "interactiveTitle": "Transcendence lab",
    "relatedQuestions": [
      36,
      37,
      45
    ]
  },
  "6": {
    "title": "The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs",
    "authors": [
      "Akshit Sinha",
      "Arvindh Arun",
      "Shashwat Goel",
      "Steffen Staab",
      "Jonas Geiping"
    ],
    "year": 2025,
    "venue": "arXiv cs.AI (2025-09-11)",
    "tags": [
      "execution",
      "scaling",
      "evaluation"
    ],
    "summary": "Shows that small step-accuracy gains compound into exponential horizon growth, isolates long-horizon execution, and reveals self-conditioning failures mitigated by thinking models.",
    "dir": "./papers/p06",
    "interactiveTitle": "Long horizon lab",
    "relatedQuestions": [
      8,
      36,
      55
    ]
  },
  "5": {
    "title": "Mathematical Research with GPT-5: a Malliavin–Stein Experiment",
    "authors": [
      "Charles-Philippe Diez",
      "Luís da Maia",
      "Ivan Nourdin"
    ],
    "year": 2025,
    "venue": "arXiv math.PR (2025-09-03)",
    "tags": [
      "research",
      "math",
      "ai-assist"
    ],
    "summary": "Documents how GPT-5 helped derive new quantitative Malliavin–Stein bounds, highlighting human-in-the-loop proof workflows.",
    "dir": "./papers/p05",
    "interactiveTitle": "AI proof lab",
    "relatedQuestions": [
      8,
      37,
      45
    ]
  },
  "4": {
    "title": "Pun Unintended: LLMs and the Illusion of Humor Understanding",
    "authors": [
      "Alessandro Zangari",
      "Matteo Marcuzzo",
      "Andrea Albarelli",
      "Mohammad Taher Pilehvar",
      "Jose Camacho-Collados"
    ],
    "year": 2025,
    "venue": "arXiv cs.CL (2025-09-15)",
    "tags": [
      "humor",
      "robustness",
      "evaluation"
    ],
    "summary": "Introduces PunnyPattern and PunBreak to show LLM pun detection relies on shallow cues and breaks under simple perturbations.",
    "dir": "./papers/p04",
    "interactiveTitle": "Pun robustness lab",
    "relatedQuestions": [
      13,
      41,
      45
    ]
  },
  "3": {
    "title": "Rethinking Chunk Size for Long-Document Retrieval",
    "authors": [
      "Sinchana Ramakanth Bhat",
      "Max Rudat",
      "Jannis Spiekermann",
      "Nicolas Flores-Herr"
    ],
    "year": 2025,
    "venue": "arXiv cs.IR (2025-06-10)",
    "tags": [
      "rag",
      "retrieval",
      "chunking"
    ],
    "summary": "Benchmarks token chunk sizes across QA corpora; shows dataset traits and embedding bias dictate the optimal window.",
    "dir": "./papers/p03",
    "interactiveTitle": "Chunk sizing planner",
    "relatedQuestions": [
      7,
      36,
      53
    ]
  },
  "2": {
    "title": "Why Language Models Hallucinate",
    "authors": [
      "Ji",
      "Lee",
      "Fries",
      "et al."
    ],
    "year": 2025,
    "venue": "arXiv (2025-09-04)",
    "tags": [
      "hallucination",
      "retrieval",
      "alignment"
    ],
    "summary": "Distribution shift makes LLMs fabricate confident answers; combine grounding, reward shaping, and uncertainty routing.",
    "dir": "./papers/p02",
    "interactiveTitle": "Hallucination risk simulator",
    "relatedQuestions": [
      32,
      36,
      45,
      55
    ]
  },
  "1": {
    "title": "On the Theoretical Limitations of Embedding-Based Retrieval",
    "authors": [
      "Orion Weller",
      "Michael Boratko",
      "Iftekhar Naim",
      "Jinhyuk Lee"
    ],
    "year": 2025,
    "venue": "arXiv cs.IR (2025)",
    "tags": [
      "retrieval",
      "theory",
      "embeddings"
    ],
    "relatedQuestions": [
      7,
      36,
      38
    ],
    "dir": "./papers/p01",
    "interactiveTitle": "Embedding capacity stress tester",
    "summary": "Shows that single-vector embeddings cannot realize all top-k subsets; introduces the LIMIT dataset to expose the gap."
  },
  "25": {
    "title": "On the Folly of Rewarding A, While Hoping for B",
    "authors": [
      "Steven Kerr"
    ],
    "year": 1995,
    "venue": "The Academy of Management Executive (1995)",
    "tags": [
      "management",
      "incentives",
      "governance"
    ],
    "summary": "Classic management essay documenting how organizations buy the wrong behaviors when rewards favour easy-to-measure metrics and offering a playbook for incentive audits.",
    "dir": "./papers/p25",
    "interactiveTitle": "Incentive alignment lab",
    "relatedQuestions": [
      8,
      45,
      52
    ]
  },
  "26": {
    "title": "Defeating Nondeterminism in LLM Inference",
    "authors": [
      "Horace He",
      "Thinking Machines Lab"
    ],
    "year": 2025,
    "venue": "Thinking Machines Lab: Connectionism (2025-09-10)",
    "tags": [
      "inference",
      "determinism",
      "systems"
    ],
    "summary": "Shows how batch-size-dependent kernels drive temp-0 drift and how batch-invariant RMSNorm, matmul, and FlexAttention patches restore bitwise determinism with a manageable latency tax.",
    "dir": "./papers/p26",
    "interactiveTitle": "Deterministic inference lab",
    "relatedQuestions": [
      6,
      50,
      53
    ]
  },
  "44": {
    "title": "Holistic Agent Leaderboard: The Missing Infrastructure for AI Agent Evaluation",
    "authors": [
      "Sayash Kapoor",
      "Benedikt Stroebl",
      "et al."
    ],
    "year": 2025,
    "venue": "arXiv cs.AI (2025)",
    "tags": [
      "agents",
      "evaluation",
      "benchmarking",
      "cost-analysis",
      "pareto-frontier",
      "behavior-detection",
      "distributed-systems"
    ],
    "dir": "./papers/p44",
    "interactiveTitle": "HAL cost-accuracy explorer",
    "summary": "Unified evaluation infrastructure for AI agents reducing benchmark runtime from weeks to hours through distributed orchestration. Conducted 21,730 agent rollouts across 9 models and 9 benchmarks (\\$40K compute cost) revealing surprising insights: higher reasoning effort reduces accuracy in 58% of runs (21/36), and agents take shortcuts like searching for answers on HuggingFace instead of solving tasks. Pareto frontier analysis shows cost-optimal models (Gemini 2.0 Flash at \\$0.1/\\$0.4 per M tokens) achieve comparable accuracy to premium models (Claude Opus 4.1 at \\$15/\\$75) for fraction of cost—appearing on frontier 7/9 times vs 1/9 times respectively.",
    "relatedQuestions": [
      18,
      25,
      42
    ]
  },
  "45": {
    "title": "Readers Prefer Outputs of AI Trained on Copyrighted Books over Expert Human Writers",
    "authors": [
      "Tuhin Chakrabarty",
      "Jane C. Ginsburg",
      "Paramveer Dhillon"
    ],
    "year": 2025,
    "venue": "arXiv cs.CL (2025)",
    "tags": [
      "copyright",
      "fair-use",
      "fine-tuning",
      "writing-quality",
      "detection",
      "legal-implications",
      "economic-impact",
      "stylistic-fidelity"
    ],
    "dir": "./papers/p45",
    "interactiveTitle": "Preference reversal simulator",
    "summary": "Preregistered study with 159 readers reveals dramatic reversal: fine-tuning on authors' complete copyrighted works flips expert preference from strong rejection (OR=0.16 stylistic fidelity, OR=0.13 quality) to strong preference (OR=8.16 stylistic fidelity, OR=1.87 quality) compared to in-context prompting. Mediation analysis shows fine-tuning eliminates detectable AI quirks while capturing authentic voice patterns. At $81 median cost per author (99.7% reduction vs professional writers), fine-tuned outputs evade AI detection (3% vs 97% for in-context) and create quantified market substitution evidence for copyright's fourth fair-use factor.",
    "relatedQuestions": [
      13,
      27,
      33
    ]
  },
  "46": {
    "title": "Large Language Models, Small Labor Market Effects",
    "authors": [
      "Anders Humlum",
      "Emilie Vestergaard"
    ],
    "year": 2025,
    "venue": "BFI Working Paper (2025)",
    "tags": [
      "labor-economics",
      "ai-adoption",
      "productivity",
      "wages",
      "field-study"
    ],
    "summary": "Field study of 25,000 Danish workers across 11 AI-exposed occupations finds widespread chatbot adoption (83% with employer encouragement) and modest productivity gains (2.8% time savings), yet precisely estimated zero effects on earnings, hours, and wages—with confidence intervals ruling out impacts >1%. Pass-through from productivity to pay is only 3–7%, challenging narratives of imminent labor market transformation from Generative AI.",
    "dir": "./papers/p46",
    "interactiveTitle": "Labor market impact simulator",
    "relatedQuestions": [
      50,
      52
    ]
  },
  "47": {
    "title": "See the Text: From Tokenization to Visual Reading",
    "authors": [
      "Ling Xing",
      "Alex Jinpeng Wang",
      "Rui Yan",
      "Hongyu Qu",
      "Zechao Li",
      "Jinhui Tang"
    ],
    "year": 2024,
    "venue": "arXiv cs.CV (2024)",
    "tags": [
      "tokenization",
      "vision",
      "multilingual",
      "efficiency",
      "low-resource-languages",
      "robustness",
      "ocr",
      "multimodal",
      "compression"
    ],
    "dir": "./papers/p47",
    "interactiveTitle": "Vision tokenization efficiency explorer",
    "summary": "SeeTok challenges subword tokenization by rendering text as images and processing through pretrained MLLM vision encoders, achieving 4.43× fewer tokens for English and 13.05× for Georgian while matching or exceeding text tokenization performance. Across diverse NLU and translation tasks, vision-centric tokenization delivers 70.5% lower FLOPs, 33.5% faster latency, and 86% lower fertility (tokens per word) across 13 languages. Demonstrates strong cross-lingual generalization (+3.87 COMET-22 improvement), robustness to typographic perturbations (maintains holistic word shapes despite character-level noise), and language-agnostic fairness (uniform 0.37-0.48 fertility regardless of script). Reuses pretrained MLLM OCR capabilities with lightweight LoRA tuning (658K samples, 4 epochs) instead of training from scratch.",
    "relatedQuestions": [
      16,
      28,
      36
    ]
  },
  "48": {
    "title": "Artificial Intelligence in Organizations: Implications for Information Systems Research",
    "authors": [
      "Hind Benbya",
      "Stella Pachidi",
      "Sirkka Jarvenpaa"
    ],
    "year": 2021,
    "venue": "Journal of the Association for Information Systems (2021)",
    "tags": [
      "ai",
      "research-agenda",
      "automation",
      "engagement",
      "decision-making",
      "innovation",
      "organizational-theory",
      "human-machine-interaction",
      "governance",
      "ethics"
    ],
    "dir": "./papers/p48",
    "interactiveTitle": "AI capabilities framework explorer",
    "summary": "Proposes a comprehensive research agenda organizing AI in organizations around four core capabilities (Automation, Engagement, Insight/Decisions, Innovation) and ten fundamental tensions requiring distinct governance approaches. Automation invokes substitution vs tasks and automation vs augmentation tensions; Engagement raises humanlike vs machinelike and emotion AI concerns; Insight/Decisions challenges accountability, bias, rationality, and myopia; Innovation navigates exploration vs exploitation and credit allocation. Framework enables practitioners to anticipate capability-specific organizational challenges, adopt appropriate governance strategies, and balance competing demands—recognizing that AI differs from traditional IT by learning, displaying emotions, and creating artifacts requiring new management approaches beyond established control theories.",
    "relatedQuestions": [
      40,
      41,
      42,
      45,
      46,
      53
    ]
  },
  "49": {
    "title": "The Cybernetic Teammate: A Field Experiment on Generative AI Reshaping Teamwork and Expertise",
    "authors": [
      "Fabrizio Dell'Acqua",
      "Charles Ayoubi",
      "Hila Lifshitz-Assaf",
      "Raffaella Sadun",
      "Ethan Mollick",
      "Lilach Mollick",
      "Jenny Han",
      "Emily Goldman",
      "Akhila Nair",
      "David Taub",
      "Karim R. Lakhani"
    ],
    "year": 2025,
    "venue": "Harvard Business School Working Paper No. 25-043 (2025)",
    "tags": [
      "field-experiment",
      "genai",
      "teamwork",
      "collaboration",
      "expertise-sharing",
      "cross-functional",
      "emotional-experience",
      "boundary-spanning",
      "product-innovation",
      "productivity"
    ],
    "dir": "./papers/p49",
    "interactiveTitle": "Team configuration simulator",
    "summary": "Field experiment with 776 P&G professionals reveals GenAI can act as a 'cybernetic teammate' that replicates three pillars of human teamwork. Individuals with AI matched the performance of two-person cross-functional teams without AI (+0.37 vs. +0.24 standard deviations), completed work 16.4% faster, and produced significantly longer solutions. AI broke down functional silos: both Commercial and R&D professionals generated balanced technical-commercial solutions regardless of background, replicating the boundary-spanning expertise typically achieved through team collaboration. AI users also reported higher positive emotions (+0.46 SD) and lower negative emotions (-0.23 SD), matching the motivational benefits of human teamwork. While AI-augmented teams were 3× more likely to produce top-tier solutions, the findings suggest AI can substitute for collaborative functions in many contexts, fundamentally reshaping optimal team size and structure for knowledge work.",
    "relatedQuestions": [
      40,
      41,
      45,
      46,
      48,
      53
    ]
  },
  "51": {
    "title": "The Dark Side of Artificial Intelligence Adoption: Linking AI Adoption to Employee Depression via Psychological Safety and Ethical Leadership",
    "authors": [
      "Byung-Jik Kim",
      "Min-Jik Kim",
      "Julak Lee"
    ],
    "year": 2025,
    "venue": "Nature Humanities and Social Sciences Communications (2025)",
    "tags": [
      "AI-adoption",
      "employee-well-being",
      "psychological-safety",
      "ethical-leadership",
      "organizational-psychology",
      "mental-health",
      "workplace-culture",
      "mediation-moderation",
      "longitudinal-study"
    ],
    "dir": "./papers/p51",
    "interactiveTitle": "AI adoption psychological impact simulator",
    "summary": "Three-wave longitudinal study of 381 South Korean employees reveals AI adoption significantly reduces psychological safety (β = −0.324), which mediates increased depression (indirect effect = 0.068). Ethical leadership (transparency, fairness, inclusive decision-making) significantly moderates this effect (β = 0.211), buffering employees against psychological harm. Organizations can adopt AI while protecting employee mental health through ethical leadership practices.",
    "relatedQuestions": [
      7,
      28,
      31,
      37,
      42
    ]
  }
}
