<div class="space-y-5">
  <div class="bg-indigo-50 border border-indigo-200 rounded-lg p-4">
    <div class="flex flex-wrap items-start justify-between gap-4">
      <div>
        <h2 class="text-xl font-semibold text-indigo-900">Why Language Models Hallucinate</h2>
        <p class="text-sm text-indigo-700">Adam Tauman Kalai, Ofir Nachum, Santosh S. Vempala, Edwin Zhang ‚Ä¢ arXiv (2025-09-04)</p>
      </div>
      <a href="https://cdn.openai.com/pdf/d04913be-3f6f-4d2b-b283-ff432ef4aaa5/why-language-models-hallucinate.pdf" target="_blank" rel="noopener" class="inline-flex items-center gap-2 px-3 py-1.5 rounded-md text-xs font-medium bg-white border border-indigo-200 text-indigo-700 hover:bg-indigo-100">
        Read paper ‚Üó
      </a>
    </div>
    <p class="mt-3 text-sm text-indigo-800 leading-relaxed">
      The authors prove that hallucinations arise from the same statistical forces that drive errors in binary classification. A reduction from generation to an ‚ÄúIs-It-Valid?‚Äù classifier shows that well-calibrated, cross-entropy-trained base models must still output invalid strings. Post-training benchmarks that grade answers as simply right or wrong then reward bluffing, keeping hallucinations alive unless scoring rules change.
    </p>
  </div>

  <div class="bg-slate-900 text-slate-100 border border-slate-800 rounded-lg p-5 space-y-3">
    <div class="flex items-center gap-2">
      <span class="text-lg">üß≠</span>
      <h3 class="text-sm font-semibold tracking-wide uppercase text-slate-200">Executive quick take</h3>
    </div>
    <p class="text-sm text-slate-200 leading-relaxed">
      Base models minimize cross-entropy, become calibrated, and therefore inherit a lower bound on errors that looks like misclassification risk. Post-training then trains them to ‚Äúguess and hope‚Äù because most evaluations treat blanks the same as wrong answers. Halting hallucinations requires changing grading incentives, not only stacking more tools on top.
    </p>
    <ul class="list-disc ml-5 text-sm text-slate-300 space-y-1">
      <li><strong>Watch calibration:</strong> large |Œ¥| gaps signal the model is no longer near a cross-entropy optimum (and may be bluffing even more).</li>
      <li><strong>Singleton mass matters:</strong> the fraction of once-seen facts bounds how often base models will miss those facts.</li>
      <li><strong>Align scoring:</strong> add penalties for wrong guesses or explicit confidence targets so abstention beats fabrication.</li>
    </ul>
  </div>

  <div class="grid md:grid-cols-2 gap-4">
    <div class="bg-cyan-50 border border-cyan-200 rounded-lg p-4 space-y-2">
      <h3 class="text-sm font-semibold text-cyan-900">IIV reduction in plain language</h3>
      <p class="text-xs text-cyan-800">Treat ‚ÄúIs this response valid?‚Äù as a binary classifier. If the classifier still mislabels some strings, the corresponding language model will generate those mistakes at least twice as often. Calibration forces the model to behave like this classifier, so residual IIV error shows up as hallucinations.</p>
    </div>
    <div class="bg-cyan-50 border border-cyan-200 rounded-lg p-4 space-y-2">
      <h3 class="text-sm font-semibold text-cyan-900">Singleton rate intuition</h3>
      <p class="text-xs text-cyan-800">Facts that appear only once in pre-training behave like ‚Äúunseen‚Äù events. Good-Turing style estimates imply a base hallucination rate at least as large as that singleton fraction‚Äîno clever decoding avoids it without extra data or abstention.</p>
    </div>
    <div class="bg-cyan-50 border border-cyan-200 rounded-lg p-4 space-y-2">
      <h3 class="text-sm font-semibold text-cyan-900">Why scoring rules matter</h3>
      <p class="text-xs text-cyan-800">Most benchmarks give 1 point for correct answers and 0 for either abstaining or being wrong. Rational models maximise expected score by guessing, so post-training makes them confidently fabricate. Tweaking the payoff so abstention beats low-confidence guesses flips that incentive.</p>
    </div>
  </div>

  <div class="grid md:grid-cols-3 gap-4">
    <div class="bg-white border border-gray-200 rounded-lg p-4 space-y-2">
      <h3 class="text-sm font-semibold text-gray-900">Key insight</h3>
      <p class="text-xs text-gray-600">Hallucination is a statistical inevitability for calibrated base models: misclassification risk in the induced validity task lower-bounds the model‚Äôs error rate.</p>
    </div>
    <div class="bg-white border border-gray-200 rounded-lg p-4 space-y-2">
      <h3 class="text-sm font-semibold text-gray-900">Method</h3>
      <p class="text-xs text-gray-600">Reductions from generative modelling to binary classification plus learning-theory lower bounds (singleton mass, VC dimension) and an evaluation survey showing binary grading dominates benchmarks.</p>
    </div>
    <div class="bg-white border border-gray-200 rounded-lg p-4 space-y-2">
      <h3 class="text-sm font-semibold text-gray-900">Implication</h3>
      <p class="text-xs text-gray-600">Reducing hallucinations demands socio-technical change: gather more evidence for sparse facts or modify grading so uncertainty is rewarded instead of punished.</p>
    </div>
  </div>

  <div class="bg-white border border-gray-200 rounded-lg p-5 space-y-3">
    <h3 class="text-sm font-semibold text-gray-900">üß™ Evidence</h3>
    <ul class="list-disc ml-5 text-sm text-gray-700 space-y-1">
      <li><strong>Theorem 1:</strong> generative error ‚â• 2 √ó IIV misclassification ‚àí calibration gap, making calibration and error tightly linked.</li>
      <li><strong>Theorem 2:</strong> in the arbitrary-facts setting, error is bounded below by the singleton rate minus small constants‚ÄîGood-Turing intuition formalised.</li>
      <li><strong>Theorem 3 / Corollary 2:</strong> even simple multiple-choice setups force ‚â•50% error for coarse models like trigrams, highlighting representational limits.</li>
      <li><strong>Observation 1 + meta-evaluation:</strong> surveying popular benchmarks shows binary scoring dominates, so abstention is never optimal.</li>
      <li><strong>Calibration argument:</strong> cross-entropy minima imply small Œ¥; deviating requires leaving the optimum, explaining why post-training breaks calibration.</li>
    </ul>
  </div>

  <div class="bg-amber-50 border border-amber-200 rounded-lg p-5 space-y-2">
    <h3 class="text-sm font-semibold text-amber-900">üî≠ For your roadmap</h3>
    <ul class="list-disc ml-5 text-sm text-amber-800 space-y-1">
      <li>Track calibration (e.g., empirical Œ¥) across releases; large shifts flag post-training that may incentivise bluffing.</li>
      <li>Measure singleton-like coverage in your knowledge store so you know where base models will inevitably guess.</li>
      <li>Experiment with scoring rules that award fractional credit for abstention and bake them into eval harnesses.</li>
      <li>Expose confidence targets to annotators and end-users so abstention is expected behaviour when evidence is thin.</li>
      <li>Log evaluation outcomes by scoring rule to demonstrate the trade-off between breadth and consistency that the paper formalises.</li>
    </ul>
  </div>
</div>
