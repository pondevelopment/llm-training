<div class="paper-overview-card space-y-6">
  <section class="panel panel-info panel-emphasis p-5 space-y-4">
    <div class="flex flex-wrap md:flex-nowrap items-start justify-between gap-4">
      <div class="md:max-w-3xl space-y-1">
        <h2 class="text-xl font-semibold text-heading">Why Language Models Hallucinate</h2>
        <p class="text-sm panel-muted">Adam Tauman Kalai &bull; Ofir Nachum &bull; Santosh S. Vempala &bull; Edwin Zhang &bull; arXiv (2025-09-04)</p>
      </div>
      <a href="https://cdn.openai.com/pdf/d04913be-3f6f-4d2b-b283-ff432ef4aaa5/why-language-models-hallucinate.pdf" target="_blank" rel="noopener" class="btn-soft text-xs font-semibold" data-accent="foundations">
        <span>View paper</span>
        <span aria-hidden="true">&#8599;</span>
      </a>
    </div>
    <p class="text-sm leading-relaxed panel-muted">
      Kalai, Nachum, Vempala, and Zhang reduce hallucinations to a binary classification problem. If a calibrated base model cannot perfectly separate valid from invalid strings, it must still produce those mistakes twice as often. Post-training then rewards guessing because mainstream benchmarks give zero credit for abstaining.
    </p>
    <div class="panel panel-neutral-soft p-3 space-y-1 text-xs">
      <p class="font-semibold text-heading">Plain-language explainer</p>
      <p class="panel-muted">Imagine grading every answer with only “right” or “wrong.” If the model can’t tell a false fact from a true one, it learns to guess with confidence. The authors show that’s exactly what today’s training and evaluation loops encourage.</p>
    </div>
  </section>

  <section class="panel panel-neutral panel-emphasis p-5 space-y-3">
    <header class="flex items-center gap-2">
      <span aria-hidden="true" class="text-lg">🧭</span>
      <h3 class="text-sm font-semibold tracking-wide uppercase text-heading">Executive quick take</h3>
    </header>
    <p class="text-sm leading-relaxed text-body">Cross-entropy makes base models calibrated, but that calibration inherits the misclassification risk of the induced validity classifier. Reinforcement learning and instruction tuning then optimise for benchmarks that refuse to accept “I don’t know,” so the system learns to bluff.</p>
    <ul class="list-disc ml-5 text-sm panel-muted space-y-1">
      <li><strong>Calibration watch:</strong> large |δ| gaps signal post-training drift from the cross-entropy optimum.</li>
      <li><strong>Singleton mass:</strong> facts seen once in pretraining set an irreducible floor on hallucinations.</li>
      <li><strong>Scoring alignment:</strong> when benchmarks award nothing for abstaining, guessing stays rational.</li>
    </ul>
  </section>

  <section class="panel panel-success p-5 space-y-3">
    <h3 class="text-sm font-semibold text-heading">Business relevance</h3>
    <div class="panel panel-neutral-soft p-3 space-y-1 text-xs">
      <p class="font-semibold text-heading">Derivative example: scoring-rule rollout</p>
      <p class="panel-muted">A healthcare knowledge base squad restates its QA suite with explicit abstention bonuses, logging when the assistant defers and using the deltas in release reviews to prove mitigations really curb hallucinations.</p>
    </div>
    <ul class="list-disc ml-5 text-sm text-body space-y-1">
      <li><strong>Product risk:</strong> instrument calibration and abstention UX so low-confidence turns fall back to tools or humans.</li>
      <li><strong>Evaluation policy:</strong> switch binary leaderboards to penalty scoring or confidence targets to reward uncertainty.</li>
      <li><strong>Governance:</strong> report calibration error and singleton coverage to substantiate claims about hallucination rates.</li>
      <li><strong>Training spend:</strong> invest in evidence collection for sparse facts instead of only tweaking decoding or rewards.</li>
    </ul>
  </section>

  <section class="panel panel-accent p-5 space-y-3">
    <h3 class="text-sm font-semibold text-heading">Mechanics explained</h3>
    <div class="grid md:grid-cols-3 gap-4">
      <article class="panel panel-neutral-soft p-4 space-y-2">
        <h4 class="text-sm font-semibold text-heading">IIV reduction</h4>
        <p class="text-sm panel-muted">Treat “Is this response valid?” as a classifier. If it still mislabels some strings, a calibrated generator must output those mistakes at least twice as often.</p>
      </article>
      <article class="panel panel-neutral-soft p-4 space-y-2">
        <h4 class="text-sm font-semibold text-heading">Singleton rate</h4>
        <p class="text-sm panel-muted">Good–Turing estimates imply that once-seen facts set the hallucination floor. A 20% singleton share means ≥20% inevitable misses.</p>
      </article>
      <article class="panel panel-neutral-soft p-4 space-y-2">
        <h4 class="text-sm font-semibold text-heading">Scoring rules</h4>
        <p class="text-sm panel-muted">Most leaderboards give 1 point for correct, 0 for blank or wrong. Guessing wins unless the evaluation adds explicit penalties or confidence targets.</p>
      </article>
    </div>
  </section>

  <section class="panel panel-neutral p-5 space-y-3">
    <h3 class="text-sm font-semibold text-heading">Paper evidence at a glance</h3>
    <div class="grid md:grid-cols-3 gap-4">
      <article class="panel panel-neutral-soft p-4 space-y-2">
        <h4 class="text-sm font-semibold text-heading">Calibration snapshot</h4>
        <p class="text-sm panel-muted">Figure 2 shows the GPT-4 base model tightly calibrated while the RLHF-tuned variant piles mass at 0% and 100%, inflating |δ|.</p>
        <p class="text-xs panel-muted">Kalai et al., 2025, Fig. 2.</p>
      </article>
      <article class="panel panel-neutral-soft p-4 space-y-2">
        <h4 class="text-sm font-semibold text-heading">Singleton floor</h4>
        <p class="text-sm panel-muted">Theorem 2 formalises that singletons impose a hard floor: if 20% of prompts appear once, hallucinations stay at ≥20% without fresh evidence.</p>
        <p class="text-xs panel-muted">Section 3.3, arbitrary facts bound.</p>
      </article>
      <article class="panel panel-neutral-soft p-4 space-y-2">
        <h4 class="text-sm font-semibold text-heading">Leaderboard survey</h4>
        <p class="text-sm panel-muted">Appendix F reviews ten mainstream benchmarks and finds only WildBench grants partial credit for “I don’t know,” so guessing still wins.</p>
        <p class="text-xs panel-muted">Appendix F, benchmark analysis.</p>
      </article>
    </div>
  </section>

  <section class="panel panel-success p-5 space-y-3">
    <h3 class="text-sm font-semibold text-heading">Paper-backed mitigation levers</h3>
    <ul class="list-disc ml-5 text-sm text-body space-y-1">
      <li><strong>Confidence-aware scoring:</strong> retrofit existing benchmarks with explicit penalties so abstention beats low-confidence guesses.</li>
      <li><strong>IIV classifier prompts:</strong> insert an “Is it valid?” pass before answering so uncertain items can defer cleanly.</li>
      <li><strong>Evidence first:</strong> use retrieval or tools to refill the singleton gap before answering, shrinking the inevitable error floor.</li>
    </ul>
    <p class="text-xs panel-muted">Sections 4–5 outline the workflows the authors tested.</p>
  </section>

  <section class="panel panel-neutral p-5 space-y-3">
    <h3 class="text-sm font-semibold text-heading">Evidence</h3>
    <ul class="list-disc ml-5 text-sm panel-muted space-y-1">
      <li><strong>Theorem 1:</strong> generative error ≥ 2 × IIV misclassification − calibration gap.</li>
      <li><strong>Theorem 2:</strong> the singleton rate minus small constants lower-bounds error in the arbitrary-facts setting.</li>
      <li><strong>Theorem 3 / Corollary 2:</strong> coarse models can fail on ≥50% of multiple-choice items.</li>
      <li><strong>Observation 1 + survey:</strong> binary scoring dominates mainstream leaderboards, rewarding bluffing.</li>
      <li><strong>Calibration argument:</strong> cross-entropy minima admit only small |δ|; deviating requires leaving the optimum.</li>
    </ul>
  </section>

  <section class="panel panel-warning p-5 space-y-2">
    <header class="flex items-center gap-2">
      <span aria-hidden="true" class="text-lg">🔭</span>
      <h3 class="text-sm font-semibold text-heading">Forward-looking roadmap</h3>
    </header>
    <ul class="list-disc ml-5 text-sm text-body space-y-1">
      <li>Track calibration (for example empirical |δ|) across releases; sharp shifts flag post-training that may be encouraging bluffing.</li>
      <li>Measure singleton-style coverage in your knowledge bases so you know where base models must guess.</li>
      <li>Adopt scoring rules that award partial credit for abstention and bake them into evaluation harnesses.</li>
      <li>Expose confidence targets to annotators and end-users so abstention is expected when evidence is thin.</li>
      <li>Log outcomes by scoring rule to document the breadth versus consistency trade-off the paper formalises.</li>
    </ul>
  </section>
</div>
