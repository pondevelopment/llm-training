<div class="paper-overview-card space-y-6">
  <section class="rounded-xl border border-indigo-200 dark:border-indigo-500/40 bg-indigo-50 dark:bg-indigo-900/25 p-5 space-y-3 transition-colors">
    <header class="flex flex-wrap md:flex-nowrap items-start justify-between gap-4">
      <div class="md:max-w-3xl space-y-1">
        <h2 class="text-xl font-semibold text-indigo-900 dark:text-indigo-100">Rethinking Chunk Size for Long-Document Retrieval</h2>
        <p class="text-sm text-indigo-700 dark:text-indigo-200">Sinchana Ramakanth Bhat, Max Rudat, Jannis Spiekermann, Nicolas Flores-Herr â€¢ arXiv:2505.21700 (Jun 2025)</p>
      </div>
      <a href="https://arxiv.org/abs/2505.21700" target="_blank" rel="noopener" class="inline-flex items-center gap-2 px-3 py-1.5 rounded-md text-xs font-medium bg-white/90 dark:bg-indigo-950/40 border border-indigo-200 dark:border-indigo-500/40 text-indigo-700 dark:text-indigo-100 hover:bg-indigo-100/70 dark:hover:bg-indigo-900/40 transition-colors">
        <span>View paper</span>
        <span aria-hidden="true" class="text-sm leading-none">â†—</span>
      </a>
    </header>
    <p class="text-sm text-indigo-800 dark:text-indigo-100 leading-relaxed">
      Fixed-size chunking is still the default in RAG, yet the best window shifts wildly across corpora. This study sweeps six QA datasets with Stella and Snowflake embedders, showing how answer locality, document length, and model context limits drive the sweet spot from 64 to 1024 tokensâ€”complete with open-source scripts to rerun the ablations.
    </p>
  </section>

  <section class="bg-slate-900/95 text-slate-100 border border-slate-800 rounded-xl p-5 space-y-3 shadow-inner">
    <div class="flex items-center gap-2">
      <span class="text-lg">ðŸ§­</span>
      <h3 class="text-sm font-semibold tracking-wide uppercase text-slate-200">Executive quick take</h3>
    </div>
    <p class="text-sm text-slate-200 leading-relaxed">
      Chunking is not one-size-fits-all. Short, factoid datasets peak at 64â€“128 token windows; long or technical answers need 512â€“1024 tokens; decoder-style embedders (Stella) love big chunks, while encoder models (Snowflake) stay sharp with small ones. Treat chunk size as a tunable hyperparameter tied to corpus structure and retriever bias.
    </p>
    <ul class="list-disc ml-5 text-sm text-slate-300 space-y-1">
      <li><strong>Know your answers:</strong> Tight spans (e.g., SQuAD) flourish with compact windows; dispersed narratives (NarrativeQA) demand wider context.</li>
      <li><strong>Mind the embedder:</strong> Long-context models exploit large chunks; 8k-token encoders saturate or degrade past 256 tokens.</li>
      <li><strong>Instrument evaluation:</strong> Track recall@k shifts per dataset slice when you alter chunk size or overlap.</li>
    </ul>
  </section>

  <section class="bg-emerald-50 dark:bg-emerald-900/20 border border-emerald-200 dark:border-emerald-500/40 rounded-xl p-5 space-y-3 transition-colors">
    <h3 class="text-sm font-semibold text-emerald-900 dark:text-emerald-100">ðŸ’¼ Business relevance</h3>
    <ul class="list-disc ml-5 text-sm text-emerald-800 dark:text-emerald-100 space-y-1">
      <li><strong>RAG onboarding:</strong> Treat chunk size as a launch checklist item; run sweeps before exposing new corpora to analysts or customers.</li>
      <li><strong>Latency and cost:</strong> Larger windows inflate index storage and retrieval time; factor the trade-offs into infrastructure budgets and SLAs.</li>
      <li><strong>Vendor alignment:</strong> Embedder-specific sweet spots mean procurement should include evaluation time when switching providers or model families.</li>
    </ul>
    <div class="bg-card/90 dark:bg-emerald-950/40 border border-emerald-200 dark:border-emerald-500/40 rounded-md p-3 mt-3 space-y-1 text-xs text-emerald-900 dark:text-emerald-100 transition-colors">
      <p class="font-semibold">Derivative example (chunk sweep in production)</p>
      <p class="text-emerald-800 dark:text-emerald-100">An in-house legal research team can clone the authors sweep, testing 128 versus 512 token chunks on past matters to set the default window before onboarding attorneys.</p>
    </div>
  </section>

  <section class="grid md:grid-cols-3 gap-4">
    <article class="bg-cyan-50 dark:bg-cyan-900/20 border border-cyan-200 dark:border-cyan-500/40 rounded-lg p-4 space-y-2 transition-colors">
      <h3 class="text-sm font-semibold text-cyan-900 dark:text-cyan-100">Chunk-size heuristics from the paper</h3>
      <p class="text-xs text-cyan-800 dark:text-cyan-100 leading-relaxed">Use answer length and locality as a first-pass heuristic. If answers average &lt;10 tokens and stay within one paragraph, start at 64â€“128 tokens. If answers regularly span multiple paragraphs or technical steps, jump to 512â€“1024 tokens.</p>
      <ul class="list-disc ml-4 text-[11px] text-cyan-700 dark:text-cyan-100 space-y-1">
        <li>NarrativeQA recall@1 triples when moving from 64 â†’ 1024 tokens.</li>
        <li>TechQA improves from 5% â†’ 71% recall@1 by scaling to 512â€“1024 tokens.</li>
        <li>SQuAD drops 20 points when oversized to 1024 tokensâ€”too much noise.</li>
      </ul>
    </article>
    <article class="bg-cyan-50 dark:bg-cyan-900/20 border border-cyan-200 dark:border-cyan-500/40 rounded-lg p-4 space-y-2 transition-colors">
      <h3 class="text-sm font-semibold text-cyan-900 dark:text-cyan-100">Embedding-model sensitivities</h3>
      <p class="text-xs text-cyan-800 dark:text-cyan-100 leading-relaxed">The Stella embedder (decoder lineage, 130k context) thrives on large chunks, while Snowflake (encoder, 8k context) peaks with compact windows. Chunk decisions should match the embedderâ€™s pretraining context length and positional encoding.</p>
      <ul class="list-disc ml-4 text-[11px] text-cyan-700 dark:text-cyan-100 space-y-1">
        <li>Stella gains 5â€“8% recall on long-document sets at 512â€“1024 tokens.</li>
        <li>Snowflake leads on SQuAD/COVID-QA at 64â€“128 tokens but lags on 1024.</li>
        <li>Mixing embedders may require dataset-specific chunk configs.</li>
      </ul>
    </article>
    <article class="bg-cyan-50 dark:bg-cyan-900/20 border border-cyan-200 dark:border-cyan-500/40 rounded-lg p-4 space-y-2 transition-colors">
      <h3 class="text-sm font-semibold text-cyan-900 dark:text-cyan-100">What counts as a document</h3>
      <p class="text-xs text-cyan-800 dark:text-cyan-100 leading-relaxed">The paper treats each chunked window as the retrieval unit. Long reports are sliced into overlapping passages, while stitched corpora combine multiple QA pairs into a single pseudo-document.</p>
      <ul class="list-disc ml-4 text-[11px] text-cyan-700 dark:text-cyan-100 space-y-1">
        <li>Original PDFs average 6â€“10k tokens; windows roll every 32â€“64 tokens to preserve context.</li>
        <li>Overlap is capped at 50% to avoid bloating the index while keeping answer spans intact.</li>
        <li>When documents are stitched, tracking provenance per chunk becomes essential for evaluation.</li>
      </ul>
    </article>
  </section>

  <section class="bg-cyan-50 dark:bg-cyan-900/20 border border-cyan-200 dark:border-cyan-500/40 rounded-lg p-4 space-y-2 transition-colors">
    <h3 class="text-sm font-semibold text-cyan-900 dark:text-cyan-100">What Recall@<em>k</em> measures</h3>
    <p class="text-xs text-cyan-800 dark:text-cyan-100 leading-relaxed">For each question, the retriever ranks all chunks and we check whether the chunk containing the ground-truth answer appears in the top <em>k</em> of that ranking.</p>
    <ul class="list-disc ml-4 text-[11px] text-cyan-700 dark:text-cyan-100 space-y-1">
      <li><strong>Recall@1:</strong> % of questions where the correct chunk was the very first result.</li>
      <li><strong>Recall@5:</strong> % of questions where the correct chunk was somewhere in the top 5 results.</li>
      <li>Higher recall is betterâ€”more retrieved chunks include the answer span.</li>
      <li>The paper reports Recall@1 primarily (what the interactive shows) and includes Recall@2-5 for completeness.</li>
    </ul>
  </section>

  <section class="grid md:grid-cols-3 gap-4">
    <article class="bg-card border border-subtle rounded-lg p-4 space-y-2 transition-colors">
      <h3 class="text-sm font-semibold text-heading">Key insight</h3>
      <p class="text-xs text-secondary">Chunk size mediates the recall vs. noise trade-off; the optimal window depends jointly on answer locality and the retrieverâ€™s ability to leverage long context.</p>
    </article>
    <article class="bg-card border border-subtle rounded-lg p-4 space-y-2 transition-colors">
      <h3 class="text-sm font-semibold text-heading">Method</h3>
      <p class="text-xs text-secondary">Split each dataset into fixed token windows (64â€“1024), index chunks with Stella and Snowflake, and measure recall@k via string matches between retrieved chunks and ground-truth answers.</p>
    </article>
    <article class="bg-card border border-subtle rounded-lg p-4 space-y-2 transition-colors">
      <h3 class="text-sm font-semibold text-heading">Implication</h3>
      <p class="text-xs text-secondary">Treat chunking as a benchmarked decision: log recall vs. chunk size, audit per dataset slice, and tune per-embedder defaults instead of shipping a single global window.</p>
    </article>
  </section>

  <section class="bg-card border border-subtle rounded-xl p-5 space-y-3 transition-colors">
    <h3 class="text-sm font-semibold text-heading">ðŸ§ª Experiments &amp; Evidence</h3>
    <ul class="list-disc ml-5 text-sm text-secondary space-y-1">
      <li><strong>Long vs. short corpora:</strong> NarrativeQA recall@1 climbs from 4.2% (64 tokens) to 10.7% (1024); SQuAD falls from 64.2% (64) to 38.6% (1024).</li>
      <li><strong>Model bias:</strong> Snowflake peaks at 54.2% recall@1 on COVID-QA with 1024 tokens, while Stella peaks at 52.1% with 64-token chunks.</li>
      <li><strong>Open-source assets:</strong> LlamaIndex pipeline + scripts released at <a href="https://github.com/fraunhofer-iais/chunking-strategies" class="text-link hover:underline" target="_blank" rel="noopener">fraunhofer-iais/chunking-strategies</a>.</li>
      <li><strong>Stitched corpora:</strong> For scarce long-form sets, the authors stitch QA pairs to 50k-character docs, highlighting a need for native long-document benchmarks.</li>
    </ul>
  </section>

  <section class="bg-amber-50 dark:bg-amber-900/25 border border-amber-200 dark:border-amber-500/40 rounded-xl p-5 space-y-2 transition-colors">
    <h3 class="text-sm font-semibold text-amber-900 dark:text-amber-100">ðŸ”­ For your roadmap</h3>
    <ul class="list-disc ml-5 text-sm text-amber-800 dark:text-amber-100 space-y-1">
      <li>Instrument chunk-size sweeps (recall@k, latency, overlap cost) for every new corpus.</li>
      <li>Align chunk windows to embedder context limits and positional encodings.</li>
      <li>Add semantic relevance checks (beyond string match) before shipping larger chunks.</li>
      <li>Contribute long-document QA datasets with ground-truth spans to reduce stitched proxies.</li>
    </ul>
  </section>
</div>
