<section class="space-y-5">
  <section class="panel panel-info p-4 space-y-4">
    <div class="flex items-center justify-between gap-4">
      <div class="flex-1 min-w-0">
        <h2 class="text-xl font-semibold text-heading">Pun Unintended: LLMs and the Illusion of Humor Understanding</h2>
        <p class="text-sm panel-muted">Alessandro Zangari, Matteo Marcuzzo, Andrea Albarelli, Mohammad Taher Pilehvar, Jose Camacho-Collados ‚Ä¢ arXiv:2509.12158 (Sep 2025)</p>
      </div>
      <a href="https://arxiv.org/abs/2509.12158" target="_blank" rel="noopener" class="btn-soft text-xs font-semibold flex-shrink-0" data-accent="foundations">
        <span>View paper</span>
        <span aria-hidden="true">‚Üó</span>
      </a>
    </div>
    <p class="text-sm leading-relaxed panel-muted">
      The authors revisit pun detection with three fresh datasets‚ÄîPunnyPattern, PunBreak, and a newly annotated pun set‚Äîto show that top LLMs lean on surface cues. Accuracy plunges when common pun templates are decoys or when pun words are swapped for synonyms or nonsense terms, and the explanation rationales reveal missing context and wrong senses.
    </p>
    <div class="panel panel-neutral-soft p-3 space-y-1 text-xs">
      <p class="font-semibold text-heading">Plain-language explainer</p>
      <p class="panel-muted">Imagine a joke detector that says "That's funny!" every time it sees "Why did the chicken cross the road?" even when the punchline makes no sense. The authors prove LLMs do exactly that with puns‚Äîthey memorize templates instead of understanding wordplay.</p>
    </div>
  </section>

  <section class="panel panel-neutral p-5 space-y-3">
    <header>
      <h3 class="flex items-center gap-2 text-heading">
        <span class="text-lg">üß≠</span>
        <span class="text-sm font-semibold tracking-wide uppercase">Executive quick take</span>
      </h3>
    </header>
    <p class="text-sm text-body leading-relaxed">
      LLMs ace legacy pun benchmarks but fail when you strip away memorized patterns. On the new PunnyPattern set, F1 drops ~10 points and precision plunges up to 23 points because models rubber-stamp anything shaped like a pun. PunBreak shows the brittleness: replace a pun word with a homophone and accuracy collapses near coin-flip for every model.
    </p>
    <ul class="list-disc ml-5 text-sm panel-muted space-y-1">
      <li><strong>Bias exposure:</strong> High recall + low precision means the models chase templates ("Old X never die‚Ä¶") rather than judging meaning.</li>
      <li><strong>Rationales help‚Äîbut not enough:</strong> Forcing models to name pun pairs slightly boosts F1, yet manual review shows hallucinated senses and context gaps.</li>
      <li><strong>Benchmark takeaway:</strong> Humor understanding needs adversarial checks, phonetic awareness, and structured rationale evaluation.</li>
    </ul>
  </section>

  <section class="panel panel-success p-5 space-y-2">
    <header>
      <h3 class="text-sm font-semibold text-heading">üíº Business relevance</h3>
    </header>
    <ul class="list-disc ml-5 text-sm text-body space-y-1">
      <li><strong>Brand voice & marketing:</strong> Copy teams lean on clever wordplay; a detector that confuses faux-puns with real jokes will surface the wrong snippets or greenlight awkward ads.</li>
      <li><strong>Moderation & safety:</strong> Puns can mask edgy or offensive content‚Äîpoor precision means noisy escalations, while poor recall misses risky posts.</li>
      <li><strong>User experience:</strong> Humor tutors, creative assistants, or social bots that misread pun structure feel tone-deaf; PunBreak-style negatives should be in QA before launch.</li>
      <li><strong>Model evaluation:</strong> Precision/recall deltas here are a warning sign for any ambiguity-heavy feature (sarcasm, idioms, irony). Treat pun benchmarks as part of a broader linguistic robustness suite.</li>
    </ul>
    <div class="panel panel-neutral-soft p-3 mt-3 space-y-1 text-xs">
      <p class="font-semibold text-heading">Derivative example (pun-aware moderation drill)</p>
      <p class="panel-muted">A moderation vendor can seed PunBreak negatives into review tooling, measuring precision drops on pun-like spam to decide when to escalate to human linguists.</p>
    </div>
  </section>

  <div class="grid md:grid-cols-2 gap-4">
    <section class="panel panel-info p-4 space-y-2">
      <h3 class="text-sm font-semibold text-heading">Pun types at a glance</h3>
      <p class="text-xs text-body">The study tracks two standard categories: heterographic puns use different written forms (sail/sale), while homographic puns reuse the same token with multiple senses (hit/hit). A pun pair is the (<em>w<sub>p</sub></em>, <em>w<sub>a</sub></em>) that triggers both readings.</p>
            <ul class="list-disc ml-4 text-[11px] panel-muted space-y-1">
        <li><strong>Rationale prompts</strong> force models to output the pun pair plus senses.</li>
        <li><strong>R1</strong> (DeepSeek) and GPT‚Äë4o best identify correct pairs (~1.5/2 words right).</li>
        <li>Mistral struggles: under 1 word of the pair correct on average, often hallucinates senses.</li>
      </ul>
      <div class="panel panel-neutral-soft p-3 text-[11px] space-y-2">
        <p class="font-semibold text-heading">Quick examples</p>
        <ul class="list-disc ml-4 panel-muted space-y-1">
          <li><strong>Heterographic:</strong> "I bought a boat because it was for <em>sale</em>." ‚Üí pun pair (<em>sail</em>, <em>sale</em>); senses: to navigate vs. to purchase cheaply.</li>
          <li><strong>Homographic:</strong> "The math book has too many <em>problems</em>." ‚Üí same token (<em>problems</em>) meaning exercises vs. difficulties.</li>
          <li><strong>Ruined homophone (PunBreak):</strong> swap <em>dragon</em> in "Long fairy tales tend to dra<strong>g on</strong>" with <em>wyvern</em>; humans notice the wordplay disappears, LLMs often still say "pun".</li>
        </ul>
      </div>
    </section>
    </div>
          </ul>
    </section>
    <section class="panel panel-info p-4 space-y-2">
      <h3 class="text-sm font-semibold text-heading">Benchmarks introduced</h3>
      <p class="text-xs text-body"><strong>PunnyPattern</strong> pairs six popular English pun templates with matched non-puns; <strong>PunBreak</strong> swaps the pun word for synonyms, homophones, or random tokens (plus random sentences). Both include 1,200+ examples with pun-pair annotations.</p>
      <ul class="list-disc ml-4 text-[11px] panel-muted space-y-1">
        <li>F1 drops 4‚Äì13 pts on PunnyPattern; precision down 16‚Äì23 pts.</li>
        <li>On PunBreak, non-pun accuracy falls to 0.33‚Äì0.59 (homophone swaps hardest).</li>
        <li>Control sentences score &gt;0.8 accuracy ‚Üí failures stem from pun-like structure, not general bias.</li>
      </ul>
    </section>
  </div>

  <section class="panel panel-info p-4 space-y-2">
    <h3 class="text-sm font-semibold text-heading">How the paper measures success</h3>
    <p class="text-xs text-body">Detection uses F1 plus precision/recall. Rationales are scored with <strong>Pun Pair Agreement (PPA)</strong>: 2 points if both pun word and alternative are correct, 1 point for one match, 0 otherwise (incorrect predictions also score 0). Manual review tags errors into context, pun-pair, word-sense, and sense-similarity issues.</p>
    <ul class="list-disc ml-4 text-[11px] panel-muted space-y-1">
      <li>GPT‚Äë4o leads rationales (PPA ‚âà1.5/2); DeepSeek‚ÄëR1 close behind.</li>
      <li>Most common failure: missing context to justify the claimed senses.</li>
      <li>Pun pair errors expose weak phonetic handling‚Äîmodels force mismatched words into wordplay.</li>
    </ul>
  </section>

  <div class="grid md:grid-cols-3 gap-4">
    <section class="panel panel-neutral p-4 space-y-2">
      <h3 class="text-sm font-semibold text-heading">Key insight</h3>
      <p class="text-xs panel-muted">High scores on legacy pun datasets hide shallow heuristics. Once templates are adversarial or pun words change, every leading LLM over-predicts ‚Äúpun‚Äù.</p>
    </div>
    <section class="panel panel-neutral p-4 space-y-2">
      <h3 class="text-sm font-semibold text-heading">Method</h3>
      <p class="text-xs panel-muted">Prompt seven LLMs with zero-/few-shot, rationale, and reasoning variants; evaluate on PunEval, JOKER, NAP, PunnyPattern, and PunBreak using structured outputs for pun pairs and senses.</p>
    </div>
    <section class="panel panel-neutral p-4 space-y-2">
      <h3 class="text-sm font-semibold text-heading">Implication</h3>
      <p class="text-xs panel-muted">Humor understanding remains a brittle frontier. Build benchmarks that target linguistic traps, collect human-reviewed rationales, and bake phonetic reasoning into retrieval/training loops.</p>
    </div>
  </div>

  <section class="panel panel-neutral p-5 space-y-3">
    <header>
      <h3 class="text-sm font-semibold text-heading">üß™ Experiments & Evidence</h3>
    </header>
    <ul class="list-disc ml-5 text-sm text-body space-y-1">
      <li><strong>PunnyPattern stress-test:</strong> precision plunges by up to 23 pts across models, revealing template overfitting.</li>
      <li><strong>PunBreak perturbations:</strong> homophone swaps drive accuracy to 0.33‚Äì0.45 even for GPT‚Äë4o.</li>
      <li><strong>Rationale scoring:</strong> PPA highlights hallucinated senses; manual audit (80 samples/model) shows context and phonetic errors dominate.</li>
      <li><strong>Open assets:</strong> Datasets + annotation guidelines released at <a href="https://github.com/alezanga/punintended" class="text-indigo-600 hover:underline" target="_blank" rel="noopener">alezanga/punintended</a>.</li>
    </ul>
  </div>

  <section class="panel panel-warning p-5 space-y-2">
    <header>
      <h3 class="text-sm font-semibold text-heading">üî≠ For your roadmap</h3>
    </header>
    <ul class="list-disc ml-5 text-sm text-body space-y-1">
      <li>Run pun-detection smoke tests on your safety or moderation stack; log precision vs. recall per template.</li>
      <li>Augment training with ruined-pun negatives (synonym + homophone swaps) to discourage sycophantic ‚Äúpun‚Äù predictions.</li>
      <li>Instrument rationale scoring (PPA) to flag hallucinated senses before shipping explanations to users.</li>
      <li>Invest in phonetic features or auxiliary speech models when handling wordplay-heavy domains (ads, entertainment, tutoring).</li>
    </ul>
  </section>
</section>
