<div class="space-y-5">
  <div class="bg-indigo-50 border border-indigo-200 rounded-lg p-4">
    <div class="flex flex-wrap md:flex-nowrap items-start justify-between gap-4">
      <div class="md:max-w-3xl">
        <h2 class="text-xl font-semibold text-indigo-900">Mathematical Research with GPT-5: a Malliavin–Stein Experiment</h2>
        <p class="text-sm text-indigo-700">Charles-Philippe Diez, Luís da Maia, Ivan Nourdin • arXiv:2509.03065 (Sep 2025)</p>
      </div>
      <a href="https://arxiv.org/abs/2509.03065" target="_blank" rel="noopener" class="inline-flex items-center gap-2 px-3 py-1.5 rounded-md text-xs font-medium bg-white border border-indigo-200 text-indigo-700 hover:bg-indigo-100">
        <span>View paper</span>
        <span aria-hidden="true" class="text-sm leading-none">↗</span>
      </a>
    </div>
    <p class="mt-3 text-sm text-indigo-800 leading-relaxed">
      Motivated by GPT‑5’s viral “breakthrough” in convex optimization, the authors ran their own controlled test in Malliavin–Stein theory. With GPT‑5 as a brainstorming partner and rigorous human verification, they turned a qualitative fourth-moment theorem into explicit total-variation rates for mixed Gaussian chaoses, then pushed the argument to Poisson settings—derivations that had not appeared in the literature.
    </p>
  </div>

  <div class="bg-slate-900 text-slate-100 border border-slate-800 rounded-lg p-5 space-y-3">
    <div class="flex items-center gap-2">
      <span class="text-lg">🧭</span>
      <h3 class="text-sm font-semibold tracking-wide uppercase text-slate-200">Executive quick take</h3>
    </div>
    <p class="text-sm text-slate-200 leading-relaxed">
      GPT‑5 can meaningfully accelerate mathematical research when experts stay in the loop. It proposed the key Malliavin–Stein reductions, but humans supplied the proofs, counterexamples, and polish. The headline result is a quantitative two-chaos fourth-moment bound with √κ₄ rates, plus a Poisson analogue with nearly sharp conditions.
    </p>
    <ul class="list-disc ml-5 text-sm text-slate-300 space-y-1">
      <li><strong>AI as co-pilot:</strong> GPT‑5 drafted lemmas, checked contractions, and suggested Poisson parallels; mathematicians validated every step.</li>
      <li><strong>New theorems:</strong> Total-variation ≤ √(6 κ₄(Z)) for Z = Iₚ(f)+I_q(g) (p odd, q even), and a Poisson version with explicit rate.</li>
      <li><strong>Workflow insight:</strong> Transcript shows iterative prompting, counterexample hunting, and the necessity of symbolic sanity checks.</li>
    </ul>
  </div>

  <div class="bg-emerald-50 border border-emerald-200 rounded-lg p-5 space-y-2">
    <h3 class="text-sm font-semibold text-emerald-900">💼 Business relevance</h3>
    <ul class="list-disc ml-5 text-sm text-emerald-800 space-y-1">
      <li><strong>Research productivity:</strong> Quantitative math results often underpin finance, risk, and scientific modelling—AI copilots can shorten discovery cycles when paired with domain experts.</li>
      <li><strong>Audit trails:</strong> The paper’s transcript + verification protocol is a template for regulated environments that must document AI-assisted derivations.</li>
      <li><strong>Talent leverage:</strong> Highlights how senior researchers can steer AI to explore proof space while juniors learn from the dialogue—relevant for R&amp;D orgs.</li>
      <li><strong>Risk management:</strong> Emphasizes rigorous human review before accepting AI proofs, a stance aligned with emerging governance policies.</li>
    </ul>
    <div class="bg-white border border-emerald-200 rounded-md p-3 mt-3 space-y-1 text-xs text-emerald-900">
      <p class="font-semibold">Derivative example (regulated proof notebook)</p>
      <p class="text-emerald-800">A quantitative risk lab can pair GPT-5 proof sessions with the paper checklist, storing prompts, outputs, and human sign-off in a shared notebook to satisfy model risk audits while accelerating derivations.</p>
    </div>
  </div>

  <div class="grid md:grid-cols-2 gap-4">
    <div class="bg-cyan-50 border border-cyan-200 rounded-lg p-4 space-y-2">
      <h3 class="text-sm font-semibold text-cyan-900">What changed in Malliavin–Stein?</h3>
      <p class="text-xs text-cyan-800">Classical fourth-moment theorems certify convergence but rarely give explicit rates. GPT‑5 helped derive</p>
      <p class="text-xs text-cyan-800"><strong>d<sub>TV</sub>(Z,𝓝(0,1)) ≤ √(6 κ₄(Z))</strong> for Z = Iₚ(f)+I_q(g) with opposite parity, tightening the qualitative result of Basse-O’Connor et al.</p>
      <ul class="list-disc ml-4 text-[11px] text-cyan-700 space-y-1">
        <li>Fourth cumulant κ₄(Z) = E[Z⁴] − 3 acts as the convergence witness.</li>
        <li>Proof hinges on careful control of Malliavin derivatives and mixed contractions.</li>
        <li>Poisson analogue retains the structure but demands an extra “odd moment” condition—shown to be near optimal.</li>
      </ul>
    </div>
    <div class="bg-cyan-50 border border-cyan-200 rounded-lg p-4 space-y-2">
      <h3 class="text-sm font-semibold text-cyan-900">How GPT‑5 was used</h3>
      <p class="text-xs text-cyan-800">Researchers supplied definitions, goals, and partial derivations; GPT‑5 suggested calculations, expansions, and Poisson parallels. Every AI step was checked, rewritten, or discarded.</p>
      <ul class="list-disc ml-4 text-[11px] text-cyan-700 space-y-1">
        <li>Transcripts (Appendix) document prompt templates, corrections, and retries.</li>
        <li>When GPT‑5 hallucinated statements, authors demanded explicit references or corrected algebra.</li>
        <li>Counterexample search: GPT‑5’s failures highlighted missing assumptions, leading to the “sharpness” discussion.</li>
      </ul>
    </div>
  </div>

  <div class="bg-white border border-gray-200 rounded-lg p-4 space-y-2">
    <h3 class="text-sm font-semibold text-gray-900">Key terminology</h3>
    <ul class="list-disc ml-5 text-xs text-gray-700 space-y-1">
      <li><strong>Fourth cumulant κ₄:</strong> Measures deviation of a zero-mean variable from Gaussianity; vanishes only for the normal law.</li>
      <li><strong>Total variation distance:</strong> Supremum over measurable sets of |P(Z ∈ A) − Φ(A)|—a strong convergence metric.</li>
      <li><strong>Wiener/Poisson chaos:</strong> Orthogonal families generated by multiple stochastic integrals; orders p and q indicate integration depth.</li>
      <li><strong>Malliavin derivative D:</strong> Differential operator on random variables; enables calculus on probability spaces.</li>
    </ul>
  </div>

  <div class="bg-white border border-gray-200 rounded-lg p-5 space-y-3">
    <h3 class="text-sm font-semibold text-gray-900">🧪 Experiments & Evidence</h3>
    <ul class="list-disc ml-5 text-sm text-gray-700 space-y-1">
      <li><strong>Theorem 2.1:</strong> d<sub>TV</sub>(Z, 𝓝) ≤ √(6 κ₄(Z)) for Z = Iₚ(f)+I_q(g), p odd, q even.</li>
      <li><strong>Poisson extension:</strong> Same rate up to an additional parity constraint; counterexample shows necessity.</li>
      <li><strong>Manual verification:</strong> Authors compared AI derivations with classical Malliavin–Stein identities, ensuring no lemma relied on unverified claims.</li>
      <li><strong>Transparency assets:</strong> Full GPT‑5 transcripts + prompt protocol included in appendices.</li>
    </ul>
  </div>

  <div class="bg-amber-50 border border-amber-200 rounded-lg p-5 space-y-2">
    <h3 class="text-sm font-semibold text-amber-900">🔭 For your roadmap</h3>
    <ul class="list-disc ml-5 text-sm text-amber-800 space-y-1">
      <li>Adopt “AI proof notebooks”: store prompts, outputs, and human validations like the authors’ Appendix A/B.</li>
      <li>Build quantitative benchmarks for AI-assisted derivations (rate bounds, counterexamples) rather than only qualitative statements.</li>
      <li>Train teams to interrogate LLM outputs—ask for references, inspect algebra, and demand explicit constants.</li>
      <li>Explore cross-domain extensions: the authors’ workflow could apply to other probabilistic or analytic open problems.</li>
    </ul>
  </div>
</div>
