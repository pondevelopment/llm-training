<div class="space-y-5">
  <div class="panel panel-info p-4 space-y-4">
    <div class="flex flex-wrap md:flex-nowrap items-start justify-between gap-4">
      <div class="space-y-1 md:max-w-3xl">
        <h2 class="text-xl font-semibold text-heading">A Taxonomy of Transcendence</h2>
        <p class="text-sm panel-muted">Natalie Abreu, Edwin Zhang, Eran Malach, Naomi Saphra ‚Ä¢ COLM 2025 (arXiv:2508.17669)</p>
      </div>
      <a href="https://arxiv.org/abs/2508.17669" target="_blank" rel="noopener" class="btn-soft text-xs font-semibold" data-accent="foundations">
        <span>View paper</span>
        <span aria-hidden="true">‚Üó</span>
      </a>
    </div>
    <p class="text-sm panel-muted leading-relaxed">
      The authors formalise when an imitative language model can outperform every individual data source it learns from. They derive three ‚Äútranscendence‚Äù modes‚Äîskill denoising, skill selection, and skill generalisation‚Äîthen prove and test the data diversity conditions that unlock each one using a synthetic knowledge-graph of fictional experts.
    </p>
  </div>

  <div class="panel panel-neutral panel-emphasis p-5 space-y-3">
    <div class="flex items-center gap-2 text-heading">
      <span aria-hidden="true" class="text-lg">üß≠</span>
      <h3 class="text-sm font-semibold tracking-wide uppercase">Executive quick take</h3>
    </div>
    <p class="text-sm text-body leading-relaxed">
      ‚ÄúAnthropomorphising‚Äù a model‚Äîtreating it like a single human mind‚Äîmisses the point: it is trained to mimic a <em>mixture</em> of experts. If your dataset blends uncorrelated errors, differentiated expertise, and compositional examples, the resulting model can denoise, route, and recombine knowledge beyond any single contributor.
    </p>
    <ul class="list-disc ml-5 text-sm panel-muted space-y-1">
      <li><strong>Denoise noise:</strong> Low-temperature decoding concentrates the majority vote when mistakes are independent.</li>
      <li><strong>Select specialists:</strong> Exposure-weighted sampling lets the model surface the right expert per context.</li>
      <li><strong>Compose skills:</strong> Rich multi-hop demonstrations bias the model toward latent structure over memorisation.</li>
    </ul>
  </div>

  <div class="panel panel-success panel-emphasis p-5 space-y-2">
    <h3 class="text-sm font-semibold text-heading">üíº Business relevance</h3>
    <ul class="list-disc ml-5 text-sm text-body space-y-1">
      <li><strong>Data procurement:</strong> Curate contributor pools with complementary coverage instead of maximising volume from a single vendor.</li>
      <li><strong>Safety budgets:</strong> Quantify correlated failure pockets‚Äîdiversity that doesn‚Äôt break correlations won‚Äôt buy you transcendence.</li>
      <li><strong>Agent routing:</strong> The taxonomy mirrors ensemble and MoE deployments; it tells you which data knobs justify that complexity.</li>
    </ul>
    <div class="panel panel-neutral-soft p-3 mt-3 space-y-1 text-xs">
      <p class="font-semibold text-heading">Derivative example (apply the paper‚Äôs setup)</p>
      <p class="panel-muted">A support-automation team could simulate ‚Äúexperts‚Äù from billing, hardware, and policy teams. By varying how often each team writes about topics inside vs outside its remit, product ops can estimate when a unified LLM will denoise (low overlap errors), when it needs routing metadata (selection), and how many cross-domain playbooks are required before the model generalises rare two-hop escalations.</p>
    </div>
  </div>

  <div class="grid md:grid-cols-3 gap-4">
    <div class="panel panel-neutral p-3 space-y-2">
      <h5 class="text-sm font-semibold text-heading">Key insight</h5>
      <p class="text-xs panel-muted">Transcendence is not mystical: it emerges precisely when training data supplies the statistical signals (variance, routing bias, compositional structure) the learner can exploit.</p>
    </div>
    <div class="panel panel-neutral p-3 space-y-2">
      <h5 class="text-sm font-semibold text-heading">Method</h5>
      <p class="text-xs panel-muted">The team derives sufficient conditions for each transcendence mode and instantiates them in controllable knowledge-graph corpora where GPT-2 is finetuned on templated paragraphs from simulated experts.</p>
    </div>
    <div class="panel panel-neutral p-3 space-y-2">
      <h5 class="text-sm font-semibold text-heading">Implication</h5>
      <p class="text-xs panel-muted">Architects can decide which diversity lever to fund‚Äîmore experts, domain-aware sampling, or compositional exemplars‚Äîbased on the failure mode they observe.</p>
    </div>
  </div>

  <div class="grid md:grid-cols-2 gap-4">
    <div class="panel panel-info p-4 space-y-2">
      <h3 class="text-sm font-semibold text-heading">Three modes cheat sheet</h3>
      <ul class="list-disc ml-5 text-xs panel-muted space-y-1">
        <li><strong>Skill denoising:</strong> Shared distribution, independent errors. Decoder temperature ‚Üì returns the modal answer.</li>
        <li><strong>Skill selection:</strong> Experts see different parts of the space. Routing probability g(i|x) rises with actual competence.</li>
        <li><strong>Skill generalisation:</strong> Test set sits off-support; the model must compose latent representations learned from diverse phrasing.</li>
      </ul>
      <p class="text-[11px] panel-muted">The taxonomy extends Zhang et al. (2024) with selection and generalisation grounded in data requirements.</p>
    </div>
    <div class="panel panel-info p-4 space-y-2">
      <h3 class="text-sm font-semibold text-heading">Data diversity levers</h3>
      <ul class="list-disc ml-5 text-xs panel-muted space-y-1">
        <li><strong>Error decorrelation:</strong> More experts with randomised corruptions lower the majority-vote variance.</li>
        <li><strong>Exposure weighting:</strong> Parameter Œ± controls how often specialists talk about their own patch.</li>
        <li><strong>Compositional samples:</strong> Within-expertise two-hop examples act as scaffolding so across-expertise queries become reachable.</li>
      </ul>
      <p class="text-[11px] panel-muted">Treat each lever as an experiment knob when auditing instruction-tuning corpora.</p>
    </div>
  </div>

  <div class="panel panel-neutral p-5 space-y-3">
    <h3 class="text-sm font-semibold text-heading">üß™ Experiments & Evidence</h3>
    <ul class="list-disc ml-5 text-sm panel-muted space-y-1">
      <li><strong>Denoising:</strong> Finetuning GPT-2 on 10M samples from 100 experts with 20% coverage reaches &gt;80% factual accuracy using greedy decoding, beating any expert.</li>
      <li><strong>Selection:</strong> With coverage 0.1 and Œ± ‚â• 0.95, accuracy approaches 100% once the pool spans enough edge clusters (Fig. 5).</li>
      <li><strong>Generalisation:</strong> Across-expertise two-hop accuracy scales linearly with the count of within-expertise scaffolds; paraphrase diversity lifts it above co-occurrence baselines (Fig. 6).</li>
      <li><strong>Baselines:</strong> Relation-majority and direct-connection heuristics lag far behind the finetuned model on held-out multi-hop queries.</li>
    </ul>
  </div>

  <div class="panel panel-warning p-5 space-y-2">
    <h3 class="text-sm font-semibold text-heading">üî≠ For your roadmap</h3>
    <ul class="list-disc ml-5 text-sm text-body space-y-1">
      <li>Instrument instruction datasets to measure error correlation, topical coverage, and compositional depth.</li>
      <li>Layer routing metadata or provenance tags when you suspect ‚Äúselection‚Äù will help more than raw denoising.</li>
      <li>Invest in synthetic or logged multi-hop exemplars before expecting zero-shot cross-domain reasoning.</li>
      <li>Explore ‚Äúskill discovery‚Äù settings‚Äîan open question the authors flag for future transcendence modes.</li>
    </ul>
  </div>
</div>
