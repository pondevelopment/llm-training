<div class="space-y-5">
  <div class="bg-indigo-50 border border-indigo-200 rounded-lg p-4">
    <div class="flex flex-wrap md:flex-nowrap items-start justify-between gap-4">
      <div class="md:max-w-3xl">
        <h2 class="text-xl font-semibold text-indigo-900">Training Compute-Optimal Large Language Models</h2>
        <p class="text-sm text-indigo-700">Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya et al. &bull; arXiv:2203.15556 (Mar 2022)</p>
      </div>
      <a href="https://arxiv.org/abs/2203.15556" target="_blank" rel="noopener" class="inline-flex items-center gap-2 px-3 py-1.5 rounded-md text-xs font-medium bg-white border border-indigo-200 text-indigo-700 hover:bg-indigo-100">
        <span>View paper</span>
        <span aria-hidden="true" class="text-sm leading-none">â†—</span>
      </a>
    </div>
    <p class="mt-3 text-sm text-indigo-800 leading-relaxed">
      DeepMind revisits scaling laws across 400 transformer runs and shows that, for a fixed compute budget, recent mega-models waste FLOPs by undertraining on data. The team proposes the compute-optimal Chinchilla recipe&mdash;grow model parameters and tokens in lockstep&mdash;and proves that a 70B parameter model trained on 1.4T tokens beats much larger peers at lower inference cost.
    </p>
  </div>

  <div class="bg-slate-900 text-slate-100 border border-slate-800 rounded-lg p-5 space-y-3">
    <div class="flex items-center gap-2">
      <span class="text-lg">ðŸ§­</span>
      <h3 class="text-sm font-semibold tracking-wide uppercase text-slate-200">Executive quick take</h3>
    </div>
    <p class="text-sm text-slate-200 leading-relaxed">
      Stop scaling parameters without scaling tokens. The paper finds the optimal frontier when tokens seen roughly equal 20 &times; the parameter count (in billions). GPT-3 and Gopher sat 3&ndash;4&times; off that curve; Chinchilla shows that matching FLOPs to more data yields higher accuracy and cheaper deployment.
    </p>
    <ul class="list-disc ml-5 text-sm text-slate-300 space-y-1">
      <li><strong>Equal scaling:</strong> Double parameters? Double training tokens if you are compute-limited.</li>
      <li><strong>Undertrained reality:</strong> Legacy 175B+ models missed the compute-optimal loss by a wide margin.</li>
      <li><strong>Deployment upside:</strong> Smaller, fully trained models deliver better evals per dollar and lower latency.</li>
    </ul>
  </div>

  <div class="bg-emerald-50 border border-emerald-200 rounded-lg p-5 space-y-2">
    <h3 class="text-sm font-semibold text-emerald-900">ðŸ’¼ Business relevance</h3>
    <ul class="list-disc ml-5 text-sm text-emerald-800 space-y-1">
      <li><strong>Infrastructure planners:</strong> Capacity teams at AWS, Azure, or GCP can forecast GPU demand from the 1:1 parameter-to-token scaling rule instead of chasing ever-bigger models.</li>
      <li><strong>Model providers:</strong> Foundation model leads at OpenAI, Anthropic, or Cohere can justify mid-sized, long-trained checkpoints that win evaluations per dollar.</li>
      <li><strong>Enterprise buyers:</strong> CTOs at banks or telcos can renegotiate SLAs around parameter count once vendors commit to Chinchilla ratios.</li>
    </ul>
    <div class="bg-white border border-emerald-200 rounded-md p-3 mt-3 space-y-1 text-xs text-emerald-900">
      <p class="font-semibold">Derivative example (enterprise model roadmap)</p>
      <p class="text-emerald-800">A telecom ML platform audits its fine-tuning fleet, flags models whose token exposure is under 10 &times; params, and reallocates budget to extend training runs before approving pricier architectures.</p>
    </div>
  </div>

  <div class="grid md:grid-cols-2 gap-4">
    <div class="bg-sky-50 border border-sky-200 rounded-lg p-4 space-y-2">
      <h3 class="text-sm font-semibold text-sky-900">Scaling law refresh</h3>
      <p class="text-xs text-sky-800">Re-derived loss vs. compute surfaces for modern transformers.</p>
      <ul class="list-disc ml-4 text-[11px] text-sky-700 space-y-1">
        <li>Trained 400+ models from 70M to 16B parameters on 5&ndash;500B tokens.</li>
        <li>Fit new exponents for loss vs. model size, data, and compute using joint regression.</li>
        <li>Optimal loss occurs when FLOPs split evenly between model growth and data growth.</li>
      </ul>
    </div>
    <div class="bg-sky-50 border border-sky-200 rounded-lg p-4 space-y-2">
      <h3 class="text-sm font-semibold text-sky-900">Chinchilla outcome</h3>
      <p class="text-xs text-sky-800">Compute-optimal training wins across downstream tasks.</p>
      <ul class="list-disc ml-4 text-[11px] text-sky-700 space-y-1">
        <li>70B parameters, 1.4T tokens (20 tokens per parameter).</li>
        <li>Matches Gopher's 2.8 &times; 10<sup>23</sup> FLOPs yet improves MMLU by 7 points.</li>
        <li>Inference costs drop around 4&times; thanks to the smaller parameter count.</li>
      </ul>
    </div>
  </div>

  <div class="bg-white border border-gray-200 rounded-lg p-4 space-y-2">
    <h3 class="text-sm font-semibold text-gray-900">Terminology</h3>
    <ul class="list-disc ml-5 text-xs text-gray-700 space-y-1">
      <li><strong>Compute-optimal frontier:</strong> Parameter and token pairings that minimise loss for a fixed FLOP budget.</li>
      <li><strong>Data scaling exponent:</strong> Fitted power-law that maps additional tokens to loss reduction.</li>
      <li><strong>Chinchilla ratio:</strong> Practical heuristic targeting ~20 tokens per parameter.</li>
      <li><strong>Undertrained model:</strong> A model that could lower loss by seeing more tokens without extra compute.</li>
    </ul>
  </div>

  <div class="bg-white border border-gray-200 rounded-lg p-5 space-y-3">
    <h3 class="text-sm font-semibold text-gray-900">ðŸ§ª Experiments & Evidence</h3>
    <ul class="list-disc ml-5 text-sm text-gray-700 space-y-1">
      <li><strong>Scaling sweeps:</strong> 400 transformer runs on Gopher infrastructure to fit joint scaling laws.</li>
      <li><strong>Chinchilla vs. baselines:</strong> Outperforms GPT-3, Gopher, Jurassic-1, and MT-NLG across language, reasoning, and QA benchmarks.</li>
      <li><strong>Data efficiency:</strong> Shows doubling data yields similar gains to doubling model size when compute-fixed.</li>
      <li><strong>Inference savings:</strong> Demonstrates lower latency and cost for comparable accuracy once models are compute-optimised.</li>
    </ul>
  </div>

  <div class="bg-amber-50 border border-amber-200 rounded-lg p-5 space-y-2">
    <h3 class="text-sm font-semibold text-amber-900">ðŸ”­ Forward-looking roadmap</h3>
    <ul class="list-disc ml-5 text-sm text-amber-800 space-y-1">
      <li>Track dataset deduplication and freshness when scaling beyond 1.4T tokens.</li>
      <li>Estimate FLOP budgets before training and enforce parameter-token pairing as a launch gate.</li>
      <li>Investigate modality mixing (vision, audio) under the same compute-optimal principles.</li>
      <li>Revisit fine-tuning budgets so adapters inherit compute-optimal pretraining.</li>
    </ul>
  </div>
</div>
