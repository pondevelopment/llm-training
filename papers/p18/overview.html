<div class="space-y-5">
  <div class="bg-indigo-50 border border-indigo-200 rounded-lg p-4">
    <div class="flex flex-wrap md:flex-nowrap items-start justify-between gap-4">
      <div class="md:max-w-3xl">
        <h2 class="text-xl font-semibold text-indigo-900">How Much Should We Spend to Reduce A.I.'s Existential Risk?</h2>
        <p class="text-sm text-indigo-700">Charles I. Jones • NBER Transformative AI Conference (Mar 2025)</p>
      </div>
      <a href="https://conference.nber.org/conf_papers/f227502.pdf" target="_blank" rel="noopener" class="inline-flex items-center gap-2 px-3 py-1.5 rounded-md text-xs font-medium bg-white border border-indigo-200 text-indigo-700 hover:bg-indigo-100">
        <span>View paper</span>
        <span aria-hidden="true" class="text-sm leading-none">&#8599;</span>
      </a>
    </div>
    <p class="mt-3 text-sm text-secondary leading-relaxed">
      Jones reuses the COVID revealed-preference logic to calibrate what a rational government should spend to mitigate advanced AI catastrophes. If a 0.3% mortality risk justified a 4% GDP outlay in 2020, similar or larger AI risks warrant multi-percent-of-GDP safety budgets.</p>
    <p class="mt-2 text-xs text-secondary leading-relaxed">
      He combines the $10M statistical value of life with hazard scenarios to show that even without valuing future generations, dedicating 1–5% of GDP annually to reduce AI existential risk is economically consistent when risk timelines are 10–20 years.</p>
    <div class="plain-explainer rounded-md p-3 mt-3 space-y-1 text-xs">
      <p class="font-semibold">Plain-language explainer</p>
      <p class="text-secondary">Treat AI catastrophe mitigation like pandemic insurance. Regulators already price a human life at about $10M. That math says shaving even a 1% chance of AI doom is worth six figures per person today—translating into national spending measured in GDP points.</p>
    </div>
  </div>

  <div class="bg-slate-900 text-slate-100 border border-slate-800 rounded-lg p-5 space-y-3">
    <h3 class="text-sm font-semibold tracking-wide uppercase text-slate-200">Executive quick take</h3>
    <p class="text-sm text-slate-200 leading-relaxed">Use cost-benefit math, not vibes, for AI safety. If experts see even single-digit existential risk this decade, allocate persistent GDP-level funding to hazard reduction, safety research, and governance.</p>
    <ul class="list-disc ml-5 text-sm text-slate-300 space-y-1">
      <li><strong>COVID yardstick:</strong> 0.3% mortality justified a 4% GDP “spend” in 2020; use that revealed willingness-to-pay for AI.</li>
      <li><strong>Value of life math:</strong> $10M VSL — $100k per capita to avoid 1% mortality—over 100% of US GDP per person.</li>
      <li><strong>Timelines matter:</strong> 10–20 year horizons support dedicating ≥1% GDP annually to mitigation, even without longtermism.</li>
    </ul>
  </div>

  <div class="bg-emerald-50 border border-emerald-200 rounded-lg p-5 space-y-2">
    <h3 class="text-sm font-semibold text-emerald-900">Business relevance</h3>
    <div class="bg-white border border-indigo-200 rounded-md p-3 space-y-1 text-xs text-indigo-900">
      <p class="font-semibold">Safety as capital spending</p>
      <p class="text-secondary">CFOs should model AI safety programs like insurance: multi-percent-of-revenue commitments can be revenue-maximizing if tail-risk odds are non-trivial.</p>
    </div>

    <ul class="list-disc ml-5 text-sm text-emerald-800 space-y-1">
      <li><strong>Boards:</strong> Demand quantified hazard assessments and tie safety budgets to risk probability × loss exposure.</li>
      <li><strong>Risk officers:</strong> Treat AI misuse and alignment failures as enterprise catastrophe risks, with mitigation spending targets.</li>
      <li><strong>Policy teams:</strong> Align corporate lobby positions with public investment levels; tax-financed safety infrastructure benefits incumbents.</li>
    </ul>
    <div class="bg-white border border-emerald-200 rounded-md p-3 mt-3 space-y-1 text-xs text-emerald-900">
      <p class="font-semibold">Business playbook examples</p>
      <ul class="list-disc ml-4 space-y-1 text-emerald-800">
        <li><strong>Frontier labs:</strong> Earmark ≥5% of compute budget for red-teaming and interpretability research pools.</li>
        <li><strong>Cloud providers:</strong> Bundle safety audit credits with large-scale inference contracts, priced off catastrophe insurance benchmarks.</li>
        <li><strong>Regulated industries:</strong> Co-invest with regulators in joint response centers to detect and contain malicious AI misuse.</li>
      </ul>
    </div>
  </div>

  <div class="grid md:grid-cols-3 gap-4">
    <div class="bg-white border border-gray-200 rounded-lg p-3">
      <h5 class="text-sm font-semibold text-gray-900">Key insight</h5>
      <p class="text-xs text-gray-600">Applying revealed-preference and VSL logic shows multi-percent-of-GDP safety spending is rational when AI catastrophe odds exceed 0.1–1%.</p>
    </div>
    <div class="bg-white border border-gray-200 rounded-lg p-3">
      <h5 class="text-sm font-semibold text-gray-900">Method</h5>
      <p class="text-xs text-gray-600">One-sector model with annual catastrophe hazard, mitigation investment that reduces risk, and calibration to pandemic spending plus regulatory VSL estimates.</p>
    </div>
    <div class="bg-white border border-gray-200 rounded-lg p-3">
      <h5 class="text-sm font-semibold text-gray-900">Implications</h5>
      <p class="text-xs text-gray-600">Budget AI safety via national R&D, standards, and compute throttles; treat >1% GDP budgets as defensible baseline, higher if timelines are short.</p>
    </div>
  </div>

  <div class="grid md:grid-cols-2 gap-4">
    <div class="bg-sky-50 border border-sky-200 rounded-lg p-4 space-y-2">
      <h3 class="text-sm font-semibold text-sky-900">Measurement priorities</h3>
      <p class="text-xs text-sky-800">Benchmark risk assumptions against observable signals.</p>
      <ul class="list-disc ml-4 text-[11px] text-sky-700 space-y-1">
        <li><strong>Hazard surveys:</strong> Track expert p(doom) and update budget triggers as consensus shifts.</li>
        <li><strong>Mitigation efficacy:</strong> Quantify how safety spending reduces hazard (e.g., alignment breakthroughs per $).</li>
        <li><strong>Legacy comps:</strong> Compare to nuclear safety, pandemic insurance, and climate adaptation cost ratios.</li>
      </ul>
    </div>
    <div class="bg-sky-50 border border-sky-200 rounded-lg p-4 space-y-2">
      <h3 class="text-sm font-semibold text-sky-900">Policy signals</h3>
      <p class="text-xs text-sky-800">How governments recalibrate budgets and oversight.</p>
      <ul class="list-disc ml-4 text-[11px] text-sky-700 space-y-1">
        <li><strong>Dedicated funds:</strong> Create AI safety trust funds funded via compute taxes or levies.</li>
        <li><strong>International pooling:</strong> Coordinate hazard spending commitments across OECD + frontier labs.</li>
        <li><strong>Regulatory VSL refresh:</strong> Update official VSL estimates to incorporate AI-specific risk valuations.</li>
      </ul>
    </div>
  </div>

  <div class="bg-white border border-gray-200 rounded-lg p-5 space-y-3">
    <h3 class="text-sm font-semibold text-gray-900">Evidence base</h3>
    <ul class="list-disc ml-5 text-sm text-gray-700 space-y-1">
      <li>COVID spending vs mortality risk (4% GDP for ~0.3% fatality probability).</li>
      <li>Value-of-statistical-life calibrations implying $100k willingness-to-pay to avoid a 1% mortality risk.</li>
      <li>Dynamic hazard model simulations showing ≥1% GDP mitigation outlays optimal across plausible timelines and hazard reductions.</li>
    </ul>
  </div>

  <div class="bg-amber-50 border border-amber-200 rounded-lg p-5 space-y-2">
    <h3 class="text-sm font-semibold text-amber-900">Forward-looking roadmap</h3>
    <ul class="list-disc ml-5 text-sm text-amber-800 space-y-1">
      <li>Set GDP-indexed AI safety budgets with automatic escalation when hazard surveys exceed thresholds.</li>
      <li>Fund cross-lab safety benchmarks and compute access for independent red teams.</li>
      <li>Publish annual AI catastrophe risk assessments tying mitigation spend to measurable hazard reduction.</li>
      <li>Plan international insurance pools to backstop low-probability, high-severity AI incidents.</li>
    </ul>
  </div>
</div>