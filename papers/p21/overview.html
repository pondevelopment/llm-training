<section class="space-y-5">
  <header class="panel panel-info p-5 space-y-4">
    <div class="flex flex-wrap items-start justify-between gap-4">
      <div>
        <h2 class="text-xl font-semibold">üß™ Godel Test: Can Large Language Models Solve Easy Conjectures?</h2>
        <p class="text-sm panel-muted">Moran Feldman ‚Ä¢ Amin Karbasi ‚Ä¢ arXiv cs.AI (Sep 2025)</p>
      </div>
      <a href="https://arxiv.org/abs/2509.18383" target="_blank" rel="noopener" class="chip chip-info text-xs font-semibold">
        <span>View paper</span>
        <span aria-hidden="true">‚Üó</span>
      </a>
    </div>
    <p class="text-sm leading-relaxed panel-muted">
      The authors benchmark GPT-5 against five hand-crafted combinatorial optimisation conjectures, offering only the originating papers and no hints. The model resolves three problems and even tightens one bound, but fails when proofs require cross-paper synthesis or rigorous self-verification.
    </p>
    <div class="panel panel-neutral-soft p-3 space-y-1 text-xs">
      <p class="font-semibold">Plain-language explainer</p>
      <p class="panel-muted">Think of a nightly math lab where an AI tackles fresh puzzles without seeing the researchers' solutions. GPT-5 can polish known proof templates and occasionally spot better answers, yet still slips on algebra and cross-references unless humans and tooling double-check every step.</p>
    </div>
  </header>

  <section class="panel panel-neutral p-5 space-y-3">
    <header class="flex flex-col gap-2">
      <h2 class="text-lg font-semibold tracking-wide uppercase">üìã Executive quick take</h2>
      <p class="text-sm leading-relaxed panel-muted">
        Feldman &amp; Karbasi introduce the Godel Test: can a frontier LLM produce correct proofs for fresh,
        human-easy conjectures? GPT-5 clears three of five bespoke submodular optimisation puzzles, even
        refuting one author conjecture, but collapses when proofs demand cross-paper synthesis or sustained
        self-critique.
      </p>
    </header>
    <ul class="list-disc ml-5 text-sm panel-muted space-y-1">
      <li>Expect competent graduate-student calibre reasoning on straightforward derivations, not independent leaps.</li>
      <li>Plan for human or tool-assisted verification&mdash;GPT-5 emits polished yet invalid proofs on the harder cases.</li>
      <li>Passing a Godel Test variant will hinge on integrating retrieval across sources plus rigorous proof checking.</li>
    </ul>
  </section>

  <section class="panel panel-success p-5 space-y-3">
    <header>
      <h3 class="text-sm font-semibold">üíº Business relevance</h3>
      <p class="text-sm panel-muted leading-relaxed">
        Use Godel-style drills before delegating mathematical discovery or verification to autonomous agents.
        They expose where today&apos;s LLMs still need prompts, tools, or humans in the loop.
      </p>
    </header>
    <ul class="list-disc ml-5 text-sm text-body space-y-1">
      <li><strong>Research leads:</strong> Spot which proof templates can be automated and where labelling debt remains.</li>
      <li><strong>Risk &amp; compliance teams:</strong> Gauge the residual danger of plausible-but-wrong reasoning before shipping model-written proofs or audits.</li>
      <li><strong>Product &amp; engineering:</strong> Prioritise tool integrations (retrieval, CAS, proof checkers) that unlock the next capability jump.</li>
      <li><strong>Executive sponsors:</strong> Budget for review workflows proportional to conjecture novelty and cross-source complexity.</li>
    </ul>
    <div class="panel panel-neutral-soft p-4 space-y-2 text-xs">
      <p class="font-semibold">Derivative example: Godel gate for R&amp;D agents</p>
      <p class="panel-muted">
        A research ops team builds a nightly Godel Test harness with five rotating conjectures from their
        portfolio. GPT-5 drafts proofs, an automated checker flags algebraic slips, and a human reviewer logs
        judgement calls. The scorecard feeds directly into release go/no-go decisions for self-serve theorem bots.
      </p>
    </div>
  </section>

  <section class="grid md:grid-cols-2 gap-4">
    <article class="panel panel-neutral p-4 space-y-2">
      <h4 class="text-sm font-semibold">How the Godel Test runs</h4>
      <ul class="list-disc ml-4 text-sm panel-muted space-y-1">
        <li>Pick conjectures simple enough for a trained grad student, yet absent from published solutions.</li>
        <li>Share only the originating papers and problem statement&mdash;the authors withheld their own working proofs so GPT-5 had to reason from the cited sources.</li>
        <li>Provide only minimal source papers; refrain from hints or interactive steering after the prompt is issued.</li>
        <li>Audit GPT-5&apos;s LaTeX proofs manually, including verification that reused lemmas still apply.</li>
      </ul>
    </article>
    <article class="panel panel-neutral p-4 space-y-2">
      <h4 class="text-sm font-semibold">Where GPT-5 stumbles</h4>
      <ul class="list-disc ml-4 text-sm panel-muted space-y-1">
        <li>Combining insights from two papers broke down (Problem 4) despite flawless phrasing.</li>
        <li>Self-generated proof checks missed algebraic gaps; faulty steps hid inside polished writeups.</li>
        <li>Longer analyses (Problem 5) ran out of patience&mdash;the model guessed the right algorithm but failed to justify it.</li>
      </ul>
    </article>
  </section>

  <section class="grid md:grid-cols-3 gap-4">
    <article class="panel panel-neutral p-4 space-y-2">
      <h5 class="text-sm font-semibold">Key insight</h5>
      <p class="text-sm panel-muted">
        Contemporary frontier models can refute easy conjectures and adapt known proof templates, yet still lack the
        integrative reasoning needed for multi-source arguments.
      </p>
    </article>
    <article class="panel panel-neutral p-4 space-y-2">
      <h5 class="text-sm font-semibold">Method</h5>
      <p class="text-sm panel-muted">
        Authors propose a Godel Test harness: hand-curated conjectures, sourced references, and strict hands-off
        prompting to measure raw reasoning instead of autoregressive polish.
      </p>
    </article>
    <article class="panel panel-neutral p-4 space-y-2">
      <h5 class="text-sm font-semibold">Implication</h5>
      <p class="text-sm panel-muted">
        Organisations should invest in proof-checking pipelines and cross-reference retrieval before trusting LLMs to
        publish novel mathematics or compliance attestations.
      </p>
    </article>
  </section>

  <section class="panel panel-neutral-soft p-5 space-y-2">
    <h4 class="text-sm font-semibold">üìä Evidence</h4>
    <ul class="list-disc ml-5 text-sm panel-muted space-y-1">
      <li>Three of five submodular optimisation conjectures received nearly correct proofs; Problem 2&apos;s answer even sharpened the authors&apos; bound.</li>
      <li>Problem 4 (requiring synthesis across two papers) failed outright: GPT-5 reused lemmas without checking new dependencies.</li>
      <li>Problem 5 produced the right algorithm but an invalid analysis, signalling that deeper proof steps still elude the model.</li>
      <li>Manual inspection uncovered multiple algebraic slips hidden inside otherwise well-written proofs.</li>
    </ul>
  </section>

  <section class="panel panel-warning p-5 space-y-2">
    <h4 class="text-sm font-semibold">üó∫Ô∏è Forward-looking roadmap</h4>
    <ul class="list-disc ml-5 text-sm text-body space-y-1">
      <li>Instrument automated proof checkers plus reviewer sign-off before publishing any LLM-derived theorem.</li>
      <li>Build domain-specific Godel Test suites (finance, safety auditing, evaluation) to monitor toolchain upgrades.</li>
      <li>Couple mathematical agents with retrieval planners that can cite and stitch multiple sources on demand.</li>
      <li>Track hallucination risk explicitly&mdash;credible-looking proofs should trigger extra scrutiny, not fast-track approval.</li>
    </ul>
  </section>
</section>
