<section class="space-y-5">
  <!-- Paper header -->
  <section class="panel panel-info p-4 space-y-4">
    <div class="flex items-center justify-between gap-4">
      <div class="flex-1 min-w-0">
        <h2 class="text-xl font-semibold text-heading">Thinkingâ€”Fast, Slow, and Artificial: How AI is Reshaping Human Reasoning and the Rise of Cognitive Surrender</h2>
        <p class="text-sm panel-muted">Steven D. Shaw, Gideon Nave â€¢ The Wharton School, University of Pennsylvania (2026)</p>
      </div>
      <a href="https://doi.org/10.2139/ssrn.6097646" target="_blank" rel="noopener" class="btn-soft text-xs font-semibold flex-shrink-0" data-accent="safety">
        <span>View paper</span>
        <span aria-hidden="true">â†—</span>
      </a>
    </div>
    <p class="text-sm leading-relaxed panel-muted">
      This paper introduces <strong>Tri-System Theory</strong>, extending <a href="https://en.wikipedia.org/wiki/Dual_process_theory" target="_blank" rel="noopener" class="text-accent-strong underline">Kahneman's dual-process model</a> of cognition by adding
      <strong>System 3: artificial cognition</strong>â€”external, automated, data-driven reasoning performed by AI. Across three
      preregistered experiments (N&nbsp;=&nbsp;1,372; 9,593 trials), the authors demonstrate <strong>cognitive surrender</strong>:
      people uncritically adopt AI-generated answers, boosting accuracy when AI is correct but tanking it when AI errs.
    </p>
    <div class="panel panel-neutral-soft p-3 space-y-1 text-xs">
      <p class="font-semibold text-heading">Plain-language explainer</p>
      <p class="panel-muted">We all know about "thinking fast" (gut instinct) and "thinking slow" (careful analysis). This paper argues there's now a third way: <strong>thinking artificial</strong>â€”letting an AI answer for you. The problem? People often accept AI answers without checking, even when those answers are wrong. The researchers call this "cognitive surrender," and it happens even under time pressure or when money is on the line.</p>
    </div>
  </section>

  <!-- Executive quick take -->
  <section class="panel panel-neutral p-5 space-y-3">
    <header class="flex items-center gap-2">
      <span aria-hidden="true" class="text-lg">ðŸ§­</span>
      <h3 class="text-sm font-semibold tracking-wide uppercase text-heading">Executive quick take</h3>
    </header>
    <p class="text-sm leading-relaxed text-body">
      When participants had access to an AI assistant, they consulted it on over 50% of reasoning trials and followed its advice
      ~80â€“93% of the timeâ€”even when the AI was deliberately wrong. This created a stark accuracy swing:
      <strong>+25 percentage points</strong> when AI was accurate, <strong>âˆ’15 pp</strong> when it was faulty (Cohen's h&nbsp;=&nbsp;0.81).
      Confidence rose regardless of AI correctness, creating a dangerous illusion of competence.
    </p>
    <ul class="list-disc ml-5 space-y-1 text-sm panel-muted">
      <li><strong>Cognitive surrender is robust:</strong> across all three studies, the AI-Accurate vs AI-Faulty accuracy gap remained large (OR&nbsp;=&nbsp;16.07 overall), persisting under time pressure and even with monetary incentives plus item-level feedback.</li>
      <li><strong>Individual differences matter:</strong> higher trust in AI predicted greater surrender; higher fluid IQ and need for cognition buffered against it.</li>
      <li><strong>Feedback helps, but doesn't cure:</strong> incentives + feedback more than doubled override rates for faulty AI (20%&nbsp;â†’&nbsp;42%), but the accuracy gap between AI-Accurate and AI-Faulty trials still spanned ~44 percentage points.</li>
    </ul>
  </section>

  <!-- Business relevance -->
  <section class="panel panel-success p-5 space-y-3">
    <h3 class="text-sm font-semibold text-heading">ðŸ’¼ Business relevance</h3>
    <ul class="list-disc ml-5 space-y-1 text-sm text-body">
      <li><strong>AI-assisted workflow designers:</strong> don't assume "human-in-the-loop" means humans are actually checking. People tend to accept AI outputs at face valueâ€”design friction points (verification steps, confidence indicators) to trigger System 2 deliberation.</li>
      <li><strong>Risk &amp; compliance teams:</strong> cognitive surrender means errors correlate with AI errors, not random noise. If your AI is wrong in a systematic direction (e.g., always approving edge cases), so will the humans using it.</li>
      <li><strong>Training &amp; onboarding:</strong> higher trust in AI predicts more surrender. New hires who "trust the tool" may be most vulnerable. Calibration training and structured override protocols can help.</li>
    </ul>
    <div class="panel panel-neutral-soft p-3 mt-3 space-y-1 text-xs">
      <p class="font-semibold text-heading">Derivative example</p>
      <p class="panel-muted"><strong>Customer support triage:</strong> Your team uses an AI assistant to classify incoming tickets as urgent/normal/low. The AI is right 85% of the time. This paper predicts agents will follow the AI classification on ~80â€“90% of tickets without independent verification. When the AI misclassifies an urgent issue as low, agents will typically let it through. Practical response: add a "confidence flag" that forces agents to review tickets where AI confidence is below a threshold, and provide per-shift accuracy feedback so agents learn when the AI tends to err.</p>
    </div>
  </section>

  <!-- Supporting callouts -->
  <div class="grid md:grid-cols-3 gap-4">
    <div class="panel panel-info p-4 space-y-2">
      <h3 class="text-sm font-semibold text-heading">System 1 â†’ 2 â†’ 3</h3>
      <p class="text-xs panel-muted">System 1 is fast/intuitive (gut feeling). System 2 is slow/deliberative (careful analysis). System 3 is external/artificial (AI-generated). The paper shows System 3 can pre-empt System 1 and short-circuit System 2, becoming the default cognitive path.</p>
    </div>
    <div class="panel panel-info p-4 space-y-2">
      <h3 class="text-sm font-semibold text-heading">Surrender vs offloading</h3>
      <p class="text-xs panel-muted">Cognitive offloading is strategicâ€”you use a calculator knowing it's a tool. Cognitive surrender is uncriticalâ€”you accept the AI's answer as your own without verification. 73% of faulty-AI engagements showed surrender, only 20% showed genuine offloading.</p>
    </div>
    <div class="panel panel-info p-4 space-y-2">
      <h3 class="text-sm font-semibold text-heading">Who surrenders most?</h3>
      <p class="text-xs panel-muted">Higher Trust in AI â†’ more surrender (OR&nbsp;=&nbsp;4.36). Higher Need for Cognition â†’ more resistance (OR&nbsp;=&nbsp;0.46). Higher Fluid IQ â†’ more resistance (OR&nbsp;=&nbsp;0.37). Importantly, fluid IQ didn't differ between AI-Users and Independentsâ€”it's about disposition, not ability.</p>
    </div>
  </div>

  <!-- Key insight / Method / Implication trio -->
  <div class="grid md:grid-cols-3 gap-4">
    <div class="panel panel-neutral p-4 space-y-2">
      <h3 class="text-sm font-semibold text-heading">Key insight</h3>
      <p class="text-xs panel-muted">Decision-making accuracy becomes a function of AI accuracy: people don't just use AIâ€”they <em>surrender</em> judgment to it. When AI is correct, collective performance rises dramatically. When it errs, performance drops below unaided baselines.</p>
    </div>
    <div class="panel panel-neutral p-4 space-y-2">
      <h3 class="text-sm font-semibold text-heading">Method</h3>
      <p class="text-xs panel-muted">Three preregistered experiments using a modified Cognitive Reflection Test (CRT-7). AI accuracy was manipulated via hidden seed prompts so the assistant returned either the correct answer or a confidently-presented wrong answer. Participants could freely use or ignore the AI.</p>
    </div>
    <div class="panel panel-neutral p-4 space-y-2">
      <h3 class="text-sm font-semibold text-heading">Implication</h3>
      <p class="text-xs panel-muted">Time pressure worsens surrender (System 2 suppressed). Incentives + feedback partially mitigate it (System 2 reactivated). Neither eliminates the pattern. The dominant force is dose of System 3 usage, not situational contextâ€”cognitive surrender is a structural feature of the triadic ecology.</p>
    </div>
  </div>

  <!-- Evidence -->
  <section class="panel panel-neutral p-5 space-y-3">
    <h3 class="text-sm font-semibold text-heading">ðŸ§ª Evidence</h3>
    <ul class="list-disc ml-5 space-y-1 text-sm panel-muted">
      <li><strong>Study 1 (N=359):</strong> Brain-Only accuracy 45.8%; AI-Accurate 71.0% (+25.2 pp); AI-Faulty 31.5% (âˆ’14.3 pp). Participants followed faulty AI on 79.8% of chat-engaged trials. Confidence rose 11.7 pp with AI access despite ~50% faulty trials.</li>
      <li><strong>Study 2 (N=485, time pressure):</strong> Time pressure reduced baseline accuracy (46.9%â†’32.6%). Among AI-Users, System 3 buffered this: AI-Accurate under time pressure still reached 71.3%. AI-Faulty dropped to 12.1%. Cognitive surrender effect remained large (OR&nbsp;=&nbsp;40.9 within AI-Users).</li>
      <li><strong>Study 3 (N=450, incentives+feedback):</strong> Override rates for faulty AI more than doubled (20%â†’42.3%). AI-Faulty accuracy improved (30.7%â†’45.5%). But the AI-Accurate vs AI-Faulty gap persisted (~44 pp), confirming surrender is malleable but not eliminable.</li>
      <li><strong>Trial-level synthesis (9,593 trials):</strong> Overall cognitive surrender OR&nbsp;=&nbsp;16.07. Per-study Cohen's h: Study 1&nbsp;=&nbsp;0.83, Study 2&nbsp;=&nbsp;0.86, Study 3&nbsp;=&nbsp;0.78. Trial-weighted h&nbsp;=&nbsp;0.82â€”a large and consistent effect.</li>
    </ul>
  </section>

  <!-- Forward-looking roadmap -->
  <section class="panel panel-warning p-5 space-y-3">
    <h3 class="text-sm font-semibold text-heading">ðŸ”­ For your roadmap</h3>
    <p class="text-sm text-body">Design AI-assisted workflows that preserve human agency and critical thinking, rather than assuming "human-in-the-loop" automatically provides oversight.</p>
    <ul class="list-disc ml-5 space-y-1 text-sm text-body">
      <li>Add <strong>friction-by-design</strong>: confidence indicators, uncertainty flags, or mandatory verification steps on AI outputsâ€”especially in high-stakes domains (medical, legal, financial).</li>
      <li>Implement <strong>item-level feedback loops</strong>: the paper shows timely accuracy feedback more than doubles override rates for faulty AI advice.</li>
      <li>Measure <strong>cognitive surrender risk</strong> in your user base: survey trust-in-AI and need-for-cognition to identify populations most prone to uncritical adoption.</li>
      <li>Distinguish <strong>systematic vs random AI errors</strong>: cognitive surrender means human-error patterns mirror AI-error patterns. Audit where your AI fails directionally, not just on average accuracy.</li>
      <li>Consider <strong>adaptive AI interfaces</strong> that adjust cognitive demands based on contextâ€”throttling AI confidence displays, requiring justifications before acceptance, or dynamically routing uncertain cases to human-only review.</li>
    </ul>
  </section>
</section>
