<section class="space-y-5">
  <section class="panel panel-info p-4 space-y-4">
    <div class="flex items-center justify-between gap-4">
      <div class="flex-1 min-w-0">
        <h2 class="text-xl font-semibold text-heading">Quantifying Human-AI Synergy</h2>
        <p class="text-sm panel-muted">Christoph Riedl, Ben Weidmann &bull; PsyArXiv (2025-09-22)</p>
      </div>
      <a href="https://psyarxiv.com/preprints/psyarxiv/wh79t" target="_blank" rel="noopener" class="btn-soft text-xs font-semibold flex-shrink-0" data-accent="foundations">
        <span>View paper</span>
        <span aria-hidden="true">â†—</span>
      </a>
    </div>
    <p class="text-sm leading-relaxed panel-muted">
      This paper introduces a Bayesian Item Response Theory framework to quantify human-AI synergy, separating individual ability from collaborative ability while controlling for task difficulty. Applied to 667 users across math, physics, and moral reasoning tasks, it finds GPT-4o boosts human performance by 29 percentage points and Llama-3.1-8B by 23pp. Crucially, collaborative ability is distinct from solo abilityâ€”users with stronger Theory of Mind achieve superior AI collaboration without improving solo performance.
    </p>
    <div class="panel panel-neutral-soft p-3 space-y-1 text-xs">
      <p class="font-semibold text-heading">Plain-language explainer</p>
      <p class="panel-muted">Working with AI is like playing doubles tennis: your individual skill matters, but so does knowing when to pass the ball to your partner. The authors show that people who naturally consider the AI's "perspective" get better answersâ€”even if they're not the strongest solo players.</p>
    </div>
  </section>

  <section class="panel panel-neutral p-5 space-y-3">
    <header class="flex items-center gap-2">
      <span aria-hidden="true" class="text-lg">ðŸ§­</span>
      <h3 class="text-sm font-semibold tracking-wide uppercase text-heading">Executive quick take</h3>
    </header>
    <p class="text-sm leading-relaxed text-body">
      Current AI benchmarks evaluate models in isolation, ignoring how they perform when collaborating with actual humans. This creates a measurement gap: a model that scores high on MMLU might still frustrate users, while a "weaker" model might deliver better real-world outcomes. This framework closes that gap by quantifying synergyâ€”the performance boost AI provides to diverse users on varying tasksâ€”enabling rigorous model comparisons beyond static prompt accuracy.
    </p>
    <ul class="list-disc ml-5 space-y-1 text-sm panel-muted">
      <li><strong>Dual ability framework reveals hidden dynamics:</strong> Solo ability (&theta;) and collaborative ability (&kappa;) are empirically distinct (&Delta;ELPD = 50.9, p &lt; 0.001), explaining why high-IQ users sometimes struggle with AI tools.</li>
      <li><strong>Complementarity beats equalization:</strong> GPT-4o provides 29pp boost while Llama-3.1-8B provides 23pp, with higher-ability users performing best overall but lower-ability users seeing larger relative gains.</li>
      <li><strong>Theory of Mind predicts synergy:</strong> Users who better infer the AI's "mental state" achieve superior collaboration (&rho;<sub>s</sub> = 0.17, p &lt; 0.001), and moment-to-moment ToM fluctuations predict AI response quality.</li>
    </ul>
  </section>

  <section class="panel panel-success p-5 space-y-3">
    <h3 class="text-sm font-semibold text-heading">ðŸ’¼ Business relevance</h3>
    <ul class="list-disc ml-5 space-y-1 text-sm text-body">
      <li><strong>Model procurement ROI:</strong> Synergy metrics enable apples-to-apples comparison of AI assistantsâ€”GPT-4o's 6pp advantage over Llama-3.1-8B translates to measurable productivity differences for your user base.</li>
      <li><strong>Deployment targeting:</strong> The framework identifies which users benefit most (lower-ability users gain more relatively) and which tasks show the biggest boost (harder problems).</li>
      <li><strong>Dual-track training:</strong> Since &theta; â‰  &kappa;, conventional skill development does not automatically improve AI collaborationâ€”teach delegation and response evaluation separately.</li>
      <li><strong>Theory of Mind as a hiring signal:</strong> Perspective-taking predicts collaborative success independent of domain expertise; consider it for AI-heavy roles.</li>
    </ul>
    <div class="panel panel-neutral-soft p-3 mt-3 space-y-1 text-xs">
      <p class="font-semibold text-heading">Derivative example: pilot your own synergy benchmark</p>
      <ol class="list-decimal ml-5 space-y-1 panel-muted">
        <li>Select 20 representative tasks spanning easy/medium/hard difficulty.</li>
        <li>Recruit 30-50 employees; have each complete tasks solo and with AI assistance.</li>
        <li>Record correctness, time, and ToM scores, then fit the IRT model from the Methods section.</li>
        <li>Compare synergy across models to prioritize rollouts and training investments.</li>
      </ol>
    </div>
  </section>

  <section class="grid md:grid-cols-2 gap-4">
    <article class="panel panel-info p-4 space-y-2">
      <h4 class="text-sm font-semibold text-heading">Why Item Response Theory for human-AI evaluation</h4>
      <p class="text-xs panel-muted">
        Standard benchmarks report aggregate accuracy, but this confounds task difficulty, user ability, and AI capability. IRT untangles these: it estimates latent difficulty for each task and latent ability for each user, then measures how much AI shifts the ability distribution. The Bayesian shrinkage approach prevents overfitting and provides uncertainty estimates around synergy claims.
      </p>
    </article>
    <article class="panel panel-info p-4 space-y-2">
      <h4 class="text-sm font-semibold text-heading">Theory of Mind as collaboration substrate</h4>
      <p class="text-xs panel-muted">
        ToM predicts human-human collaboration because it enables communication repair and ambiguity resolution. The authors find that both stable ToM traits and within-dialogue fluctuations predict AI response quality, meaning collaboration skill is not just intelligenceâ€”it's perspective taking.
      </p>
    </article>
  </section>

  <section class="grid md:grid-cols-3 gap-4">
    <article class="panel panel-neutral p-3 space-y-1">
      <h5 class="text-xs font-semibold uppercase tracking-wide text-heading">Key insight</h5>
      <p class="text-xs text-body">
        Human-AI collaboration is a distinct skill (&kappa;) separate from individual problem-solving ability (&theta;), mediated by Theory of Mind.
      </p>
    </article>
    <article class="panel panel-neutral p-3 space-y-1">
      <h5 class="text-xs font-semibold uppercase tracking-wide text-heading">Method</h5>
      <p class="text-xs text-body">
        Bayesian IRT decomposes performance into user ability (&theta;, &kappa;), task difficulty (&beta;, &gamma;), and AI capability, with LOO validation confirming &theta; â‰  &kappa;.
      </p>
    </article>
    <article class="panel panel-neutral p-3 space-y-1">
      <h5 class="text-xs font-semibold uppercase tracking-wide text-heading">Implication</h5>
      <p class="text-xs text-body">
        Evaluate AI assistants with synergy benchmarks, not just static prompt accuracy, and train for collaboration explicitly.
      </p>
    </article>
  </section>

  <section class="panel panel-neutral p-5 space-y-3">
    <h3 class="text-sm font-semibold text-heading">ðŸ§ª Evidence from the study</h3>
    <ul class="list-disc ml-5 space-y-1 text-sm panel-muted">
      <li><strong>Sample &amp; design:</strong> 667 users, 2,072 solo observations, tasks span math/physics/moral reasoning; within-subjects design controlling for order effects.</li>
      <li><strong>Synergy quantification:</strong> GPT-4o boosts performance by 29 points, Llama-3.1-8B by 23ppâ€”non-overlapping confidence intervals confirm GPT-4o's advantage.</li>
      <li><strong>Baseline performance:</strong> Humans alone 55.5% correct; GPT-4o alone 71%; Llama-3.1-8B alone 39%.</li>
      <li><strong>Dual ability framework:</strong> Separate &theta; and &kappa; model fits better (&Delta;ELPD = 50.9, SE = 10.2), with R-hat = 1.00 and ESS &gt; 1,500.</li>
      <li><strong>Task difficulty gradient:</strong> AI provides largest boost on the most difficult tasks (strong negative correlation between difficulty and boost).</li>
      <li><strong>Theory of Mind mechanism:</strong> ToM predicts collaborative ability but not solo ability; within-dialogue ToM fluctuations track AI response quality.</li>
    </ul>
  </section>

  <section class="panel panel-warning p-5 space-y-3">
    <h3 class="text-sm font-semibold text-heading">ðŸ”­ For your roadmap</h3>
    <p class="text-sm leading-relaxed text-body">
      This framework shifts AI evaluation from "How accurate is the model?" to "How much does the model improve human performance?" Organizations deploying AI assistants should adopt synergy benchmarks as a core procurement and training metric.
    </p>
    <div class="panel panel-info p-4 space-y-2">
      <h4 class="text-sm font-semibold text-heading">Building internal synergy benchmarks</h4>
      <p class="text-xs panel-muted">
        Start with domain-specific task batteries spanning easy/medium/hard difficulty. Run a within-subjects design where each user completes half the tasks solo and half with AI. Track correctness, time, and confidence, then fit the IRT model from the paper to estimate &theta;, &kappa;, &beta;, and &gamma;. Repeat quarterly to monitor learning effects and model drift.
      </p>
    </div>
    <div class="panel panel-info p-4 space-y-2">
      <h4 class="text-sm font-semibold text-heading">Training for collaborative ability (&kappa;)</h4>
      <p class="text-xs panel-muted">
        Since &theta; â‰  &kappa;, skill training alone will not improve AI collaboration. Teach prompt framing, delegation judgment, response evaluation, and Theory of Mind cues. Track pre/post &kappa; estimates to validate training effects and prioritize teams with low synergy.
      </p>
    </div>
    <ul class="list-disc ml-5 space-y-1 text-sm panel-muted">
      <li>Extend benchmarks to multi-turn tasks so synergy scores map to real workflows.</li>
      <li>Test ToM-aware interface nudges or prompt scaffolds to improve collaboration.</li>
      <li>Compare synergy across model modalities (text, code, multimodal) to focus investments.</li>
      <li>Track synergy over time to catch capability drift and novelty decay.</li>
    </ul>
  </section>
</section>
