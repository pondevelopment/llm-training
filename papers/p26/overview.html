<div class="paper-overview-card space-y-5">
  <section class="panel panel-info p-4 space-y-3">
    <div class="flex flex-wrap items-start justify-between gap-4">
      <div>
        <h2 class="text-xl font-semibold text-heading">Defeating Nondeterminism in LLM Inference</h2>
        <p class="text-sm panel-muted">Horace He, Thinking Machines Lab &amp;bull; Thinking Machines Lab: Connectionism (Sep 2025)</p>
      </div>
      <a href="https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/" target="_blank" rel="noopener" class="btn-soft text-xs font-semibold" data-accent="foundations">
        <span>View paper</span>
        <span aria-hidden="true">&#8599;</span>
      </a>
    </div>
    <p class="text-sm leading-relaxed panel-muted">
      The team traces temperature-0 drift to kernels whose reduction order depends on batch size. By swapping in batch-invariant RMSNorm, matmul, and FlexAttention tiles through <code>torch.Library</code>, they collapse 80 divergent completions down to a single deterministic answer and keep RLHF samplers numerically aligned with trainers.
    </p>
  </section>

  <section class="panel panel-neutral p-5 space-y-3">
    <div class="flex items-center gap-2 text-heading">
      <span aria-hidden="true" class="text-lg">&#128205;</span>
      <h3 class="text-sm font-semibold tracking-wide uppercase">Executive quick take</h3>
    </div>
    <p class="text-sm leading-relaxed text-body">
      Floating-point non-associativity is the setup, not the punchline. The real culprit is batch-size volatility: when kernels pick different reduction trees as load shifts, even greedy sampling yields new token paths. Fix the reduction order and deterministic, temp-0 inference becomes practical.
    </p>
    <ul class="list-disc ml-5 space-y-1 text-sm panel-muted">
      <li><strong>Root cause:</strong> Split-K heuristics and attention cleanup steps change with batch size, reordering reductions.</li>
      <li><strong>Mitigation:</strong> Force fixed tile sizes and deterministic cleanup so the tree never depends on queue depth.</li>
      <li><strong>Impact:</strong> Expect ~1.6&amp;times; latency versus default kernels today, but regain bitwise reproducibility for audits and RL.</li>
    </ul>
  </section>

  <section class="panel panel-success p-5 space-y-3">
    <h3 class="text-sm font-semibold text-heading">&#128188; Business relevance</h3>
    <ul class="list-disc ml-5 space-y-1 text-sm text-body">
      <li><strong>Platform leads:</strong> Ship deterministic tiers for regulated customers without freezing global traffic.</li>
      <li><strong>Inference SREs:</strong> Monitor batch histograms and nightly temp-0 diffs; alert when unique completions exceed one.</li>
      <li><strong>RL teams:</strong> Lock sampler numerics so on-policy assumptions hold&mdash;no more surprise reward crashes.</li>
      <li><strong>Finance &amp; PM:</strong> Price the throughput tax and decide where deterministic inference earns premium placement.</li>
    </ul>
    <div class="panel panel-neutral-soft p-3 mt-3 space-y-1 text-xs">
      <p class="font-semibold text-heading">Derivative example: Batch invariance audit</p>
      <p class="panel-muted">Clone production prompts, sweep allowable batch sizes at temperature 0, and chart unique completions plus first-divergence token. Route VIP or regulated tenants through the deterministic lane until vendor kernels expose batch-invariant modes.</p>
    </div>
  </section>

  <section class="grid md:grid-cols-2 gap-4">
    <article class="panel panel-info p-4 space-y-2">
      <h4 class="text-sm font-semibold text-heading">Floating point is innocent (mostly)</h4>
      <p class="text-sm panel-muted">Non-associativity explains <em>why</em> order matters, but matmuls run deterministically if the tree stays fixed. The blog reproduces bitwise-equal matmuls across 1,000 runs&mdash;proof that order drift, not GPUs alone, breaks determinism.</p>
      <ul class="list-disc ml-4 space-y-1 text-xs panel-muted">
        <li>Modern kernels avoid atomics; they use split reductions plus ordered cleanup stages.</li>
        <li>Only RMSNorm, matmul, and attention reductions need rework; pointwise ops already respect batch invariance.</li>
      </ul>
    </article>
    <article class="panel panel-info p-4 space-y-2">
      <h4 class="text-sm font-semibold text-heading">Batch invariance checklist</h4>
      <ul class="list-disc ml-4 space-y-1 text-sm panel-muted">
        <li><strong>RMSNorm:</strong> Fixed-width tiles with deterministic cleanup keep per-token stats stable.</li>
        <li><strong>Matmul:</strong> Replace fixed split-count heuristics with fixed tile sizes so split-K never depends on batch size.</li>
        <li><strong>FlexAttention:</strong> Deterministic K/V chunking plus ordered cleanup avoids reordering reductions under load.</li>
        <li>Track queue strategies (pinning, replay caches) to limit batch volatility at the scheduler level.</li>
      </ul>
    </article>
  </section>

  <section class="grid md:grid-cols-3 gap-4">
    <article class="panel panel-neutral p-4 space-y-2">
      <h5 class="text-sm font-semibold text-heading">Key insight</h5>
      <p class="text-xs panel-muted">Nondeterminism appears when batch-size uncertainty meets kernels whose reduction schedule depends on that batch. Control the reduction schedule and deterministic inference follows.</p>
    </article>
    <article class="panel panel-neutral p-4 space-y-2">
      <h5 class="text-sm font-semibold text-heading">Method</h5>
      <p class="text-xs panel-muted">Override vLLM kernels via <code>torch.Library</code>, log pairwise logprob diffs across load sweeps, and compare default versus batch-invariant execution for matmul and attention.</p>
    </article>
    <article class="panel panel-neutral p-4 space-y-2">
      <h5 class="text-sm font-semibold text-heading">Implication</h5>
      <p class="text-xs panel-muted">Deterministic inference is available now, but expect a latency tax until vendors upstream fixed-tile modes or ship faster deterministic attention kernels.</p>
    </article>
  </section>

  <section class="panel panel-neutral p-5 space-y-3">
    <h4 class="text-sm font-semibold text-heading">&#128202; Evidence</h4>
    <ul class="list-disc ml-5 space-y-1 text-sm panel-muted">
      <li>Default vLLM returned 80 unique temp-0 completions; batch-invariant kernels dropped that to exactly one.</li>
      <li>Divergence moved from token 103 to beyond 1,000 tokens once all reductions were fixed-order.</li>
      <li>Latency for 1k&amp;times;90 token runs: 26 s (default), 55 s (deterministic without attention patch), 42 s (with patched FlexAttention).</li>
      <li>RLVR experiments showed KL between sampler and trainer pinned at 0 when kernels matched; off-policy runs spiked and rewards collapsed.</li>
    </ul>
  </section>

  <section class="panel panel-warning p-5 space-y-2">
    <h4 class="text-sm font-semibold text-heading">&#128295; Roadmap</h4>
    <ul class="list-disc ml-5 space-y-1 text-sm text-body">
      <li>Lobby vendors for batch-invariant modes and publish latency deltas so product can price deterministic SKUs.</li>
      <li>Version-control sampler kernels with model weights; rebuild whenever CUDA or driver updates land.</li>
      <li>Automate nightly temp-0 replay tests and alert when unique completions rise above one.</li>
      <li>Pair deterministic inference with RLHF loops to remove hidden off-policy drift and simplify evals.</li>
    </ul>
  </section>
</div>
