<section class="space-y-5">
  <section class="panel panel-info p-4 space-y-4">
    <div class="flex items-center justify-between gap-4">
      <div class="flex-1 min-w-0">
        <h2 class="text-xl font-semibold text-heading">Defeating Nondeterminism in LLM Inference</h2>
        <p class="text-sm panel-muted">Horace He, Thinking Machines Lab &bull; Thinking Machines Lab: Connectionism (Sep 2025)</p>
      </div>
      <a href="https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/" target="_blank" rel="noopener" class="btn-soft text-xs font-semibold flex-shrink-0" data-accent="foundations">
        <span>View paper</span>
        <span aria-hidden="true">â†—</span>
      </a>
    </div>
    <p class="text-sm leading-relaxed panel-muted">
      The team traces temperature-0 drift to kernels whose reduction order depends on batch size. By swapping in batch-invariant RMSNorm, matmul, and FlexAttention tiles through <code>torch.Library</code>, they collapse 80 divergent completions down to a single deterministic answer and keep RLHF samplers numerically aligned with trainers.
    </p>
    <div class="panel panel-neutral-soft p-3 space-y-1 text-xs">
      <p class="font-semibold text-heading">Plain-language explainer</p>
      <p class="panel-muted">Think of inference as a calculator that sometimes gives different answers to the same problem because it rearranges the order of operations based on how busy it is. The team fixed the calculation order so you always get the same answer, making AI responses predictable for audits and regulations.</p>
    </div>
  </section>

  <section class="panel panel-neutral p-5 space-y-3">
    <header class="flex items-center gap-2">
      <span aria-hidden="true" class="text-lg">ðŸ§­</span>
      <h3 class="text-sm font-semibold tracking-wide uppercase text-heading">Executive quick take</h3>
    </header>
    <p class="text-sm leading-relaxed text-body">
      Floating-point non-associativity is the setup, not the punchline. The real culprit is batch-size volatility: when kernels pick different reduction trees as load shifts, even greedy sampling yields new token paths. Fix the reduction order and deterministic, temp-0 inference becomes practical.
    </p>
    <ul class="list-disc ml-5 space-y-1 text-sm panel-muted">
      <li><strong>Root cause:</strong> Split-K heuristics and attention cleanup steps change with batch size, reordering reductions.</li>
      <li><strong>Mitigation:</strong> Force fixed tile sizes and deterministic cleanup so the tree never depends on queue depth.</li>
      <li><strong>Impact:</strong> Expect ~1.6&amp;times; latency versus default kernels today, but regain bitwise reproducibility for audits and RL.</li>
    </ul>
  </section>

  <section class="panel panel-success p-5 space-y-3">
    <h3 class="text-sm font-semibold text-heading">ðŸ’¼ Business relevance</h3>
    <ul class="list-disc ml-5 space-y-1 text-sm text-body">
      <li><strong>Platform leads:</strong> Ship deterministic tiers for regulated customers without freezing global traffic.</li>
      <li><strong>Inference SREs:</strong> Monitor batch histograms and nightly temp-0 diffs; alert when unique completions exceed one.</li>
      <li><strong>RL teams:</strong> Lock sampler numerics so on-policy assumptions hold&mdash;no more surprise reward crashes.</li>
      <li><strong>Finance &amp; PM:</strong> Price the throughput tax and decide where deterministic inference earns premium placement.</li>
    </ul>
    <div class="panel panel-neutral-soft p-3 mt-3 space-y-1 text-xs">
      <p class="font-semibold text-heading">Derivative example: Batch invariance audit</p>
      <p class="panel-muted">Clone production prompts, sweep allowable batch sizes at temperature 0, and chart unique completions plus first-divergence token. Route VIP or regulated tenants through the deterministic lane until vendor kernels expose batch-invariant modes.</p>
    </div>
  </section>

  <div class="grid md:grid-cols-2 gap-4">
    <div class="panel panel-info p-4 space-y-2">
      <h3 class="text-sm font-semibold text-heading">Floating point is innocent (mostly)</h3>
      <p class="text-xs panel-muted">Non-associativity explains <em>why</em> order matters, but matmuls run deterministically if the tree stays fixed. The blog reproduces bitwise-equal matmuls across 1,000 runs&mdash;proof that order drift, not GPUs alone, breaks determinism.</p>
      <ul class="list-disc ml-4 space-y-1 text-xs panel-muted">
        <li>Modern kernels avoid atomics; they use split reductions plus ordered cleanup stages.</li>
        <li>Only RMSNorm, matmul, and attention reductions need rework; pointwise ops already respect batch invariance.</li>
      </ul>
    </div>
    <div class="panel panel-info p-4 space-y-2">
      <h3 class="text-sm font-semibold text-heading">Batch invariance checklist</h3>
      <ul class="list-disc ml-4 space-y-1 text-xs panel-muted">
        <li><strong>RMSNorm:</strong> Fixed-width tiles with deterministic cleanup keep per-token stats stable.</li>
        <li><strong>Matmul:</strong> Replace fixed split-count heuristics with fixed tile sizes so split-K never depends on batch size.</li>
        <li><strong>FlexAttention:</strong> Deterministic K/V chunking plus ordered cleanup avoids reordering reductions under load.</li>
        <li>Track queue strategies (pinning, replay caches) to limit batch volatility at the scheduler level.</li>
      </ul>
    </div>
  </div>

  <div class="grid md:grid-cols-3 gap-4">
    <div class="panel panel-neutral p-4 space-y-2">
      <h3 class="text-sm font-semibold text-heading">Key insight</h3>
      <p class="text-xs panel-muted">Nondeterminism appears when batch-size uncertainty meets kernels whose reduction schedule depends on that batch. Control the reduction schedule and deterministic inference follows.</p>
    </div>
    <div class="panel panel-neutral p-4 space-y-2">
      <h3 class="text-sm font-semibold text-heading">Method</h3>
      <p class="text-xs panel-muted">Override vLLM kernels via <code>torch.Library</code>, log pairwise logprob diffs across load sweeps, and compare default versus batch-invariant execution for matmul and attention.</p>
    </div>
    <div class="panel panel-neutral p-4 space-y-2">
      <h3 class="text-sm font-semibold text-heading">Implication</h3>
      <p class="text-xs panel-muted">Deterministic inference is available now, but expect a latency tax until vendors upstream fixed-tile modes or ship faster deterministic attention kernels.</p>
    </div>
  </div>

  <section class="panel panel-neutral p-5 space-y-3">
    <h3 class="text-sm font-semibold text-heading">ðŸ§ª Evidence</h3>
    <ul class="list-disc ml-5 space-y-1 text-sm panel-muted">
      <li>Default vLLM returned 80 unique temp-0 completions; batch-invariant kernels dropped that to exactly one.</li>
      <li>Divergence moved from token 103 to beyond 1,000 tokens once all reductions were fixed-order.</li>
      <li>Latency for 1k&amp;times;90 token runs: 26 s (default), 55 s (deterministic without attention patch), 42 s (with patched FlexAttention).</li>
      <li>RLVR experiments showed KL between sampler and trainer pinned at 0 when kernels matched; off-policy runs spiked and rewards collapsed.</li>
    </ul>
  </section>

  <section class="panel panel-warning p-5 space-y-2">
    <h3 class="text-sm font-semibold text-heading">ðŸ”­ For your roadmap</h3>
    <ul class="list-disc ml-5 space-y-1 text-sm text-body">
      <li>Lobby vendors for batch-invariant modes and publish latency deltas so product can price deterministic SKUs.</li>
      <li>Version-control sampler kernels with model weights; rebuild whenever CUDA or driver updates land.</li>
      <li>Automate nightly temp-0 replay tests and alert when unique completions rise above one.</li>
      <li>Pair deterministic inference with RLHF loops to remove hidden off-policy drift and simplify evals.</li>
    </ul>
  </section>
</section>
