<section class="space-y-6" id="p44-explorer">
  <!-- Intro panel with analogy -->
  <div class="panel panel-neutral-soft p-4 space-y-2">
    <p class="text-sm text-body">
      <strong>Think of it like this:</strong> Choosing an AI agent is like buying a car. You don't just look at top speed—you compare fuel efficiency, reliability, and safety features at different price points. Similarly, HAL's Pareto frontier shows which models offer the best accuracy-for-cost trade-off. A $10,000 car that's 95% as fast as a $100,000 supercar might be the smarter choice. This simulator lets you explore these trade-offs across real benchmarks.
    </p>
    <p class="text-xs panel-muted">
      <em>Note: Model performance numbers in this simulator are illustrative, derived from HAL paper patterns and figures. Actual performance varies by task. Use HAL infrastructure to benchmark your specific workload.</em>
    </p>
  </div>

  <!-- Controls -->
  <div class="panel panel-info p-4 space-y-4">
    <h3 class="text-sm font-semibold text-heading">HAL cost-accuracy explorer</h3>
    
    <!-- Benchmark selector -->
    <div class="space-y-2">
      <label for="p44-benchmark" class="block text-sm font-medium text-heading">
        Benchmark
      </label>
      <select id="p44-benchmark" 
              class="w-full px-3 py-2 rounded border border-divider bg-card text-body focus-visible:outline focus-visible:outline-2 focus-visible:outline-offset-1 focus-visible:outline-[color:var(--accent-strong)]">
        <option value="assistantbench">AssistantBench (customer service)</option>
        <option value="gaia">GAIA (general assistant)</option>
        <option value="mind2web">Online Mind2Web (web navigation)</option>
        <option value="swebench">SWE-bench Verified Mini (coding)</option>
        <option value="scicode">SciCode (scientific research)</option>
      </select>
      <p class="text-xs panel-muted">
        Each benchmark tests different agent capabilities: customer support, web browsing, coding, and scientific reasoning
      </p>
    </div>

    <!-- Model filter -->
    <div class="space-y-2">
      <label class="block text-sm font-medium text-heading">
        Show models
      </label>
      <div class="flex flex-wrap gap-2">
        <label class="flex items-center gap-1 text-xs">
          <input type="checkbox" id="p44-show-all" class="accent-[color:var(--accent-strong)]">
          <span class="text-body">Include dominated models</span>
        </label>
      </div>
      <p class="text-xs panel-muted">
        By default, only Pareto-optimal models are shown (no other model is both cheaper AND more accurate)
      </p>
    </div>
  </div>

  <!-- Pareto Frontier Visualization -->
  <div class="panel panel-neutral p-4 space-y-4">
    <h3 class="text-sm font-semibold text-heading">Cost vs. Accuracy Pareto frontier</h3>
    
    <div class="panel panel-neutral-soft p-4" style="min-height: 350px;">
      <div id="p44-chart" class="relative w-full" style="min-height: 300px;"></div>
    </div>

    <div class="text-xs panel-muted space-y-1">
      <p><strong>Reading the list:</strong> Models are sorted by cost (low to high). Those marked "Frontier" are Pareto-optimal—no other model offers both better accuracy AND lower cost. Click any row to see detailed stats.</p>
      <p><strong>Key insight:</strong> Most expensive models rarely justify their cost. Gemini 2.0 Flash ($0.1/$0.4 per million tokens) appears on frontier for 7 of 9 benchmarks, while Claude Opus 4.1 ($15/$75) appears on just 1 of 8 benchmarks where it was tested.</p>
    </div>
  </div>

  <!-- Selected Model Details -->
  <div class="panel panel-success p-4 space-y-3">
    <h3 class="text-sm font-semibold text-heading">Selected model: <span id="p44-selected-model">Gemini 2.0 Flash</span></h3>
    
    <div class="grid grid-cols-2 md:grid-cols-4 gap-3">
      <div class="panel panel-neutral-soft p-2">
        <div class="text-[10px] uppercase tracking-wide panel-muted">Accuracy</div>
        <div class="text-lg font-bold text-heading"><span id="p44-accuracy">42%</span></div>
      </div>
      <div class="panel panel-neutral-soft p-2">
        <div class="text-[10px] uppercase tracking-wide panel-muted">Cost per task</div>
        <div class="text-lg font-bold text-heading">$<span id="p44-cost">0.18</span></div>
      </div>
      <div class="panel panel-neutral-soft p-2">
        <div class="text-[10px] uppercase tracking-wide panel-muted">Tokens per task</div>
        <div class="text-lg font-bold text-heading"><span id="p44-tokens">45K</span></div>
      </div>
      <div class="panel panel-neutral-soft p-2">
        <div class="text-[10px] uppercase tracking-wide panel-muted">On frontier?</div>
        <div class="text-lg font-bold" id="p44-frontier-status">✓ Yes</div>
      </div>
    </div>

    <div id="p44-model-insight" class="text-xs text-body panel panel-neutral-soft p-3">
      <strong>Cost efficiency:</strong> This model offers strong accuracy at low cost, making it Pareto-optimal for this benchmark.
    </div>
  </div>

  <!-- Reasoning Effort Analysis -->
  <div class="panel panel-warning p-4 space-y-3">
    <h3 class="text-sm font-semibold text-heading">Higher reasoning effort paradox</h3>
    <p class="text-xs text-body">
      HAL tested 4 model pairs with standard vs. extended reasoning (Claude Sonnet 3.7/4, Opus 4.1, o4-mini). Surprising result: <strong>in 21 of 36 runs (58%), higher reasoning effort reduced accuracy</strong> instead of improving it. More "thinking" can lead to overthinking, second-guessing, or getting stuck in loops.
    </p>

    <div class="space-y-2">
      <div class="flex items-center justify-between text-xs">
        <span class="text-body font-medium">Runs where reasoning helped</span>
        <span class="font-semibold text-body">15 / 36 (42%)</span>
      </div>
      <div class="w-full bg-surface rounded-full h-2">
        <div class="h-2 rounded-full" style="width: 42%; background-color: var(--tone-emerald-strong);"></div>
      </div>

      <div class="flex items-center justify-between text-xs mt-3">
        <span class="text-body font-medium">Runs where reasoning hurt</span>
        <span class="font-semibold text-body">21 / 36 (58%)</span>
      </div>
      <div class="w-full bg-surface rounded-full h-2">
        <div class="h-2 rounded-full" style="width: 58%; background-color: var(--tone-rose-strong);"></div>
      </div>
    </div>

    <p class="text-xs panel-muted mt-3">
      <strong>Takeaway:</strong> Don't assume extended reasoning always helps. Profile your specific workload to find optimal inference-time compute settings—you might be paying more for worse performance.
    </p>
  </div>

  <!-- Behavioral Analysis -->
  <div class="panel panel-neutral p-5 space-y-4">
    <h3 class="text-sm font-semibold text-heading">Behavioral analysis: What log inspection reveals</h3>
    <p class="text-xs text-body">
      HAL analyzed 2.5 billion tokens of agent logs using LLM-aided inspection (Docent) to uncover behaviors invisible to accuracy metrics. Select a behavior type to see examples:
    </p>

    <div class="flex flex-wrap gap-2">
      <button data-behavior="shortcuts" class="px-3 py-2 text-xs font-medium rounded border border-divider bg-card hover:bg-surface text-body transition-colors">
        Shortcuts
      </button>
      <button data-behavior="catastrophic" class="px-3 py-2 text-xs font-medium rounded border border-divider bg-card hover:bg-surface text-body transition-colors">
        Catastrophic actions
      </button>
      <button data-behavior="bugs" class="px-3 py-2 text-xs font-medium rounded border border-divider bg-card hover:bg-surface text-body transition-colors">
        Scaffold bugs
      </button>
      <button data-behavior="benchmark" class="px-3 py-2 text-xs font-medium rounded border border-divider bg-card hover:bg-surface text-body transition-colors">
        Benchmark issues
      </button>
    </div>

    <div id="p44-behavior-detail" class="panel panel-neutral-soft p-3 space-y-2 text-xs">
      <p class="font-semibold text-heading">Select a behavior type above to see examples</p>
    </div>
  </div>

  <!-- Cost Calculator -->
  <div class="panel panel-info p-5 space-y-4">
    <h3 class="text-sm font-semibold text-heading">Production cost calculator</h3>
    <p class="text-xs text-body">
      Estimate monthly costs for deploying agents at scale. Compare different model choices for your workload.
    </p>

    <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
      <div class="space-y-2">
        <label for="p44-tasks-per-month" class="text-sm font-medium text-body block">Tasks per month: <span id="p44-tasks-label">100,000</span></label>
        <input 
          type="range" 
          id="p44-tasks-per-month" 
          min="10000" 
          max="5000000" 
          step="10000" 
          value="100000"
          class="w-full accent-[color:var(--accent-strong)]">
        <p class="text-xs panel-muted">Number of agent tasks executed monthly (support tickets, code reviews, etc.)</p>
      </div>

      <div class="space-y-2">
        <label for="p44-compare-model" class="text-sm font-medium text-body block">Compare with model</label>
        <select id="p44-compare-model" class="w-full px-3 py-2 rounded border border-divider bg-card text-body focus-visible:outline focus-visible:outline-2 focus-visible:outline-offset-1 focus-visible:outline-[color:var(--accent-strong)]">
          <option value="opus">Claude Opus 4.1 (most expensive)</option>
          <option value="gpt5">GPT-5 (high performance)</option>
          <option value="flash" selected>Gemini 2.0 Flash (cost-optimal)</option>
          <option value="o4mini">o4-mini Low (budget)</option>
        </select>
      </div>
    </div>

    <div class="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-4 gap-3 mt-4">
      <div class="panel panel-success p-3 space-y-1">
        <div class="text-xs font-semibold text-heading">Selected model cost</div>
        <div class="text-2xl font-bold text-heading">$<span id="p44-selected-cost">18,000</span>/mo</div>
        <div class="text-[10px] panel-muted">Based on current selection</div>
      </div>

      <div class="panel panel-neutral-soft p-3 space-y-1">
        <div class="text-xs font-semibold text-heading">Comparison model cost</div>
        <div class="text-2xl font-bold text-heading">$<span id="p44-compare-cost">18,000</span>/mo</div>
        <div class="text-[10px] panel-muted">Alternative model monthly cost</div>
      </div>

      <div class="panel panel-warning-soft p-3 space-y-1">
        <div class="text-xs font-semibold text-heading" id="p44-savings-label">Cost difference</div>
        <div class="text-2xl font-bold text-heading" id="p44-savings">$0</div>
        <div class="text-[10px] panel-muted" id="p44-savings-pct">0% cost difference</div>
      </div>

      <div class="panel panel-info p-3 space-y-1">
        <div class="text-xs font-semibold text-heading" id="p44-accuracy-label">Accuracy difference</div>
        <div class="text-2xl font-bold text-heading" id="p44-accuracy-diff">0%</div>
        <div class="text-[10px] panel-muted" id="p44-accuracy-pct">Selected vs comparison</div>
      </div>
    </div>

    <div id="p44-cost-insight" class="panel panel-neutral-soft p-3 text-xs text-body mt-3">
      <strong>Analysis:</strong> Cost estimates based on HAL's measured token usage and current API pricing.
    </div>
  </div>
</section>
