<section class="space-y-5">
  <!-- Paper header -->
  <section class="panel panel-info p-4 space-y-4">
    <div class="flex items-center justify-between gap-4">
      <div class="flex-1 min-w-0">
        <h2 class="text-xl font-semibold text-heading">The Dark Side of Artificial Intelligence Adoption: Linking AI Adoption to Employee Depression via Psychological Safety and Ethical Leadership</h2>
        <p class="text-sm panel-muted">Byung-Jik Kim, Min-Jik Kim, Julak Lee • Nature Humanities and Social Sciences Communications (2025)</p>
      </div>
      <a href="https://www.nature.com/articles/s41599-025-05040-2" target="_blank" rel="noopener" class="btn-soft text-xs font-semibold flex-shrink-0" data-accent="foundations">
        <span>View paper</span>
        <span aria-hidden="true">↗</span>
      </a>
    </div>
    <p class="text-sm leading-relaxed panel-muted">
      A three-wave longitudinal study of 381 South Korean employees measured AI adoption, psychological safety, ethical leadership, and depression at separate time intervals. Key finding: AI adoption significantly reduces psychological safety, which mediates an increase in depression symptoms. However, ethical leadership (leaders demonstrating fairness, transparency, and ethical decision-making) moderates this effect, substantially buffering employees against the psychological harms of AI adoption.
    </p>
    <div class="panel panel-neutral-soft p-3 space-y-1 text-xs">
      <p class="font-semibold text-heading">Plain-language explainer</p>
      <p class="panel-muted">When organizations deploy AI, employees often feel less secure—they worry about job changes, lack clarity on how AI will affect their roles, and fear making mistakes in AI-integrated systems. This erosion of psychological safety (feeling safe to speak up, make mistakes, ask for help) acts as the engine driving depression. But leaders who communicate transparently about AI, involve employees in decisions, and keep fairness at the center can reverse the effect: they restore psychological safety, and depression risk drops sharply.</p>
    </div>
  </section>

  <!-- Executive quick take -->
  <section class="panel panel-neutral p-5 space-y-3">
    <header class="flex items-center gap-2">
      <span aria-hidden="true" class="text-lg">🧭</span>
      <h3 class="text-sm font-semibold tracking-wide uppercase text-heading">Executive quick take</h3>
    </header>
    <p class="text-sm leading-relaxed text-body">
      AI adoption creates psychological costs employees may not voice unless leaders actively build psychological safety. AI systems trigger uncertainty (how will my role change?), raise autonomy concerns (will I have decision-making power?), and demand new skills. These stressors degrade psychological safety—the belief that it's safe to take interpersonal risks—which in turn increases depression risk. Organizations with ethical leaders (transparent, fair, inclusive decision-making) cut this psychological injury in half by restoring safety and boosting employee confidence in navigating AI transitions.
    </p>
    <ul class="list-disc ml-5 space-y-1 text-sm panel-muted">
      <li><strong>The mechanism:</strong> AI adoption (−0.324 effect on psychological safety) → reduced psychological safety (−0.211 effect on depression) = 0.068 indirect effect on depression.</li>
      <li><strong>The buffer:</strong> Ethical leadership (+0.211 moderation effect) weakens the AI-to-safety link, protecting psychological safety and reducing downstream depression.</li>
      <li><strong>The implication:</strong> Technical AI implementation succeeds only when paired with ethical leadership practices that maintain human dignity, autonomy, and voice during transition.</li>
    </ul>
  </section>

  <!-- Business relevance -->
  <section class="panel panel-success p-5 space-y-3">
    <h3 class="text-sm font-semibold text-heading">💼 Business relevance</h3>
    <ul class="list-disc ml-5 space-y-1 text-sm text-body">
      <li><strong>Employee well-being ROI:</strong> Depression increases absenteeism, healthcare costs, and turnover. Ethical AI leadership pays measurable dividends in retention and productivity.</li>
      <li><strong>Psychological safety as infrastructure:</strong> Don't treat safety as soft skills training; make it a KPI during AI rollouts. Measure it post-deployment; misses predict burnout and attrition months later.</li>
      <li><strong>Leadership preparation:</strong> Managers overseeing AI transitions need explicit ethical leadership training—not just technical enablement. Fairness, transparency, and inclusion in AI governance become core competencies.</li>
      <li><strong>Change management audits:</strong> Include psychological safety and well-being metrics (depression, anxiety screens, engagement surveys) in AI implementation reviews, not just adoption rates and accuracy metrics.</li>
    </ul>
    <div class="panel panel-neutral-soft p-3 mt-3 space-y-2 text-xs">
      <p class="font-semibold text-heading">Derivative example (patterned on the paper's findings)</p>
      <p class="panel-muted">A mid-sized financial services firm deploys AI for credit risk assessment. The same technology, two different outcomes based on leadership approach.</p>
      <p class="panel-muted"><strong>Low ethical leadership scenario:</strong> IT announces the AI system with no employee input sessions. Leadership makes ambiguous statements about job impacts. Employees feel unsafe voicing concerns; psychological safety drops 30%. Six months later, depression screens show elevated rates and turnover spikes among credit analysts.</p>
      <p class="panel-muted"><strong>High ethical leadership scenario:</strong> Same AI system, but leadership holds forums to explain AI's role and human oversight, commits to no layoffs, involves analysts in quality audits, and establishes clear career paths in AI-augmented roles. Psychological safety remains stable; depression rates flat; turnover minimal.</p>
    </div>
  </section>

  <!-- Evidence grid -->
  <section class="space-y-3">
    <h3 class="text-sm font-semibold text-heading">📊 Evidence</h3>
    <div class="grid grid-cols-3 gap-3">
      <div class="panel panel-neutral-soft p-3 space-y-2">
        <p class="text-xs font-semibold text-heading">Study design</p>
        <p class="text-xs panel-muted">3-wave longitudinal survey; 381 South Korean employees; 5–6 week intervals between waves; organizational AI adoption, ethical leadership, psychological safety, and depression measured at separate time points to establish temporal sequence.</p>
      </div>
      <div class="panel panel-neutral-soft p-3 space-y-2">
        <p class="text-xs font-semibold text-heading">Key mechanism (Hypothesis 4)</p>
        <p class="text-xs panel-muted">Psychological safety significantly mediates the AI adoption→depression link. Full mediation model fit: CFI=0.954, TLI=0.946, RMSEA=0.050. Indirect effect 95% CI: [0.025–0.127], excluding zero, confirming mediation.</p>
      </div>
      <div class="panel panel-neutral-soft p-3 space-y-2">
        <p class="text-xs font-semibold text-heading">Moderating protection (Hypothesis 5)</p>
        <p class="text-xs panel-muted">Ethical leadership significantly moderates AI adoption→psychological safety link (β=0.211, p&lt;0.001). High ethical leadership reduces harm of AI adoption on psychological safety.</p>
      </div>
      <div class="panel panel-neutral-soft p-3 space-y-2">
        <p class="text-xs font-semibold text-heading">AI adoption effect</p>
        <p class="text-xs panel-muted">AI adoption significantly reduces psychological safety (β=−0.324, p&lt;0.001), measured across HR systems, operations, marketing, strategy, and finance functions.</p>
      </div>
      <div class="panel panel-neutral-soft p-3 space-y-2">
        <p class="text-xs font-semibold text-heading">Psychological safety effect</p>
        <p class="text-xs panel-muted">Psychological safety significantly reduces depression (β=−0.211, p&lt;0.001). Safety captures ability to voice concerns, ask for help, and take interpersonal risks without fear of retaliation.</p>
      </div>
      <div class="panel panel-neutral-soft p-3 space-y-2">
        <p class="text-xs font-semibold text-heading">Measurement rigor</p>
        <p class="text-xs panel-muted">Cronbach's α: AI adoption (0.943), ethical leadership (0.867), psychological safety (0.783), depression (0.948). CES-D-10 depression scale; 5-point Likert items; common method bias test (first factor 26% variance, well below 50% threshold).</p>
      </div>
    </div>
  </section>

  <!-- Roadmap / implementation -->
  <section class="panel panel-neutral p-5 space-y-3">
    <h3 class="text-sm font-semibold text-heading">🔭 Implementation roadmap</h3>
    <ol class="list-decimal ml-5 space-y-2 text-sm text-body">
      <li><strong>Audit current psychological safety:</strong> Administer baseline psychological safety survey before or during early AI pilots. Benchmark depression/anxiety screens among pilot cohorts to establish baseline mental health.</li>
      <li><strong>Establish ethical AI governance:</strong> Create cross-functional oversight boards (IT, HR, legal, ethics) with authority to slow or halt AI rollouts if psychological safety indicators deteriorate or fairness concerns emerge.</li>
      <li><strong>Train leaders in ethical AI leadership:</strong> Equip managers with skills in transparent communication about AI capabilities/limitations, inclusive decision-making on AI-driven role changes, and fair performance evaluation during transitions.</li>
      <li><strong>Embed psychological safety in change management:</strong> Design AI implementation roadmaps to include safety-building checkpoints: town halls, Q&amp;A forums, input channels for frontline workers, career pathway clarity for AI-affected roles.</li>
      <li><strong>Monitor well-being signals post-deployment:</strong> Track psychological safety, depression, engagement, and turnover monthly for 6–12 months post-AI launch. Flag teams with declining safety or rising mental health risks; intensify ethical leadership interventions in those areas.</li>
    </ol>
  </section>
</section>
