{
  "1": {
    "title": "1. What does tokenization entail, and why is it critical for LLMs?",
    "dir": "./questions/q01",
    "interactiveTitle": "ğŸ” Interactive Tokenization Explorer"
  },
  "2": {
    "title": "2. How does the attention mechanism function in transformer models?",
    "dir": "./questions/q02",
    "interactiveTitle": "ğŸ” Interactive Attention Visualizer"
  },
  "3": {
    "title": "3. What is the context window in LLMs, and why does it matter?",
    "dir": "./questions/q03",
    "interactiveTitle": "ğŸ” Interactive Context Window Explorer"
  },
  "4": {
    "title": "4. What distinguishes LoRA from QLoRA in fine-tuning LLMs?",
    "dir": "./questions/q04",
    "interactiveTitle": "ğŸ”§ Interactive Fine-tuning Comparator"
  },
  "5": {
    "title": "5. How does beam search improve text generation compared to greedy decoding?",
    "dir": "./questions/q05",
    "interactiveTitle": "ğŸš€ Interactive Text Generation Comparator"
  },
  "6": {
    "title": "6. What is temperature in text generation and how does it affect output?",
    "dir": "./questions/q06",
    "interactiveTitle": "ğŸŒ¡ï¸ Interactive Temperature Laboratory"
  },
  "7": {
    "title": "7. What are embeddings and how do they enable LLMs to understand semantic meaning?",
    "dir": "./questions/q07",
    "interactiveTitle": "ğŸ§  Interactive Embedding Space Laboratory"
  },
  "8": {
    "title": "8. What is RLHF and how does it improve LLM alignment with human preferences?",
    "dir": "./questions/q08",
    "interactiveTitle": "ğŸ¤ Interactive RLHF Training Simulator"
  },
  "9": {
    "title": "9. How do autoregressive and masked models differ in LLM training?",
    "dir": "./questions/q09",
    "interactiveTitle": "ğŸ”„ Interactive Model Training Simulator"
  },
  "10": {
    "title": "10. What are embeddings, and how are they initialized in LLMs?",
    "dir": "./questions/q10",
    "interactiveTitle": "ğŸ¯ Interactive Embedding Explorer"
  },
  "11": {
    "title": "11. What is next sentence prediction, and how does it enhance LLMs?",
    "dir": "./questions/q11",
    "interactiveTitle": "ğŸ”— Interactive Next Sentence Prediction Explorer"
  },
  "12": {
    "title": "12. How do top-k and top-p sampling differ in text generation?",
    "dir": "./questions/q12",
    "interactiveTitle": "ğŸ² Interactive Text Generation Sampling Explorer"
  },
  "13": {
    "title": "13. Why is prompt engineering crucial for LLM performance?",
    "dir": "./questions/q13",
    "interactiveTitle": "ğŸ› ï¸ Interactive Prompt Engineering Workshop"
  },
  "14": {
    "title": "14. How can LLMs avoid catastrophic forgetting during fine-tuning?",
    "dir": "./questions/q14",
    "interactiveTitle": "ğŸ§ª Catastrophic Forgetting Prevention Simulator"
  },
  "15": {
    "title": "15. What is model distillation, and how does it benefit LLMs?",
    "dir": "./questions/q15",
    "interactiveTitle": "ğŸ§ª Model Distillation Simulator"
  },
  "16": {
    "title": "16. How do LLMs manage out-of-vocabulary (OOV) words?",
    "dir": "./questions/q16",
    "interactiveTitle": "ğŸ” OOV Word Tokenization Explorer"
  },
  "17": {
    "title": "17. How do transformers improve on traditional Seq2Seq models?",
    "dir": "./questions/q17",
    "interactiveTitle": "ğŸ”„ Seq2Seq vs Transformer Architecture Comparison"
  },
  "18": {
    "title": "18. What is overfitting, and how can it be mitigated in LLMs?",
    "dir": "./questions/q18",
    "interactiveTitle": "ğŸ¯ Overfitting Mitigation Simulator"
  },
  "19": {
    "title": "19. What are generative versus discriminative models in NLP?",
    "dir": "./questions/q19",
    "interactiveTitle": "ğŸ”¬ Model Type Classifier & Generator"
  },
  "20": {
    "title": "20. How do GPT-3, GPT-4, and GPT-5 differ in features and applications?",
    "dir": "./questions/q20",
    "interactiveTitle": "ğŸ”¬ GPT Model Comparison Simulator"
  },
  "21": {
    "title": "21. What are positional encodings, and why are they used?",
    "dir": "./questions/q21",
    "interactiveTitle": "ğŸ§® Interactive Positional Encoding Visualizer"
  },
  "22": {
    "title": "22. What is multi-head attention, and how does it enhance LLMs?",
    "dir": "./questions/q22",
    "interactiveTitle": "ğŸ§  Interactive Multi-Head Attention Analyzer"
  },
  "23": {
    "title": "23. How is the softmax function applied in attention mechanisms?",
    "dir": "./questions/q23",
    "interactiveTitle": "ğŸ¯ Interactive Softmax Attention Explorer"
  },
  "24": {
    "title": "24. How does the dot product contribute to self-attention?",
    "dir": "./questions/q24",
    "interactiveTitle": "ğŸ” Interactive Dot Product Attention Explorer"
  },
  "25": {
    "title": "25. Why is cross-entropy loss used in language modeling?",
    "dir": "./questions/q25",
    "interactiveTitle": "ğŸ” Interactive Cross-Entropy Loss Calculator"
  },
  "26": {
    "title": "26. How are gradients computed for embeddings in LLMs?",
    "dir": "./questions/q26",
    "interactiveTitle": "ğŸ” Interactive Embedding Gradient Calculator"
  },
  "27": {
    "title": "27. What is the Jacobian matrix's role in transformer backpropagation?",
    "dir": "./questions/q27",
    "interactiveTitle": "ğŸ” Interactive Jacobian Matrix Calculator"
  },
  "28": {
    "title": "28. How do eigenvalues and eigenvectors relate to dimensionality reduction?",
    "dir": "./questions/q28",
    "interactiveTitle": "ğŸ” Interactive Eigenvalue & PCA Explorer"
  },
  "29": {
    "title": "29. What is KL divergence, and how is it used in LLMs?",
    "dir": "./questions/q29",
    "interactiveTitle": "ğŸ“Š Interactive KL Divergence Explorer"
  },
  "30": {
    "title": "30. What is the derivative of the ReLU function, and why is it significant?",
    "dir": "./questions/q30",
    "interactiveTitle": "ğŸ” Interactive ReLU Derivative Explorer"
  },
  "31": {
    "title": "31. How does backpropagation work, and why is the chain rule critical?",
    "dir": "./questions/q31",
    "interactiveTitle": "ğŸ” Interactive Backpropagation Visualizer"
  },
  "32": {
    "title": "32. How are attention scores calculated in transformers?",
    "dir": "./questions/q32",
    "interactiveTitle": "ğŸ”¬ Interactive Attention Explorer"
  },
  "33": {
    "title": "33. How do multimodal LLMs integrate multiple modalities efficiently?",
    "dir": "./questions/q33",
    "interactiveTitle": "ğŸ”® Interactive Multimodal Training Optimizer"
  },
  "34": {
    "title": "34. What types of foundation models exist?",
    "dir": "./questions/q34",
    "interactiveTitle": "ğŸ§­ Foundation Model Taxonomy Explorer"
  },
  "35": {
    "title": "35. How does PEFT mitigate catastrophic forgetting?",
    "dir": "./questions/q35",
    "interactiveTitle": "ğŸ§ª PEFT Forgetting Mitigation Simulator"
  },
  "36": {
    "title": "36. What are the steps in Retrieval-Augmented Generation (RAG)?",
    "dir": "./questions/q36",
    "interactiveTitle": "ğŸ§ª RAG Pipeline Explorer (Retrieve â†’ Rank â†’ Generate)"
  },
  "37": {
    "title": "37. How does Mixture of Experts (MoE) enhance LLM scalability?",
    "dir": "./questions/q37",
    "interactiveTitle": "ğŸ§ª MoE Routing Simulator (Experts, Topâ€‘k, Balance)"
  },
  "38": {
    "title": "38. What is Chain-of-Thought (CoT) prompting, and how does it aid reasoning?",
    "dir": "./questions/q38",
    "interactiveTitle": "ğŸ§ª CoT Reasoning Playground (Direct vs CoT vs Selfâ€‘Consistency)"
  },
  "39": {
    "title": "39. How do discriminative and generative AI models differ?",
    "dir": "./questions/q39",
    "interactiveTitle": "ğŸ§ª Discriminative vs Generative miniâ€‘lab"
  },
  "40": {
    "title": "40. How does knowledge graph integration improve LLMs?",
    "dir": "./questions/q40",
    "interactiveTitle": "ğŸ§ª Knowledge Graph Grounding Playground"
  },
  "41": {
    "title": "41. What is zero-shot learning, and how do LLMs implement it?",
    "dir": "./questions/q41",
    "interactiveTitle": "ğŸ§ª Zero-shot vs Few-shot Prompting Playground"
  },
  "42": {
    "title": "42. How does Adaptive Softmax optimize LLMs?",
    "dir": "./questions/q42",
    "interactiveTitle": "ğŸ§ª Adaptive Softmax Simulator (compute + parameter savings)"
  },
  "43": {
    "title": "43. How do transformers address the vanishing gradient problem?",
    "dir": "./questions/q43",
    "interactiveTitle": "ğŸ§ª Gradient Flow Explorer (vanishing vs residual + LayerNorm)"
  },
  "44": {
    "title": "44. What is few-shot learning, and what are its benefits?",
    "dir": "./questions/q44",
    "interactiveTitle": "ğŸ§ª Inâ€‘Context Fewâ€‘Shot Explorer (benefit vs token cost)"
  },
  "45": {
    "title": "45. How would you fix an LLM generating biased or incorrect outputs?",
    "dir": "./questions/q45",
    "interactiveTitle": "ğŸ§ª Bias & Error Mitigation Planner"
  },
  "46": {
    "title": "46. How do encoders and decoders differ in transformers?",
    "dir": "./questions/q46",
    "interactiveTitle": "ğŸ§© Encoder vs Decoder â€” information flow explorer"
  },
  "47": {
    "title": "47. How do LLMs differ from traditional statistical language models?",
    "dir": "./questions/q47",
    "interactiveTitle": "ğŸ§ª N-gram vs Transformer â€” context and perplexity explorer"
  },
  "48": {
    "title": "48. What is a hyperparameter, and why is it important?",
    "dir": "./questions/q48",
    "interactiveTitle": "âš™ï¸ Hyperparameter impact explorer"
  },
  "49": {
    "title": "49. What defines a Large Language Model (LLM)?",
    "dir": "./questions/q49",
    "interactiveTitle": "ğŸ“ˆ LLM scale vs capability explorer"
  },
  "50": {
    "title": "50. What challenges do LLMs face in deployment?",
    "dir": "./questions/q50",
    "interactiveTitle": "ğŸš€ Deployment challenges explorer"
  },
  "51": {
    "title": "51. What is an LLM, and how are LLMs trained end-to-end?",
    "dir": "./questions/q51",
    "interactiveTitle": "ğŸ” LLM Training Pipeline Explorer"
  },
  "52": {
    "title": "52. How do you estimate and compare SaaS vs self-host LLM costs?",
    "dir": "./questions/q52",
    "interactiveTitle": "ğŸ§® LLM Cost Break-even Estimator"
  },
  "53": {
    "title": "53. What are decoding strategies for selecting output tokens?",
    "dir": "./questions/q53",
    "interactiveTitle": "ğŸ§ª Compare Decoding Strategies"
  },
  "54": {
    "title": "54. What are logits, and why don't LLMs output probabilities directly?",
    "dir": "./questions/q54",
    "interactiveTitle": "ğŸ§ª Logit Lab: Shift, Scale, Bias"
  },
  "55": {
    "title": "55. How do you decide when to stop generating text with an LLM?",
    "dir": "./questions/q55",
    "interactiveTitle": "ğŸ§ª Stopping Criteria Simulator"
  },
  "56": {
    "title": "56. When should I use Fine-tuning instead of RAG?",
    "dir": "./questions/q56",
    "interactiveTitle": "ğŸ§ª Adaptation Strategy Recommender"
  },
  "57": {
    "title": "57. What are the fundamentals of in-context learning?",
    "dir": "./questions/q57",
    "interactiveTitle": "ğŸ›ï¸ In-context prompt tuner"
  }
}
