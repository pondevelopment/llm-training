<div class="space-y-4">
  <div class="panel panel-info p-3">
    <h4 class="font-semibold mb-1">&#x1F4F0; Recommended reading (related)</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><a href="#question-01" class="underline">Question 1: What does tokenization entail?</a></li>
      <li><a href="#question-09" class="underline">Question 9: What is a transformer architecture?</a></li>
      <li><a href="#question-14" class="underline">Question 14: What are embeddings and why do they matter?</a></li>
      <li><a href="#question-47" class="underline">Question 47: What is context and perplexity?</a></li>
    </ul>
  </div>

  <div class="panel panel-info panel-emphasis p-4 space-y-2">
    <h4 class="font-semibold">&#x1F50D; What is Attention?</h4>
    <p>The attention mechanism lets transformers dynamically focus on the most relevant parts of the input sequence when processing each token. Think of it as highlighting the words that best explain a specific term in a sentence.</p>
  </div>

  <div class="grid md:grid-cols-3 gap-4">
    <div class="panel panel-success panel-emphasis p-3 stacked-card">
      <h5 class="font-medium">Query (Q)</h5>
      <p class="text-sm">The token asking “what should I pay attention to?”</p>
      <span class="chip chip-success font-mono text-xs">Current word seeking context</span>
    </div>

    <div class="panel panel-accent panel-emphasis p-3 stacked-card">
      <h5 class="font-medium">Key (K)</h5>
      <p class="text-sm">Tokens being evaluated for relevance to the query</p>
      <span class="chip chip-accent font-mono text-xs">Words that might be important</span>
    </div>

    <div class="panel panel-warning panel-emphasis p-3 stacked-card">
      <h5 class="font-medium">Value (V)</h5>
      <p class="text-sm">The information copied from relevant tokens</p>
      <span class="chip chip-warning font-mono text-xs">Content to incorporate</span>
    </div>
  </div>

  <div class="panel panel-warning p-4 space-y-2">
    <h4 class="font-semibold">&#x1F3AF; Why This Matters</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><strong>Context understanding:</strong> Links words even when they are far apart</li>
      <li><strong>Parallel processing:</strong> Computes attention scores for all tokens simultaneously</li>
      <li><strong>Interpretability:</strong> Attention weights reveal what the model focuses on</li>
      <li><strong>Long-range dependencies:</strong> Maintains relationships across long sequences</li>
    </ul>
  </div>
</div>
