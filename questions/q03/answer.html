<div class="space-y-4">
  <div class="panel panel-info p-3">
    <h4 class="font-semibold mb-1">📰 Recommended reading (related)</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><a href="#question-01" class="underline">Question 1: What does tokenization entail?</a></li>
      <li><a href="#question-02" class="underline">Question 2: How does the attention mechanism function?</a></li>
      <li><a href="#question-09" class="underline">Question 9: What is a transformer architecture?</a></li>
      <li><a href="#question-47" class="underline">Question 47: What is context and perplexity?</a></li>
    </ul>
  </div>

  <div class="panel panel-info p-4 space-y-2">
    <h4 class="font-semibold">🦠 What is a Context Window?</h4>
    <p>The context window is the maximum number of tokens an LLM can process and remember at once. Think of it as short-term memory while reading – the more you can hold in mind, the more coherent the story stays, but the harder it is on your brain.</p>
  </div>

  <div class="grid md:grid-cols-3 gap-4">
    <div class="panel panel-success p-3 space-y-2">
      <h5 class="font-medium">Small window (4K–16K tokens)</h5>
      <p class="text-sm">Fast, inexpensive; best for short chats or utility prompts.</p>
      <span class="chip chip-success font-mono text-xs">Great for: quick support, command-style prompts</span>
    </div>

    <div class="panel panel-accent p-3 space-y-2">
      <h5 class="font-medium">Medium window (32K–128K tokens)</h5>
      <p class="text-sm">Mainstream production range with dependable context.</p>
      <span class="chip chip-accent font-mono text-xs">Great for: multi-turn chats, code review, documents</span>
    </div>

    <div class="panel panel-warning p-3 space-y-2">
      <h5 class="font-medium">Large window (200K–1M+ tokens)</h5>
      <p class="text-sm">Extreme context capacity; powerful but expensive.</p>
      <span class="chip chip-warning font-mono text-xs">Great for: research briefs, legal docs, long-form Q&amp;A</span>
    </div>
  </div>

  <div class="panel panel-warning p-4 space-y-2">
    <h4 class="font-semibold">🎯 Why This Matters</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><strong>Memory continuity:</strong> Larger windows preserve earlier turns in long conversations.</li>
      <li><strong>Document comprehension:</strong> Enables summarising or analysing entire files at once.</li>
      <li><strong>Cost vs. performance:</strong> Prefill cost rises quickly as the window grows (decode stays cheap with KV cache).</li>
      <li><strong>Real-world fit:</strong> Critical for workflows like research synthesis, analytics, and policy review.</li>
    </ul>
  </div>

  <div class="panel panel-neutral p-4 space-y-2">
    <h4 class="font-semibold">📄 Current model snapshots (verify before launch)</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><strong>Anthropic Claude:</strong> 200K; Sonnet 4 beta shows 1M context. <a class="underline" href="https://docs.anthropic.com/en/docs/models-overview" target="_blank" rel="noopener">Docs</a></li>
      <li><strong>Google Gemini:</strong> Multiple models advertise 1M+ tokens. <a class="underline" href="https://ai.google.dev/gemini-api/docs/long-context" target="_blank" rel="noopener">Long-context guide</a></li>
      <li><strong>Mistral:</strong> 32K–128K typical; Codestral up to ~256K. <a class="underline" href="https://docs.mistral.ai/getting-started/models/" target="_blank" rel="noopener">Models</a></li>
      <li><strong>OpenAI / Llama / DeepSeek:</strong> Commonly 128K–200K; always confirm model cards before rollout.</li>
    </ul>
    <p class="text-xs text-muted">Snapshots are indicative only. Last reviewed: 2025-08.</p>
  </div>
</div>
