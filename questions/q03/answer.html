<div class="space-y-4">
        <div class="bg-indigo-50 p-3 rounded-lg border border-indigo-200">
            <h4 class="font-semibold text-indigo-900 mb-1">üìö Recommended reading (related)</h4>
            <ul class="list-disc ml-5 text-sm text-indigo-800 space-y-1">
                <li><a href="#question-01" class="text-indigo-700 underline hover:text-indigo-900">Question 1: What does tokenization entail?</a></li>
                <li><a href="#question-02" class="text-indigo-700 underline hover:text-indigo-900">Question 2: How does the attention mechanism function?</a></li>
                <li><a href="#question-09" class="text-indigo-700 underline hover:text-indigo-900">Question 9: What is a transformer architecture?</a></li>
                <li><a href="#question-47" class="text-indigo-700 underline hover:text-indigo-900">Question 47: What is context and perplexity?</a></li>
            </ul>
        </div>
        <div class="bg-blue-50 p-4 rounded-lg border-l-4 border-blue-400">
            <h4 class="font-semibold text-blue-900 mb-2">üß† What is a Context Window?</h4>
            <p class="text-blue-800">The context window is the maximum number of tokens an LLM can process and remember at one time. Think of it like your short-term memory when reading - you can only hold so much information in your head before earlier details start to fade away.</p>
        </div>
        
        <div class="grid md:grid-cols-3 gap-4">
            <div class="bg-green-50 p-3 rounded border-l-4 border-green-400">
                <h5 class="font-medium text-green-900">Small Window (4K‚Äì16K tokens)</h5>
                <p class="text-sm text-green-700">Fast processing, lower costs; fine for short tasks</p>
                <code class="text-xs bg-green-100 px-1 rounded">Good for: Short chats, utility prompts</code>
            </div>
            
            <div class="bg-purple-50 p-3 rounded border-l-4 border-purple-400">
                <h5 class="font-medium text-purple-900">Medium Window (32K‚Äì128K tokens)</h5>
                <p class="text-sm text-purple-700">Mainstream production range with solid context</p>
                <code class="text-xs bg-purple-100 px-1 rounded">Good for: Multi-turn chats, code, docs</code>
            </div>
            
            <div class="bg-orange-50 p-3 rounded border-l-4 border-orange-400">
                <h5 class="font-medium text-orange-900">Large Window (200K‚Äì1M+ tokens)</h5>
                <p class="text-sm text-orange-700">Longest contexts; expensive, requires careful UX</p>
                <code class="text-xs bg-orange-100 px-1 rounded">Good for: Long documents, complex Q&A</code>
            </div>
        </div>
        
        <div class="bg-yellow-50 p-4 rounded-lg">
            <h4 class="font-semibold text-yellow-900 mb-2">üéØ Why This Matters</h4>
            <ul class="text-sm text-yellow-800 space-y-1">
                <li>‚Ä¢ <strong>Memory Continuity:</strong> Larger windows maintain conversation context across many exchanges</li>
                <li>‚Ä¢ <strong>Document Understanding:</strong> Can process entire articles, books, or codebases at once</li>
                <li>‚Ä¢ <strong>Cost vs Performance:</strong> Prefill cost grows ~quadratically with window size (decode benefits from KV caching)</li>
                <li>‚Ä¢ <strong>Real-world Applications:</strong> Critical for tasks like summarization, analysis, and long-form content</li>
            </ul>
        </div>

        <div class="bg-white border border-gray-200 rounded-lg p-4">
            <h4 class="font-semibold text-gray-900 mb-2">üìå Current model snapshots (verify before use)</h4>
            <ul class="text-sm text-gray-700 list-disc pl-5 space-y-1">
                <li><strong>Anthropic Claude:</strong> 200K; Sonnet 4 <em>1M</em> (beta header). <a class="text-blue-700 underline" href="https://docs.anthropic.com/en/docs/models-overview" target="_blank" rel="noopener">Docs</a></li>
                <li><strong>Google Gemini:</strong> ‚ÄúMany models with <em>1M+</em> tokens.‚Äù <a class="text-blue-700 underline" href="https://ai.google.dev/gemini-api/docs/long-context" target="_blank" rel="noopener">Long context</a></li>
                <li><strong>Mistral:</strong> 32K‚Äì128K typical; Codestral up to 256K. <a class="text-blue-700 underline" href="https://docs.mistral.ai/getting-started/models/" target="_blank" rel="noopener">Models</a></li>
                <li><strong>OpenAI / Llama / DeepSeek:</strong> Commonly 128K‚Äì200K+; confirm on model cards before use.</li>
            </ul>
            <p class="text-xs text-gray-500 mt-2">Estimates only; providers change quickly. Last reviewed: 2025‚Äë08.</p>
        </div>
    </div>
