<div class="space-y-4">
  <div class="panel panel-info p-3">
    <h4 class="font-semibold mb-1">📚 Recommended reading (related)</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><a href="#question-06" class="underline">Question 6: What is temperature in text generation and how does it affect output?</a></li>
      <li><a href="#question-12" class="underline">Question 12: How do top-k and top-p sampling differ in text generation?</a></li>
      <li><a href="#question-23" class="underline">Question 23: How is the softmax function applied in attention mechanisms?</a></li>
      <li><a href="#question-25" class="underline">Question 25: Why is cross-entropy loss used in language modeling?</a></li>
    </ul>
  </div>

  <div class="panel panel-info p-4 space-y-2">
    <h4 class="font-semibold">💡 What is a text generation strategy?</h4>
    <p>Generation strategies decide <strong>which token comes next</strong> when the model predicts many plausible options. They balance confidence in the highest-probability token with the need for variety so the model avoids repetitive, low-quality loops.</p>
  </div>

  <div class="grid md:grid-cols-2 gap-4">
    <div class="panel panel-success p-3">
      <h5 class="font-medium">Greedy decoding</h5>
      <p class="text-sm">Selects the single most likely token at each step.</p>
      <div class="text-xs space-y-1">
        <div>✅ <strong>Fast:</strong> One forward pass per token.</div>
        <div>✅ <strong>Deterministic:</strong> Same prompt, same answer.</div>
        <div>❌ <strong>Myopic:</strong> Ignores options that unlock better endings.</div>
        <div>❌ <strong>Loops:</strong> Prone to repeating phrases.</div>
      </div>
    </div>

    <div class="panel panel-accent p-3">
      <h5 class="font-medium">Beam search</h5>
      <p class="text-sm">Maintains several promising continuations and expands them in parallel.</p>
      <div class="text-xs space-y-1">
        <div>✅ <strong>Higher quality:</strong> Explores multiple paths.</div>
        <div>✅ <strong>Recoverable:</strong> Can backtrack from weak early picks.</div>
        <div>❌ <strong>Slower:</strong> Needs extra compute and memory.</div>
        <div>❌ <strong>Tighter beams:</strong> Still risks bland, over-optimized text.</div>
      </div>
    </div>
  </div>

  <div class="panel panel-warning p-4 space-y-2">
    <h4 class="font-semibold">📌 Why this matters</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><strong>Quality versus latency:</strong> Wider beams improve coherence but add milliseconds per token.</li>
      <li><strong>Product fit:</strong> Creative tools prefer diversity; form-fill workflows value stability.</li>
      <li><strong>Safety:</strong> Broader search uncovers alternative phrasingsâboth better and riskier.</li>
      <li><strong>Evaluation:</strong> Understanding the decoder helps explain when outputs drift.</li>
    </ul>
  </div>

  <div class="panel panel-neutral p-4 space-y-2">
    <h4 class="font-semibold">ℹ Practical notes</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li>Combine beam search with sampling or penalties to avoid over-optimised, high-probability clichés.</li>
      <li>Clamp beam width for latency budgets; many production flows stop at widths of 2â4.</li>
      <li>Logging per-beam scores offers breadcrumbs for debugging and trust analysis.</li>
    </ul>
  </div>
</div>