<div class="space-y-4">
        <div class="bg-indigo-50 p-3 rounded-lg border border-indigo-200">
            <h4 class="font-semibold text-indigo-900 mb-1">ğŸ“š Recommended reading (related)</h4>
            <ul class="list-disc ml-5 text-sm text-indigo-800 space-y-1">
                <li><a href="#question-17" class="text-indigo-700 underline hover:text-indigo-900">Question 17: What is RLHF & preference optimization (DPO vs PPO)?</a></li>
                <li><a href="#question-45" class="text-indigo-700 underline hover:text-indigo-900">Question 45: How can we reduce bias and hallucinations in LLMs?</a></li>
                <li><a href="#question-49" class="text-indigo-700 underline hover:text-indigo-900">Question 49: How do you choose an LLM training strategy?</a></li>
                <li><a href="#question-50" class="text-indigo-700 underline hover:text-indigo-900">Question 50: How do you design a full LLM stack?</a></li>
            </ul>
        </div>
        <div class="bg-blue-50 p-4 rounded-lg border-l-4 border-blue-400">
            <h4 class="font-semibold text-blue-900 mb-2">ğŸ¤ What is RLHF?</h4>
            <p class="text-blue-800">Reinforcement Learning from Human Feedback (RLHF) is a training technique that aligns LLMs with human preferences and values. Think of it as teaching an AI to not just generate technically correct text, but text that humans actually find helpful, harmless, and honest. It's the secret sauce behind ChatGPT's conversational abilities.</p>
        </div>
        
        <div class="grid md:grid-cols-3 gap-4">
            <div class="bg-green-50 p-3 rounded border-l-4 border-green-400">
                <h5 class="font-medium text-green-900 mb-2">ğŸ“ Step 1: Data Collection</h5>
                <p class="text-sm text-green-700 mb-2">Collect human feedback on model outputs</p>
                <div class="text-xs space-y-1">
                    <div>âœ… <strong>Ranking:</strong> Humans rank multiple responses</div>
                    <div>âœ… <strong>Comparison:</strong> A vs B preference data</div>
                    <div>âœ… <strong>Quality:</strong> Direct quality assessments</div>
                    <div>ğŸ”„ <strong>Scale:</strong> Thousands of comparisons needed</div>
                </div>
                <code class="text-xs bg-green-100 px-1 rounded mt-2 block">Human annotators rate outputs</code>
            </div>
            
            <div class="bg-purple-50 p-3 rounded border-l-4 border-purple-400">
                <h5 class="font-medium text-purple-900 mb-2">ğŸ§  Step 2: Reward Model</h5>
                <p class="text-sm text-purple-700 mb-2">Train a model to predict human preferences</p>
                <div class="text-xs space-y-1">
                    <div>âœ… <strong>Learning:</strong> Predicts which response humans prefer</div>
                    <div>âœ… <strong>Scoring:</strong> Assigns reward scores to outputs</div>
                    <div>âœ… <strong>Generalizing:</strong> Works on unseen examples</div>
                    <div>ğŸ”„ <strong>Proxy:</strong> Approximates human judgment</div>
                </div>
                <code class="text-xs bg-purple-100 px-1 rounded mt-2 block">Neural network reward predictor</code>
            </div>
            
            <div class="bg-orange-50 p-3 rounded border-l-4 border-orange-400">
                <h5 class="font-medium text-orange-900 mb-2">ğŸ¯ Step 3: RL Optimization</h5>
                <p class="text-sm text-orange-700 mb-2">Fine-tune LLM using reinforcement learning</p>
                <div class="text-xs space-y-1">
                    <div>âœ… <strong>PPO:</strong> Proximal Policy Optimization</div>
                    <div>âœ… <strong>Rewards:</strong> Higher scores for preferred outputs</div>
                    <div>âœ… <strong>Balance:</strong> Maintain original capabilities</div>
                    <div>ğŸ”„ <strong>Iterative:</strong> Continuous improvement cycle</div>
                </div>
                <code class="text-xs bg-orange-100 px-1 rounded mt-2 block">Policy gradient optimization</code>
            </div>
        </div>
        
        <div class="bg-yellow-50 p-4 rounded-lg">
            <h4 class="font-semibold text-yellow-900 mb-2">ğŸ¯ Why RLHF Matters</h4>
            <ul class="text-sm text-yellow-800 space-y-1">
                <li>â€¢ <strong>Alignment:</strong> Ensures AI systems behave according to human values and preferences</li>
                <li>â€¢ <strong>Safety:</strong> Reduces harmful, biased, or inappropriate outputs through human guidance</li>
                <li>â€¢ <strong>Usefulness:</strong> Makes LLMs more helpful and relevant for real-world applications</li>
                <li>â€¢ <strong>Commercial Success:</strong> Powers ChatGPT, Claude, and other leading conversational AI systems</li>
            </ul>
        </div>
    </div>
