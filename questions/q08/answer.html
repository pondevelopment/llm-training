<div class="space-y-4">
  <div class="panel panel-info p-3">
    <h4 class="font-semibold mb-1">&#x1F4DA; Recommended reading (related)</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><a href="#question-17" class="underline">Question 17: What is RLHF & preference optimization (DPO vs PPO)?</a></li>
      <li><a href="#question-45" class="underline">Question 45: How can we reduce bias and hallucinations in LLMs?</a></li>
      <li><a href="#question-49" class="underline">Question 49: How do you choose an LLM training strategy?</a></li>
      <li><a href="#question-50" class="underline">Question 50: How do you design a full LLM stack?</a></li>
    </ul>
  </div>

  <div class="panel panel-info panel-emphasis p-4 space-y-2">
    <h4 class="font-semibold">&#x1F91D; What is RLHF?</h4>
    <p>Reinforcement Learning from Human Feedback (RLHF) aligns a model with human preferences. Think of it as teaching an AI to produce answers that are not only correct but helpful, safe, and honest&mdash;the recipe behind human-friendly assistants.</p>
  </div>

  <div class="grid md:grid-cols-3 gap-4">
    <div class="panel panel-success panel-emphasis p-3 stacked-card">
      <h5 class="font-medium">&#x1F4DD; Step 1: Data collection</h5>
      <p class="text-sm">Gather human preference data on model outputs.</p>
      <div class="text-xs space-y-1">
        <div>&#x2705; <strong>Ranking:</strong> Annotators order multiple responses.</div>
        <div>&#x2705; <strong>Comparisons:</strong> A/B preference judgments.</div>
        <div>&#x2705; <strong>Quality:</strong> Direct assessments of style and safety.</div>
        <div>&#x1F504; <strong>Scale:</strong> Thousands of comparisons required.</div>
      </div>
      <span class="chip chip-success text-xs mt-2">Human annotators rate outputs</span>
    </div>

    <div class="panel panel-accent panel-emphasis p-3 stacked-card">
      <h5 class="font-medium">&#x1F9E0; Step 2: Reward model</h5>
      <p class="text-sm">Train a model to predict which answer humans prefer.</p>
      <div class="text-xs space-y-1">
        <div>&#x2705; <strong>Learn:</strong> Predicts the preferred response.</div>
        <div>&#x2705; <strong>Score:</strong> Assigns reward values to outputs.</div>
        <div>&#x2705; <strong>Generalise:</strong> Handles unseen prompts.</div>
        <div>&#x1F504; <strong>Proxy:</strong> Only approximates human judgment.</div>
      </div>
      <span class="chip chip-accent text-xs mt-2">Neural reward predictor</span>
    </div>

    <div class="panel panel-warning panel-emphasis p-3 stacked-card">
      <h5 class="font-medium">&#x1F3AF; Step 3: RL optimisation</h5>
      <p class="text-sm">Fine-tune the base model with PPO using reward scores.</p>
      <div class="text-xs space-y-1">
        <div>&#x2705; <strong>PPO:</strong> Stable policy-gradient updates.</div>
        <div>&#x2705; <strong>Reward-driven:</strong> Prefers high-scoring outputs.</div>
        <div>&#x2705; <strong>Balanced:</strong> Retains base-model strengths.</div>
        <div>&#x1F504; <strong>Iterative:</strong> Train &rarr; evaluate &rarr; repeat.</div>
      </div>
      <span class="chip chip-warning text-xs mt-2">Policy gradient optimisation</span>
    </div>
  </div>

  <div class="panel panel-warning p-4 space-y-2">
    <h4 class="font-semibold">&#x1F3AF; Why RLHF matters</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><strong>Alignment:</strong> Steers AI behaviour toward human values.</li>
      <li><strong>Safety:</strong> Reduces harmful or biased outputs via human oversight.</li>
      <li><strong>Usefulness:</strong> Produces answers people actually want.</li>
      <li><strong>Commercial success:</strong> The foundation of ChatGPT, Claude, and other assistant models.</li>
    </ul>
  </div>
</div>
