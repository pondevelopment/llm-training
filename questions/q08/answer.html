<div class="space-y-4">
  <div class="panel panel-info p-3">
    <h4 class="font-semibold mb-1">ğŸ“š Recommended reading (related)</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><a href="#question-17" class="underline">Question 17: What is RLHF & preference optimization (DPO vs PPO)?</a></li>
      <li><a href="#question-45" class="underline">Question 45: How can we reduce bias and hallucinations in LLMs?</a></li>
      <li><a href="#question-49" class="underline">Question 49: How do you choose an LLM training strategy?</a></li>
      <li><a href="#question-50" class="underline">Question 50: How do you design a full LLM stack?</a></li>
    </ul>
  </div>

  <div class="panel panel-info p-4 space-y-2">
    <h4 class="font-semibold">ğŸ¤ What is RLHF?</h4>
    <p>Reinforcement Learning from Human Feedback (RLHF) aligns a model with human preferences. Think of it as teaching an AI to produce answers that are not only correct but helpful, safe, and honest&mdash;the recipe behind human-friendly assistants.</p>
  </div>

  <div class="grid md:grid-cols-3 gap-4">
    <div class="panel panel-success p-3">
      <h5 class="font-medium">ğŸ“ Step 1: Data collection</h5>
      <p class="text-sm">Gather human preference data on model outputs.</p>
      <div class="text-xs space-y-1">
        <div>âœ… <strong>Ranking:</strong> Annotators order multiple responses.</div>
        <div>âœ… <strong>Comparisons:</strong> A/B preference judgments.</div>
        <div>âœ… <strong>Quality:</strong> Direct assessments of style and safety.</div>
        <div>ğŸ”„ <strong>Scale:</strong> Thousands of comparisons required.</div>
      </div>
      <span class="chip chip-success text-xs mt-2">Human annotators rate outputs</span>
    </div>

    <div class="panel panel-accent p-3">
      <h5 class="font-medium">ğŸ§  Step 2: Reward model</h5>
      <p class="text-sm">Train a model to predict which answer humans prefer.</p>
      <div class="text-xs space-y-1">
        <div>âœ… <strong>Learn:</strong> Predicts the preferred response.</div>
        <div>âœ… <strong>Score:</strong> Assigns reward values to outputs.</div>
        <div>âœ… <strong>Generalise:</strong> Handles unseen prompts.</div>
        <div>ğŸ”„ <strong>Proxy:</strong> Only approximates human judgment.</div>
      </div>
      <span class="chip chip-accent text-xs mt-2">Neural reward predictor</span>
    </div>

    <div class="panel panel-warning p-3">
      <h5 class="font-medium">ğŸ¯ Step 3: RL optimisation</h5>
      <p class="text-sm">Fine-tune the base model with PPO using reward scores.</p>
      <div class="text-xs space-y-1">
        <div>âœ… <strong>PPO:</strong> Stable policy-gradient updates.</div>
        <div>âœ… <strong>Reward-driven:</strong> Prefers high-scoring outputs.</div>
        <div>âœ… <strong>Balanced:</strong> Retains base-model strengths.</div>
        <div>ğŸ”„ <strong>Iterative:</strong> Train â†’ evaluate â†’ repeat.</div>
      </div>
      <span class="chip chip-warning text-xs mt-2">Policy gradient optimisation</span>
    </div>
  </div>

  <div class="panel panel-warning p-4 space-y-2">
    <h4 class="font-semibold">ğŸ¯ Why RLHF matters</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><strong>Alignment:</strong> Steers AI behaviour toward human values.</li>
      <li><strong>Safety:</strong> Reduces harmful or biased outputs via human oversight.</li>
      <li><strong>Usefulness:</strong> Produces answers people actually want.</li>
      <li><strong>Commercial success:</strong> The foundation of ChatGPT, Claude, and other assistant models.</li>
    </ul>
  </div>
</div>
