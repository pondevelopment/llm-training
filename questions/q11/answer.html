<div class="space-y-4">
  <div class="panel panel-info p-3 space-y-2">
    <h4 class="font-semibold">📚 Recommended reading (related)</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><a href="#question-01" class="underline">Question 1: Tokenization – how input is segmented before NSP pairs are formed</a></li>
      <li><a href="#question-02" class="underline">Question 2: Attention – mechanism modeling cross-sentence dependencies</a></li>
      <li><a href="#question-09" class="underline">Question 9: Autoregressive vs Masked – where NSP fits in pretraining objectives</a></li>
      <li><a href="#question-25" class="underline">Question 25: Cross-Entropy Loss – objective used by the NSP classification head</a></li>
    </ul>
  </div>

  <div class="panel panel-info panel-emphasis p-4 space-y-2">
    <h4 class="font-semibold">🧭 What is Next Sentence Prediction?</h4>
    <p>Next Sentence Prediction (NSP) teaches a model whether two sentences naturally follow one another. Think of it as checking whether two shuffled paragraphs belong together—perfect practice for learning document structure and discourse flow.</p>
  </div>

  <div class="grid gap-4 md:grid-cols-2">
    <div class="panel panel-success p-3 space-y-2">
      <h5 class="font-medium">✅ Positive pairs (50%)</h5>
      <p class="text-sm">Sequential sentences that actually co-occur in the source document.</p>
      <div class="text-xs space-y-1">
        <div><strong>Example:</strong> "The cat sat on the mat. It was a sunny afternoon."</div>
        <div><strong>Label:</strong> <span class="chip chip-success text-xs">IsNext</span></div>
      </div>
    </div>
    <div class="panel panel-warning p-3 space-y-2">
      <h5 class="font-medium">❌ Negative pairs (50%)</h5>
      <p class="text-sm">Random sentences pulled from unrelated documents.</p>
      <div class="text-xs space-y-1">
        <div><strong>Example:</strong> "The cat sat on the mat. Quantum computing uses qubits."</div>
        <div><strong>Label:</strong> <span class="chip chip-warning text-xs">NotNext</span></div>
      </div>
    </div>
  </div>

  <div class="grid gap-4 md:grid-cols-3">
    <div class="panel panel-accent p-3 space-y-2">
      <h5 class="font-medium">💬 Dialogue systems</h5>
      <p class="text-sm">Track conversation flow so replies connect naturally.</p>
      <div class="panel panel-neutral-soft font-mono text-xs p-2 q11-snippet">User: "How's the weather?" → Bot: "It's sunny today!"</div>
    </div>
    <div class="panel panel-info p-3 space-y-2">
      <h5 class="font-medium">📰 Document summarization</h5>
      <p class="text-sm">Finds adjacent sentences that anchor coherent summaries.</p>
      <div class="panel panel-neutral-soft font-mono text-xs p-2 q11-snippet">Paragraph 1 + Paragraph 2 → Coherent summary</div>
    </div>
    <div class="panel panel-warning p-3 space-y-2">
      <h5 class="font-medium">🔍 Reading comprehension</h5>
      <p class="text-sm">Strengthens logical flow for question–answer pairs.</p>
      <div class="panel panel-neutral-soft font-mono text-xs p-2 q11-snippet">Question + Context → Grounded answer</div>
    </div>
  </div>

  <div class="panel panel-neutral p-4 space-y-2">
    <h4 class="font-semibold">🧠 How NSP training works</h4>
    <div class="text-sm space-y-2">
      <p><strong>Data preparation:</strong> Build 50% positive (sequential) and 50% negative (random) sentence pairs from documents.</p>
      <p><strong>Model input:</strong> <code class="font-mono">[CLS] Sentence A [SEP] Sentence B [SEP]</code> feeds a binary classification head.</p>
      <p><strong>Objective:</strong> Predict whether Sentence B naturally follows Sentence A.</p>
      <div class="panel panel-neutral-soft p-3 text-sm space-y-1">
        <strong>Example:</strong> BERT pairs NSP with Masked Language Modeling so it learns both token-level meaning and sentence-level coherence.</div>
    </div>
  </div>

  <div class="panel panel-neutral p-4 space-y-2">
    <h4 class="font-semibold">⚠️ Caveats and modern practice</h4>
    <ul class="text-sm space-y-1">
      <li>BERT used NSP, but follow-ups such as <strong>RoBERTa</strong> removed it and matched or exceeded performance.</li>
      <li><strong>ALBERT</strong> swaps in Sentence Order Prediction (SOP) with harder negatives that require deeper discourse checks.</li>
      <li>Decoder-only LLMs (GPT-style) skip NSP entirely—long-context next-token prediction already teaches ordering.</li>
      <li>Add NSP/SOP only if downstream tasks benefit; always validate on sentence-coherence benchmarks.</li>
    </ul>
  </div>

  <div class="panel panel-warning p-4 space-y-2">
    <h4 class="font-semibold">🎯 Why NSP still matters</h4>
    <ul class="text-sm space-y-1">
      <li><strong>Coherence:</strong> Helps models maintain logical flow in long generations.</li>
      <li><strong>Document structure:</strong> Improves awareness of paragraph-to-paragraph relationships.</li>
      <li><strong>Contextual reasoning:</strong> Encourages linking implicit references across sentences.</li>
      <li><strong>Task transfer:</strong> Boosts sentence-level QA, summarization, and dialogue tasks.</li>
      <li><strong>Safety rails:</strong> Reduces abrupt topic jumps in assistants and agents.</li>
    </ul>
  </div>
</div>
