<div class="space-y-4">
  <div class="panel panel-info p-3 space-y-2">
    <h4 class="font-semibold">📚 Recommended reading (related)</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><a href="#question-05" class="underline">Question 5: Beam Search vs Greedy – broader search vs local sampling</a></li>
      <li><a href="#question-06" class="underline">Question 6: Temperature – how logits are reshaped before sampling</a></li>
      <li><a href="#question-23" class="underline">Question 23: Softmax in Attention – same probabilistic normalization core</a></li>
      <li><a href="#question-25" class="underline">Question 25: Cross-Entropy Loss – trains the token distribution you sample from</a></li>
    </ul>
  </div>
  <div class="panel panel-info panel-emphasis p-4 space-y-2">
    <h4 class="font-semibold">🎲 What is Sampling in Text Generation?</h4>
    <p class="text-sm leading-relaxed"><strong>Sampling methods control how language models choose the next token during text generation.</strong> Instead of always picking the most probable word (greedy decoding), sampling introduces controlled randomness to create more diverse and creative outputs. Think of it like a creative writer choosing between several good word options rather than always using the most obvious one.</p>
  </div>
  <div class="panel panel-neutral p-4 space-y-2">
    <h4 class="font-semibold">🛠️ Terminology: tokens vs. words</h4>
    <p class="text-sm panel-muted">Sampling operates over <strong>tokens</strong>, which are usually <em>subword pieces</em> (not whole words). A word can be split into multiple tokens (for example, "unbelievable" → "un", "believ", "able"). In this demo we visualize whole words as tokens for clarity, but the same principles apply at the token level.</p>
  </div>
  <div class="grid md:grid-cols-3 gap-4">
    <div class="q12-approach panel panel-success panel-emphasis p-3">
      <h5 class="font-medium">🔢 Top-k Sampling</h5>
      <p class="text-sm">Selects from the k most probable tokens for random sampling, ensuring controlled diversity.</p>
      <div class="q12-approach-foot text-xs">
        <div><strong>Example (k=3):</strong> "The cat" → {sat: 40%, jumped: 30%, ran: 20%}</div>
        <div class="chip chip-success text-xs">Fixed vocabulary size — always considers exactly k options</div>
      </div>
    </div>
    <div class="q12-approach panel panel-accent panel-emphasis p-3">
      <h5 class="font-medium">🎯 Top-p Sampling (Nucleus)</h5>
      <p class="text-sm">Chooses tokens whose cumulative probability exceeds threshold p, adapting to context.</p>
      <div class="q12-approach-foot text-xs">
        <div><strong>Example (p=0.9):</strong> "The cat" → tokens until 90% probability mass</div>
        <div class="chip chip-accent text-xs">Dynamic vocabulary size — adapts to probability distribution</div>
      </div>
    </div>
    <div class="q12-approach panel panel-warning panel-emphasis p-3">
      <h5 class="font-medium">⚡ Greedy Decoding</h5>
      <p class="text-sm">Always selects the most probable token for deterministic, predictable output.</p>
      <div class="q12-approach-foot text-xs">
        <div><strong>Example:</strong> "The cat" → sat (40% — highest probability)</div>
        <div class="chip chip-warning text-xs">No randomness — deterministic but can be repetitive</div>
      </div>
    </div>
  </div>
  <div class="panel panel-info p-4 space-y-3">
    <h4 class="font-semibold">🔍 Key Differences</h4>
    <div class="grid md:grid-cols-2 gap-4 text-sm">
      <div class="space-y-1">
        <h5 class="font-medium">Top-k Sampling:</h5>
        <ul class="space-y-1">
          <li>• <strong>Fixed vocabulary:</strong> always considers exactly k tokens</li>
          <li>• <strong>Simple implementation:</strong> sort by probability, take top k</li>
          <li>• <strong>Consistent diversity:</strong> same number of options every time</li>
          <li>• <strong>Risk:</strong> may include very low-probability tokens</li>
        </ul>
      </div>
      <div class="space-y-1">
        <h5 class="font-medium">Top-p Sampling:</h5>
        <ul class="space-y-1">
          <li>• <strong>Dynamic vocabulary:</strong> size varies based on distribution</li>
          <li>• <strong>Context-adaptive:</strong> more choices for uncertain contexts</li>
          <li>• <strong>Quality focus:</strong> excludes very low-probability tokens</li>
          <li>• <strong>Flexibility:</strong> better for creative and coherent generation</li>
        </ul>
      </div>
    </div>
  </div>
  <div class="panel panel-warning p-4 space-y-2">
    <h4 class="font-semibold">🎯 Why Sampling Strategy Matters</h4>
    <ul class="text-sm space-y-1">
      <li>• <strong>Creative writing:</strong> top-p produces more varied and engaging narratives</li>
      <li>• <strong>Coherence control:</strong> proper sampling prevents repetitive or nonsensical text</li>
      <li>• <strong>Task adaptation:</strong> different tasks benefit from different sampling approaches</li>
      <li>• <strong>Quality vs. diversity:</strong> balance between creative output and meaningful content</li>
      <li>• <strong>User experience:</strong> sampling affects how natural and engaging AI text feels</li>
      <li>• <strong>Common practice:</strong> combine <em>top-p</em> with <em>temperature</em> (for example τ≈0.7–1.0) for stable, creative outputs</li>
    </ul>
  </div>
</div>
