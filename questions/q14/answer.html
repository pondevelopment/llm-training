<div class="space-y-4">
  <div class="panel panel-info p-3">
    <h4 class="font-semibold mb-1">ğŸ“’ Recommended reading (if these terms are new)</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><a class="underline" href="#question-4">Question 4: LoRA vs QLoRA</a> â€” what LoRA modules are and why they help prevent forgetting.</li>
      <li><a class="underline" href="#question-35">Question 35: PEFT and catastrophic forgetting</a> â€” how parameter-efficient tuning (LoRA, adapters, BitFit) preserves base knowledge.</li>
    </ul>
  </div>

  <div class="panel panel-info p-4 space-y-2">
    <h4 class="font-semibold">ğŸ§  What is catastrophic forgetting?</h4>
    <p>Catastrophic forgetting happens when a neural network rapidly overwrites earlier skills while learning a new task. Imagine cramming for a specialised exam so intensely that you forget everything from last semester â€” unfettered fine-tuning can push an LLM into exactly that failure mode.</p>
  </div>

  <div class="grid md:grid-cols-3 gap-4">
    <div class="panel panel-success p-3 space-y-2">
      <h5 class="font-medium">ğŸ”„ Rehearsal methods</h5>
      <p class="text-sm">Mix a slice of the original dataset with new examples so gradients keep rehearsing prior knowledge.</p>
      <span class="chip chip-success text-xs">70% new data + 30% old</span>
    </div>
    <div class="panel panel-accent p-3 space-y-2">
      <h5 class="font-medium">âš¡ï¸ Elastic Weight Consolidation</h5>
      <p class="text-sm">Estimate which weights matter for past tasks and penalise moving them too far during the new update.</p>
      <div class="panel panel-neutral-soft p-2 text-xs font-mono text-center">
        $$
        \mathcal{L}(\theta) = \mathcal{L}_{\text{task}}(\theta) + \lambda \sum_i F_i\,(\theta_i - \theta_i^{*})^2
        $$
      </div>
    </div>
    <div class="panel panel-warning p-3 space-y-2">
      <h5 class="font-medium">ğŸ”§ Modular architectures</h5>
      <p class="text-sm">Attach task-specific adapters or LoRA modules so the frozen backbone keeps its general abilities.</p>
      <span class="chip chip-warning text-xs">Adapters â€¢ LoRA â€¢ Prefix tuning</span>
    </div>
  </div>

  <div class="grid md:grid-cols-2 gap-4">
    <div class="panel panel-info p-3 space-y-2">
      <h5 class="font-medium">ğŸ““ Progressive neural networks</h5>
      <p class="text-sm">Grow a new column for each task and share lateral connections so earlier expertise remains intact.</p>
      <span class="chip chip-info text-xs">Task 1 â†’ Task 2 â†’ Task 3</span>
    </div>
    <div class="panel panel-success p-3 space-y-2">
      <h5 class="font-medium">ğŸ¯ Task-specific heads</h5>
      <p class="text-sm">Keep a shared encoder but branch into lightweight output heads tuned for individual objectives.</p>
      <span class="chip chip-success text-xs">Shared encoder + custom decoders</span>
    </div>
  </div>

  <div class="panel panel-neutral p-4 space-y-3">
    <h4 class="font-semibold">ğŸ§¬ Memory consolidation approaches</h4>
    <div class="grid md:grid-cols-2 gap-4 text-sm">
      <div class="space-y-2">
        <h6 class="font-medium text-heading">Regularisation-based</h6>
        <ul class="list-disc ml-5 space-y-1 text-muted">
          <li><strong>EWC:</strong> Fisher Information Matrix reweights critical parameters.</li>
          <li><strong>SI:</strong> Synaptic Intelligence tracks contribution over training steps.</li>
          <li><strong>MAS:</strong> Memory Aware Synapses scores neurons by output sensitivity.</li>
          <li><strong>L2:</strong> Standard weight decay keeps drift in check.</li>
        </ul>
      </div>
      <div class="space-y-2">
        <h6 class="font-medium text-heading">Architecture-based</h6>
        <ul class="list-disc ml-5 space-y-1 text-muted">
          <li><strong>LoRA:</strong> Low-rank adapters sit alongside frozen matrices.</li>
          <li><strong>Adapters:</strong> Bottleneck modules per layer or block.</li>
          <li><strong>Prefix tuning:</strong> Learnable prompt vectors steer attention.</li>
          <li><strong>BitFit:</strong> Train only bias terms for cheap task alignment.</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="panel panel-warning p-4 space-y-2">
    <h4 class="font-semibold">ğŸ¯ Why preventing catastrophic forgetting matters</h4>
    <ul class="list-disc ml-5 text-sm space-y-1 text-muted">
      <li><strong>Versatility:</strong> One model can handle diverse domains without retraining from scratch.</li>
      <li><strong>Cost efficiency:</strong> Reusing the backbone avoids repeated pretraining cycles.</li>
      <li><strong>Knowledge transfer:</strong> Shared representations boost performance on related tasks.</li>
      <li><strong>Deployment flexibility:</strong> Update task heads without redeploying the entire model.</li>
      <li><strong>Continuous learning:</strong> Enables incremental updates without erasing history.</li>
    </ul>
  </div>

  <div class="panel panel-accent p-4 space-y-2">
    <h4 class="font-semibold">âš ï¸ Trade-offs and considerations</h4>
    <ul class="list-disc ml-5 text-sm space-y-1 text-muted">
      <li><strong>Memory vs. performance:</strong> Storing rehearsal data or importance weights increases footprint.</li>
      <li><strong>Plasticity vs. stability:</strong> Strong constraints can slow adaptation to new domains.</li>
      <li><strong>Compute overhead:</strong> Regularisation terms or adapter routing add training cost.</li>
      <li><strong>Task similarity:</strong> Methods shine when tasks share structure; disjoint domains may still drift.</li>
    </ul>
  </div>
</div>
