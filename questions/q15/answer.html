<div class="space-y-4">
  <!-- Recommended Reading Links -->
  <div class="panel panel-info p-3 space-y-2">
    <h4 class="font-semibold text-heading">📚 Recommended reading (if these terms are new)</h4>
    <ul class="list-disc pl-5 text-sm space-y-1">
      <li><a href="#question-4" class="underline">Question 4: LoRA vs QLoRA</a> — adapters vs full fine‑tuning and memory trade‑offs.</li>
      <li><a href="#question-29" class="underline">Question 29: KL divergence</a> — the divergence used in many distillation losses.</li>
      <li><a href="#question-6" class="underline">Question 6: Temperature in generation</a> — intuition for temperature T used during distillation.</li>
    </ul>
  </div>

  <!-- Main Concept -->
  <div class="panel panel-info p-4 space-y-2">
    <h4 class="font-semibold text-heading">🎓 What is Model Distillation?</h4>
    <p class="text-sm">
      Model distillation is like a master craftsman teaching an apprentice. A large, powerful "teacher" model trains a smaller "student" model
      by sharing not just the final answers, but also its confidence levels and reasoning patterns. The student learns to mimic the teacher's
      decision-making process while being much smaller and faster.
    </p>
  </div>

  <!-- Distillation Types -->
  <div class="grid md:grid-cols-3 gap-4">
    <div class="panel panel-success p-3 space-y-2">
      <h5 class="font-medium text-heading">📚 Knowledge Distillation</h5>
      <p class="text-sm">
        Train student models using teacher soft probability distributions instead of hard labels.
      </p>
      <div class="math-display text-xs">$$\mathcal{L} = \alpha\, T^2\, D_{KL}\!\left(p_{t}^{(T)}\,\big\|\, p_{s}^{(T)}\right) + (1-\alpha)\, \mathrm{CE}(y, p_s)$$</div>
    </div>

    <div class="panel panel-accent p-3 flex flex-col gap-2 h-full">
      <h5 class="font-medium text-heading">🏗️ Progressive Distillation</h5>
      <p class="text-sm">
        Gradually reduce model size through multiple teacher-student generations.
      </p>
      <div class="panel panel-neutral-soft px-2 py-1 text-xs font-mono text-center mt-auto">Large → Medium → Small → Tiny</div>
    </div>

    <div class="panel panel-warning p-3 flex flex-col gap-2 h-full">
      <h5 class="font-medium text-heading">🎯 Task-Specific Distillation</h5>
      <p class="text-sm">
        Specialise student models for specific tasks while maintaining general teacher knowledge.
      </p>
      <div class="panel panel-neutral-soft px-2 py-1 text-xs font-mono text-center mt-auto">General Teacher → Task Expert</div>
    </div>
  </div>

  <!-- Technical Process -->
  <div class="panel panel-neutral p-4 space-y-3">
    <h4 class="font-semibold text-heading">⚙️ Distillation Process</h4>
    <div class="grid md:grid-cols-2 gap-4 text-sm">
      <div class="space-y-1">
        <h6 class="font-medium text-heading">Teacher Model (Large):</h6>
        <ul class="list-disc pl-4 space-y-1">
          <li>Generates soft probability distributions</li>
          <li>Provides rich knowledge about uncertainty</li>
          <li>Shows confidence patterns across classes</li>
          <li>Reveals dark knowledge from training</li>
        </ul>
      </div>
      <div class="space-y-1">
        <h6 class="font-medium text-heading">Student Model (Small):</h6>
        <ul class="list-disc pl-4 space-y-1">
          <li>Learns from teacher probability distributions</li>
          <li>Mimics teacher confidence patterns</li>
          <li>Compressed architecture with fewer parameters</li>
          <li>Optimised for target deployment environment</li>
        </ul>
      </div>
    </div>
  </div>

  <!-- Advanced Techniques -->
  <div class="grid md:grid-cols-2 gap-4">
    <div class="panel panel-info p-3 space-y-2">
      <h5 class="font-medium text-heading">🌡️ Temperature Scaling</h5>
      <p class="text-sm">
        Adjust softmax temperature to control probability distribution smoothness during distillation.
      </p>
      <div class="math-display text-xs">$$p_i = \frac{e^{z_i / T}}{\sum_j e^{z_j / T}}$$</div>
    </div>

    <div class="panel panel-success p-3 space-y-2">
      <h5 class="font-medium text-heading">🔄 Online Distillation</h5>
      <p class="text-sm">
        Train teacher and student simultaneously, with mutual learning between peer networks.
      </p>
      <div class="panel panel-neutral-soft px-2 py-1 text-xs font-mono text-center">Peer Network A ↔ Peer Network B</div>
    </div>
  </div>

  <!-- Benefits and Applications -->
  <div class="panel panel-success p-4 space-y-3">
    <h4 class="font-semibold text-heading">🎯 Key Benefits for LLMs</h4>
    <div class="grid md:grid-cols-2 gap-4 text-sm">
      <div class="space-y-1">
        <h6 class="font-medium text-heading">Performance Benefits:</h6>
        <ul class="list-disc pl-4 space-y-1">
          <li><strong>Size Reduction:</strong> 10-100x smaller models</li>
          <li><strong>Speed Increase:</strong> 5-50x faster inference</li>
          <li><strong>Memory Efficiency:</strong> Lower GPU/CPU requirements</li>
          <li><strong>Energy Savings:</strong> Reduced power consumption</li>
        </ul>
      </div>
      <div class="space-y-1">
        <h6 class="font-medium text-heading">Deployment Benefits:</h6>
        <ul class="list-disc pl-4 space-y-1">
          <li><strong>Mobile Deployment:</strong> Run on smartphones/tablets</li>
          <li><strong>Edge Computing:</strong> Local processing capabilities</li>
          <li><strong>Real-time Applications:</strong> Low-latency responses</li>
          <li><strong>Cost Reduction:</strong> Lower infrastructure costs</li>
        </ul>
      </div>
    </div>
  </div>

  <!-- Trade-offs and Considerations -->
  <div class="panel panel-warning p-4 space-y-2">
    <h4 class="font-semibold text-heading">⚖️ Trade-offs and Considerations</h4>
    <div class="text-sm space-y-2">
      <p><strong>Performance vs. Size:</strong> Smaller models typically have some performance degradation compared to teachers.</p>
      <p><strong>Task Generalisation:</strong> Distilled models may struggle with tasks very different from training data.</p>
      <p><strong>Training Complexity:</strong> Requires careful tuning of temperature, loss weights, and architectures.</p>
      <p><strong>Teacher Quality:</strong> Student performance is fundamentally limited by teacher model quality.</p>
    </div>
  </div>

  <!-- Standard Distillation Loss (Reference) -->
  <div class="panel panel-neutral p-3 space-y-2">
    <h5 class="font-medium text-heading">🧮 Standard Distillation Loss</h5>
    <p class="text-sm">Weighted combination of soft (teacher) and hard (ground-truth) terms with temperature scaling:</p>
    <div class="math-display text-sm">$$\mathcal{L}_{\mathrm{KD}} = \alpha\, T^2\, D_{KL}\!\left(p_{t}^{(T)}\,\big\|\, p_{s}^{(T)}\right) + (1-\alpha)\, \mathrm{CE}(y, p_s)$$</div>
    <p class="text-xs text-neutral-muted">Where: \(T\) is temperature, \(\alpha\) balances soft vs. hard targets, \(p_{t}^{(T)}\) and \(p_{s}^{(T)}\) are teacher/student with temperature.</p>
  </div>
</div>
