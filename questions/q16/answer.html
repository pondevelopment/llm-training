<div class="space-y-4">
  <div class="panel panel-info p-3">
    <h4 class="font-semibold mb-1">ğŸ“š Recommended reading (if these terms are new)</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><a href="#question-01" class="underline">Question 1: What is tokenization and why does it matter?</a></li>
      <li><a href="#question-12" class="underline">Question 12: Tokens vs. words â€” how are they different?</a></li>
    </ul>
  </div>

  <div class="panel panel-info p-4 space-y-2">
    <h4 class="font-semibold">ğŸ› ï¸ What are Out-of-Vocabulary (OOV) words?</h4>
    <p>Out-of-vocabulary words are the unfamiliar ingredients in a recipe. Older NLP systems replaced them with an â€œunknownâ€ token, erasing nuance. Modern subword tokenizers split unfamiliar words into smaller, known pieces so models keep meaning even when they have never seen the exact term.</p>
  </div>

  <div class="grid md:grid-cols-3 gap-4">
    <div class="panel panel-success p-3">
      <div class="space-y-1">
        <h5 class="font-medium">ğŸ§© Byte-Pair Encoding (BPE)</h5>
        <p class="text-sm text-body">Merges the most frequent character pairs to build a vocabulary of reusable subwords.</p>
      </div>
      <div class="panel panel-neutral-soft font-mono text-xs p-2">"cryptocurrency" â†’ ["crypto", "currency"]</div>
    </div>

    <div class="panel panel-accent p-3">
      <div class="space-y-1">
        <h5 class="font-medium">ğŸ¯ SentencePiece</h5>
        <p class="text-sm text-body">Treats text as raw bytes so it can tokenise any language without a pre-tokeniser.</p>
      </div>
      <div class="panel panel-neutral-soft font-mono text-xs p-2">"â–crypto" + "currency"</div>
    </div>

    <div class="panel panel-warning p-3">
      <div class="space-y-1">
        <h5 class="font-medium">ğŸ”§ WordPiece</h5>
        <p class="text-sm text-body">Selects merges that maximise likelihood and leans on continuation markers.</p>
      </div>
      <div class="panel panel-neutral-soft font-mono text-xs p-2">"crypto" + "##currency"</div>
    </div>
  </div>

  <div class="panel panel-neutral p-4 space-y-3">
    <h4 class="font-semibold">âš™ï¸ Subword tokenization workflow</h4>
    <div class="grid md:grid-cols-2 gap-4 text-sm">
      <div class="space-y-2">
        <h6 class="font-medium text-heading">Training phase</h6>
        <ul class="list-disc list-inside space-y-1 text-body">
          <li>Start with a character-level vocabulary.</li>
          <li>Count the frequency of character or byte pairs.</li>
          <li>Merge the most frequent pairs iteratively.</li>
          <li>Grow the subword inventory until the target size (for example, 50K tokens).</li>
          <li>Store merge rules and the final vocabulary.</li>
        </ul>
      </div>
      <div class="space-y-2">
        <h6 class="font-medium text-heading">Inference phase</h6>
        <ul class="list-disc list-inside space-y-1 text-body">
          <li>Apply the learned merge rules to new text.</li>
          <li>Break unfamiliar words into known subwords.</li>
          <li>Fall back to characters when no merges apply.</li>
          <li>Preserve semantic relationships via shared subpieces.</li>
          <li>Enable robust generation and understanding.</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="grid md:grid-cols-2 gap-4">
    <div class="panel panel-info p-3">
      <div class="space-y-1">
        <h5 class="font-medium">ğŸŒ Multilingual handling</h5>
        <p class="text-sm text-body">Subword vocabularies naturally capture cross-lingual patterns when languages share roots or scripts.</p>
      </div>
      <div class="panel panel-neutral-soft font-mono text-xs p-2">"Tokyo" + "é§…" â†’ ["To", "ky", "o", "é§…"]</div>
    </div>

    <div class="panel panel-success p-3">
      <div class="space-y-1">
        <h5 class="font-medium">ğŸ§ª Adaptive vocabularies</h5>
        <p class="text-sm text-body">Some models expand vocabularies during domain fine-tuning to catch specialised terms.</p>
      </div>
      <div class="panel panel-neutral-soft font-mono text-xs p-2">Medical example: "cardio" + "vascular" + "itis"</div>
    </div>
  </div>

  <div class="panel panel-warning p-4 space-y-3">
    <h4 class="font-semibold">ğŸ¯ Key benefits of subword tokenization</h4>
    <div class="grid md:grid-cols-2 gap-4 text-sm">
      <div class="space-y-2">
        <h6 class="font-medium text-heading">Robustness</h6>
        <ul class="list-disc list-inside space-y-1">
          <li><strong>No â€œUNKâ€ tokens:</strong> Every word receives a representation.</li>
          <li><strong>Morphological awareness:</strong> Captures prefixes, stems, and suffixes.</li>
          <li><strong>Cross-lingual transfer:</strong> Shares subwords across related languages.</li>
          <li><strong>Domain adaptation:</strong> Handles fast-changing terminology.</li>
        </ul>
      </div>
      <div class="space-y-2">
        <h6 class="font-medium text-heading">Efficiency</h6>
        <ul class="list-disc list-inside space-y-1">
          <li><strong>Vocabulary control:</strong> Keeps lookup tables bounded.</li>
          <li><strong>Better compression:</strong> Balances sequence length with coverage.</li>
          <li><strong>Semantic preservation:</strong> Reuses meaningful building blocks.</li>
          <li><strong>Training stability:</strong> Produces consistent token distributions.</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="panel panel-success p-4 space-y-3 text-sm">
    <h4 class="font-semibold">ğŸŒŸ Real-world OOV handling examples</h4>
    <div class="grid md:grid-cols-2 gap-4">
      <div class="space-y-2">
        <h6 class="font-medium text-heading">Technical terms</h6>
        <ul class="list-disc list-inside space-y-1">
          <li>"blockchain" â†’ ["block", "chain"]</li>
          <li>"cryptocurrency" â†’ ["crypto", "currency"]</li>
          <li>"biodegradable" â†’ ["bio", "de", "grad", "able"]</li>
          <li>"neuroscientist" â†’ ["neuro", "scientist"]</li>
        </ul>
      </div>
      <div class="space-y-2">
        <h6 class="font-medium text-heading">Proper nouns &amp; neologisms</h6>
        <ul class="list-disc list-inside space-y-1">
          <li>"COVID-19" â†’ ["COVID", "-", "19"]</li>
          <li>"PokÃ©mon" â†’ ["Po", "kÃ©", "mon"]</li>
          <li>"unfriend" â†’ ["un", "friend"]</li>
          <li>"livestream" â†’ ["live", "stream"]</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="panel panel-warning p-4 space-y-2 text-sm">
    <h4 class="font-semibold">âš ï¸ Challenges and considerations</h4>
    <p><strong>Semantic boundaries:</strong> Subword splits may ignore morphology, muddying meaning.</p>
    <p><strong>Sequence length:</strong> More tokens per word mean higher compute costs.</p>
    <p><strong>Training data bias:</strong> Vocabularies mirror the corpora they were trained on, underrepresenting niche languages or domains.</p>
    <p><strong>Consistency across tokenizers:</strong> Different merge rules can represent the same concept differently.</p>
  </div>
</div>