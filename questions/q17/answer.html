
        <div class="space-y-4">
            <!-- Recommended Reading -->
            <div class="bg-indigo-50 p-3 rounded-lg border border-indigo-200">
                <h4 class="font-semibold text-indigo-900 mb-1">üìö Recommended reading (if these terms are new)</h4>
                <ul class="list-disc ml-5 text-sm text-indigo-800 space-y-1">
                    <li><a href="#question-2" class="text-indigo-700 underline hover:text-indigo-900">Question 2: Attention mechanisms</a></li>
                </ul>
            </div>
            <!-- Main Concept Box -->
            <div class="bg-blue-50 p-4 rounded-lg border-l-4 border-blue-400">
                <h4 class="font-semibold text-blue-900 mb-2">üîÑ Transformer vs Seq2Seq Evolution</h4>
                <p class="text-blue-800">
                    Transformers revolutionized sequence-to-sequence learning by replacing sequential RNNs with parallel self-attention mechanisms. 
                    Think of it like switching from reading a book word-by-word to instantly accessing any part of the text while understanding 
                    how all parts relate to each other.
                </p>
            </div>
            
            <!-- RNN Explanation -->
            <div class="bg-gray-50 p-4 rounded-lg border-l-4 border-gray-400">
                <h4 class="font-semibold text-gray-900 mb-2">üß† What are RNNs (Recurrent Neural Networks)?</h4>
                <p class="text-gray-800 mb-2">
                    RNNs are neural networks designed for sequential data that process one element at a time, maintaining a "memory" (hidden state) 
                    that gets updated at each step. Imagine reading a sentence word by word, where you remember what you've read so far to understand the current word.
                </p>
                <div class="text-sm text-gray-700 space-y-1">
                    <div>‚Ä¢ <strong>Sequential Processing:</strong> Must process tokens one after another (can't parallelize)</div>
                    <div>‚Ä¢ <strong>Hidden State:</strong> Carries information from previous time steps</div>
                    <div>‚Ä¢ <strong>Memory Limitation:</strong> Earlier information tends to fade over long sequences</div>
                </div>
            </div>
            
            <!-- Key Improvements Comparison -->
            <div class="grid md:grid-cols-3 gap-4">
                <div class="bg-green-50 p-3 rounded border-l-4 border-green-400">
                    <h5 class="font-medium text-green-900">üöÄ Parallel Processing</h5>
                    <p class="text-sm text-green-700 mb-2">Self-attention processes all tokens simultaneously</p>
                    <code class="text-xs bg-green-100 px-1 rounded block">RNN: n sequential steps ‚Üí Transformer: 1 parallel round (per layer)</code>
                </div>
                
                <div class="bg-purple-50 p-3 rounded border-l-4 border-purple-400">
                    <h5 class="font-medium text-purple-900">üéØ Long-Range Dependencies</h5>
                    <p class="text-sm text-purple-700 mb-2">Direct attention to any position in the sequence</p>
                    <div class="math-display text-xs">$$\mathrm{Attention}(Q,K,V) = \mathrm{softmax}\!\left( \frac{QK^{T}}{\sqrt{d_k}} \right) V$$</div>
                </div>
                
                <div class="bg-orange-50 p-3 rounded border-l-4 border-orange-400">
                    <h5 class="font-medium text-orange-900">üìç Positional Encoding</h5>
                    <p class="text-sm text-orange-700 mb-2">Preserves sequence order without sequential processing</p>
                    <div class="math-display text-xs">$$\begin{align*}
                        \mathrm{PE}(pos,2i) &= \sin\!\left( pos / 10000^{\frac{2i}{d_{model}}} \right) \\
                        \mathrm{PE}(pos,2i+1) &= \cos\!\left( pos / 10000^{\frac{2i}{d_{model}}} \right)
                        \end{align*}$$</div>
                </div>
            </div>
            
            <!-- Traditional Seq2Seq Limitations -->
            <div class="bg-red-50 p-4 rounded-lg border-l-4 border-red-400">
                <h4 class="font-semibold text-red-900 mb-2">‚ö†Ô∏è Traditional Seq2Seq (RNN-based) Limitations</h4>
                <ul class="text-sm text-red-800 space-y-1">
                    <li>‚Ä¢ <strong>Sequential Bottleneck:</strong> RNNs must process tokens one by one (h‚ÇÅ ‚Üí h‚ÇÇ ‚Üí h‚ÇÉ...), preventing parallelization</li>
                    <li>‚Ä¢ <strong>Vanishing Gradients:</strong> Information from early tokens fades as it passes through many RNN steps</li>
                    <li>‚Ä¢ <strong>Fixed Context Vector:</strong> Encoder compresses entire sequence into single vector, losing details</li>
                    <li>‚Ä¢ <strong>Slow Training:</strong> Sequential nature prevents efficient GPU utilization (can't train in parallel)</li>
                    <li>‚Ä¢ <strong>Memory Bottleneck:</strong> Hidden state size limits how much information can be preserved</li>
                </ul>
            </div>

            <!-- Why This Matters -->
            <div class="bg-yellow-50 p-4 rounded-lg">
                <h4 class="font-semibold text-yellow-900 mb-2">üéØ Why This Revolution Matters</h4>
                <ul class="text-sm text-yellow-800 space-y-1">
                    <li>‚Ä¢ <strong>Training Speed:</strong> 10-100x faster training through parallelization</li>
                    <li>‚Ä¢ <strong>Model Scale:</strong> Enables billion-parameter models like GPT and BERT</li>
                    <li>‚Ä¢ <strong>Translation Quality:</strong> Better handling of long sentences and context</li>
                    <li>‚Ä¢ <strong>Transfer Learning:</strong> Pre-trained transformers work across many tasks</li>
                </ul>
            </div>
        </div>
    
