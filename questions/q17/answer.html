<div class="space-y-4">
  <div class="panel panel-info p-3">
    <h4 class="font-semibold mb-1">&#128218; Recommended reading (related)</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><a href="#question-2" class="underline">Question 2: Attention mechanisms</a></li>
    </ul>
  </div>

  <div class="panel panel-info panel-emphasis p-4 space-y-2">
    <h4 class="font-semibold">&#129504; Transformer vs Seq2Seq evolution</h4>
    <p>Transformers revolutionised sequence-to-sequence learning by replacing sequential RNNs with parallel self-attention. It is like switching from reading a book word-by-word to instantly referencing any passage while remembering how every part connects.</p>
  </div>

  <div class="panel panel-neutral p-4 space-y-2">
    <h4 class="font-semibold text-heading">&#129520; What are RNNs (Recurrent Neural Networks)?</h4>
    <p>RNNs process sequential data one element at a time, maintaining a hidden state as memory. Imagine reading a sentence word-by-word while keeping track of what you have already read to understand the next word.</p>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><strong>Sequential processing:</strong> Tokens flow strictly left-to-right, so there is no parallelism.</li>
      <li><strong>Hidden state:</strong> Information is carried forward as <code>h<sub>t</sub></code>, updated at each step.</li>
      <li><strong>Memory limitation:</strong> Signals fade across long sequences, making early words hard to recover.</li>
    </ul>
  </div>

  <div class="grid md:grid-cols-3 gap-4">
    <div class="panel panel-success panel-emphasis p-3 space-y-2 stacked-card">
      <h5 class="font-medium">&#128640; Parallel processing</h5>
      <p class="text-sm">Self-attention evaluates every token simultaneously.</p>
      <div class="panel panel-neutral-soft p-2 text-xs font-mono rounded-md mt-auto">RNN: n sequential steps &rarr; Transformer: 1 parallel round (per layer)</div>
    </div>
    <div class="panel panel-accent panel-emphasis p-3 space-y-2 stacked-card">
      <h5 class="font-medium">&#127919; Long-range dependencies</h5>
      <p class="text-sm">Each position can attend to any other position directly.</p>
      <div class="math-display text-xs mt-auto">$$\mathrm{Attention}(Q,K,V) = \mathrm{softmax} \left( \frac{QK^{T}}{\sqrt{d_k}} \right) V$$</div>
    </div>
    <div class="panel panel-warning panel-emphasis p-3 space-y-2 stacked-card">
      <h5 class="font-medium">&#128205; Positional encoding</h5>
      <p class="text-sm">Maintains order without sequential computation.</p>
      <div class="math-display text-xs mt-auto">$$\begin{align*}
\mathrm{PE}(pos,2i) &= \sin \left( \frac{pos}{10000^{\frac{2i}{d_{model}}}} \right) \\
\mathrm{PE}(pos,2i+1) &= \cos \left( \frac{pos}{10000^{\frac{2i}{d_{model}}}} \right)
\end{align*}$$</div>
    </div>
  </div>

  <div class="panel panel-warning p-4 space-y-2">
    <h4 class="font-semibold">&#9888; Traditional Seq2Seq limitations</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><strong>Sequential bottleneck:</strong> Tokens must be processed one by one (h<sub>1</sub> &rarr; h<sub>2</sub> &rarr; h<sub>3</sub>&hellip;), blocking GPU parallelism.</li>
      <li><strong>Vanishing gradients:</strong> Early information fades as it passes through many timesteps.</li>
      <li><strong>Fixed context vector:</strong> Encoder compresses the entire sentence into a single vector.</li>
      <li><strong>Slow training:</strong> Limited parallelism reduces throughput.</li>
      <li><strong>Memory ceiling:</strong> Hidden state size caps how much context can persist.</li>
    </ul>
  </div>

  <div class="panel panel-success p-4 space-y-2">
    <h4 class="font-semibold">&#127919; Why this revolution matters</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><strong>Training speed:</strong> 10&ndash;100&times; faster thanks to full parallelism.</li>
      <li><strong>Model scale:</strong> Enables billion-parameter models such as GPT and BERT.</li>
      <li><strong>Translation quality:</strong> Captures long sentences and nuanced context.</li>
      <li><strong>Transfer learning:</strong> Pre-trained transformers adapt across many tasks.</li>
    </ul>
  </div>
</div>
