<div class="space-y-4">
  <div class="panel panel-info p-3 space-y-2">
    <h4 class="font-semibold"> üìö Recommended reading (related topics)</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><a href="#question-15" class="underline">Question 15: Knowledge distillation</a></li>
      <li><a href="#question-35" class="underline">Question 35: LoRA and parameter-efficient fine-tuning</a></li>
    </ul>
  </div>

  <div class="panel panel-info p-4 space-y-2">
    <h4 class="font-semibold"> üéØ What is overfitting?</h4>
    <p class="text-sm leading-relaxed"><strong>Overfitting happens when a model memorizes the training set instead of learning general patterns.</strong> Think of a student who memorizes past exam questions but struggles when the wording changes‚Äîthe model performs brilliantly on training data yet fumbles on new inputs.</p>
  </div>

  <div class="panel panel-warning p-4 space-y-2">
    <h4 class="font-semibold"> ‚ö†Ô∏è Signs of overfitting in LLMs</h4>
    <ul class="text-sm space-y-1">
      <li><strong>Training vs validation gap:</strong> Training loss keeps falling while validation loss rises.</li>
      <li><strong>Memorization:</strong> The model repeats training snippets verbatim instead of generalizing.</li>
      <li><strong>Poor generalization:</strong> Fails on tasks that differ slightly from the fine-tuning data.</li>
      <li><strong>Input sensitivity:</strong> Tiny prompt edits cause wildly different outputs.</li>
    </ul>
  </div>

  <div class="grid md:grid-cols-3 gap-4">
    <div class="panel panel-success p-3 space-y-2">
      <h5 class="font-medium">üéØ Regularization</h5>
      <p class="text-sm">Add penalties so weights stay small and the model stays simple.</p>
      <div class="math-display text-xs">$$ \text{L2: } \lambda \lVert w \rVert_2^2 \quad \text{or L1: } \lambda \lVert w \rVert_1 $$</div>
      <p class="text-xs">Useful when you can tolerate slower convergence in exchange for stability.</p>
    </div>

    <div class="panel panel-accent p-3 space-y-2">
      <h5 class="font-medium">üé≤ Dropout</h5>
      <p class="text-sm">Randomly disables neurons during training so the network learns redundant features.</p>
      <code class="text-xs block">p = 0.1 - 0.5</code>
      <p class="text-xs">Dial it up for small datasets; dial it down when you already have heavy regularization.</p>
    </div>

    <div class="panel panel-warning p-3 space-y-2">
      <h5 class="font-medium">‚èπÔ∏è Early stopping</h5>
      <p class="text-sm">Monitor validation loss and halt training when improvements stall.</p>
      <code class="text-xs block">patience = 3 - 10 epochs</code>
      <p class="text-xs">Protects against over-training long before catastrophic overfitting appears.</p>
    </div>
  </div>

  <div class="panel panel-neutral p-4 space-y-3">
    <h4 class="font-semibold"> üõ†Ô∏è Advanced mitigation techniques</h4>
    <div class="grid md:grid-cols-2 gap-4 text-sm">
      <div class="space-y-1">
        <h5 class="font-medium">Data augmentation</h5>
        <ul class="list-disc ml-5 space-y-1 panel-muted">
          <li>Paraphrasing or synonym substitution to diversify phrasing.</li>
          <li>Back-translation pipelines for multilingual breadth.</li>
          <li>Synthetic data generation to cover weak spots.</li>
        </ul>
      </div>
      <div class="space-y-1">
        <h5 class="font-medium">Architecture tweaks</h5>
        <ul class="list-disc ml-5 space-y-1 panel-muted">
          <li>Batch normalization or LayerNorm for smoother gradients.</li>
          <li>Weight decay baked into the optimizer.</li>
          <li>Ensembles and cross-validation for robust predictions.</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="panel panel-warning p-4 space-y-2">
    <h4 class="font-semibold"> üí° Why preventing overfitting matters</h4>
    <ul class="text-sm space-y-1">
      <li><strong>Real-world performance:</strong> Production prompts rarely match the fine-tuning set.</li>
      <li><strong>Cost efficiency:</strong> Memorization wastes compute without boosting generalization.</li>
      <li><strong>Robustness:</strong> Diverse regularization builds tolerance to edge cases.</li>
      <li><strong>Trust &amp; safety:</strong> Overfit systems can amplify bias or hallucinate with high confidence.</li>
    </ul>
  </div>
</div>
