
        <div class="space-y-4">
            <!-- Recommended Reading -->
            <div class="bg-indigo-50 p-3 rounded-lg border border-indigo-200">
                <h4 class="font-semibold text-indigo-900 mb-1">üìö Recommended reading (related topics)</h4>
                <ul class="list-disc ml-5 text-sm text-indigo-800 space-y-1">
                    <li><a href="#question-15" class="text-indigo-700 underline hover:text-indigo-900">Question 15: Knowledge distillation</a></li>
                    <li><a href="#question-35" class="text-indigo-700 underline hover:text-indigo-900">Question 35: LoRA and parameter-efficient fine-tuning</a></li>
                </ul>
            </div>
            <!-- Main Concept Box -->
            <div class="bg-blue-50 p-4 rounded-lg border-l-4 border-blue-400">
                <h4 class="font-semibold text-blue-900 mb-2">üéØ What is Overfitting?</h4>
                <p class="text-blue-800">
                    Overfitting occurs when a model memorizes training data instead of learning generalizable patterns. 
                    Think of it like a student who memorizes specific exam questions but can't solve new problems with the same concepts. 
                    The model performs excellently on training data but poorly on new, unseen data.
                </p>
            </div>
            
            <!-- Overfitting Signs -->
            <div class="bg-red-50 p-4 rounded-lg border-l-4 border-red-400">
                <h4 class="font-semibold text-red-900 mb-2">‚ö†Ô∏è Signs of Overfitting in LLMs</h4>
                <ul class="text-sm text-red-800 space-y-1">
                    <li>‚Ä¢ <strong>Training vs Validation Gap:</strong> Low training loss but high validation loss</li>
                    <li>‚Ä¢ <strong>Memorization:</strong> Model repeats exact training sequences without understanding</li>
                    <li>‚Ä¢ <strong>Poor Generalization:</strong> Fails on tasks similar but not identical to training data</li>
                    <li>‚Ä¢ <strong>Sensitivity to Input:</strong> Small input changes cause dramatic output changes</li>
                </ul>
            </div>
            
            <!-- Mitigation Techniques Comparison -->
            <div class="grid md:grid-cols-3 gap-4">
                <div class="bg-green-50 p-3 rounded border-l-4 border-green-400">
                    <h5 class="font-medium text-green-900">üéõÔ∏è Regularization</h5>
                    <p class="text-sm text-green-700 mb-2">Add penalties to model complexity</p>
                        <div class="math-display text-xs">$$ \text{L2: } \lambda \lVert w \rVert_2^2 \quad \text{or L1: } \lambda \lVert w \rVert_1 $$</div>
                    <div class="text-xs text-green-600 mt-1">Prevents large weights</div>
                </div>
                
                <div class="bg-purple-50 p-3 rounded border-l-4 border-purple-400">
                    <h5 class="font-medium text-purple-900">üé≤ Dropout</h5>
                    <p class="text-sm text-purple-700 mb-2">Randomly disable neurons during training</p>
                    <code class="text-xs bg-purple-100 px-1 rounded block">p = 0.1-0.5 dropout rate</code>
                    <div class="text-xs text-purple-600 mt-1">Forces redundant learning</div>
                </div>
                
                <div class="bg-orange-50 p-3 rounded border-l-4 border-orange-400">
                    <h5 class="font-medium text-orange-900">‚èπÔ∏è Early Stopping</h5>
                    <p class="text-sm text-orange-700 mb-2">Stop training when validation plateaus</p>
                    <code class="text-xs bg-orange-100 px-1 rounded block">patience = 3-10 epochs</code>
                    <div class="text-xs text-orange-600 mt-1">Prevents overtraining</div>
                </div>
            </div>
            
            <!-- Advanced Techniques -->
            <div class="bg-gray-50 p-4 rounded-lg border-l-4 border-gray-400">
                <h4 class="font-semibold text-gray-900 mb-2">üî¨ Advanced Mitigation Techniques</h4>
                <div class="grid md:grid-cols-2 gap-4 text-sm">
                    <div>
                        <h6 class="font-medium text-gray-800">Data Augmentation:</h6>
                        <ul class="text-gray-700 text-xs space-y-1 mt-1">
                            <li>‚Ä¢ Paraphrasing and synonym replacement</li>
                            <li>‚Ä¢ Back-translation for text diversity</li>
                            <li>‚Ä¢ Synthetic data generation</li>
                        </ul>
                    </div>
                    <div>
                        <h6 class="font-medium text-gray-800">Architecture Changes:</h6>
                        <ul class="text-gray-700 text-xs space-y-1 mt-1">
                            <li>‚Ä¢ Batch normalization for stability</li>
                            <li>‚Ä¢ Weight decay in optimization</li>
                            <li>‚Ä¢ Model ensembling</li>
                        </ul>
                    </div>
                </div>
            </div>

            <!-- Why This Matters -->
            <div class="bg-yellow-50 p-4 rounded-lg">
                <h4 class="font-semibold text-yellow-900 mb-2">üéØ Why Preventing Overfitting Matters</h4>
                <ul class="text-sm text-yellow-800 space-y-1">
                    <li>‚Ä¢ <strong>Real-world Performance:</strong> Models must work on new, unseen data in production</li>
                    <li>‚Ä¢ <strong>Cost Efficiency:</strong> Overfitted models waste computational resources on memorization</li>
                    <li>‚Ä¢ <strong>Robustness:</strong> Generalized models handle edge cases and variations better</li>
                    <li>‚Ä¢ <strong>Trust & Safety:</strong> Overfitting can lead to biased or unreliable outputs</li>
                </ul>
            </div>
        </div>
    
