<div class="space-y-4">
  <div class="panel panel-info p-3 space-y-2">
    <h4 class="font-semibold">üìö Recommended reading (related topics)</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><a href="#question-07" class="underline">Question 7: Embeddings and semantic meaning</a></li>
      <li><a href="#question-10" class="underline">Question 10: Embeddings and initialization in LLMs</a></li>
      <li><a href="#question-22" class="underline">Question 22: Multi-head attention</a></li>
      <li><a href="#question-32" class="underline">Question 32: Attention score calculation</a></li>
      <li><a href="#question-46" class="underline">Question 46: Encoders vs decoders in transformers</a></li>
    </ul>
  </div>

  <div class="panel panel-info p-4 space-y-2">
    <h4 class="font-semibold">üìç What are positional encodings?</h4>
    <p class="text-sm leading-relaxed">
      Positional encodings are mathematical representations added to token embeddings to give transformers a sense of
      word order. Think of them like GPS coordinates for words&mdash;they tell the model where each word sits in the
      sequence, since self-attention naturally treats all positions equally.
    </p>
  </div>

  <div class="grid md:grid-cols-3 gap-4">
    <div class="panel panel-success p-3 space-y-2 q21-answer-card">
      <h5 class="font-medium">Sinusoidal (fixed)</h5>
      <p class="text-sm panel-muted">
        Uses sine and cosine functions with different frequencies to create unique patterns for each position.
      </p>
      <div class="math-display text-xs">
        $$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right),\; PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right)$$
      </div>
    </div>

    <div class="panel panel-accent p-3 space-y-2 q21-answer-card">
      <h5 class="font-medium">Learned embeddings</h5>
      <p class="text-sm panel-muted">
        Trainable position vectors that the model learns during training, similar to word embeddings.
      </p>
      <div class="math-display text-xs">$$ pos_{emb} = \mathrm{Embedding}(max_{len}, d_{model}) $$</div>
    </div>

    <div class="panel panel-warning p-3 space-y-2 q21-answer-card">
      <h5 class="font-medium">Relative encodings</h5>
      <p class="text-sm panel-muted">
        Focus on relative distances between tokens rather than absolute positions. Examples include RoPE (rotary) and
        ALiBi (additive bias).
      </p>
      <div class="math-display text-xs">$$\text{Attn}(q_i, k_j) \mathrel{+}= b(i-j)$$</div>
    </div>
  </div>

  <div class="panel panel-neutral p-4 space-y-2">
    <h4 class="font-semibold">üõ† Key levers</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li>Sinusoidal encodings generalise beyond training lengths because the pattern is analytic.</li>
      <li>Learned embeddings can specialise for domain-specific order patterns but cap out at training context.</li>
      <li>Relative schemes excel when the task depends on distance (e.g., dialogue or code blocks).</li>
    </ul>
  </div>

  <div class="panel panel-warning p-4">
    <h4 class="font-semibold mb-2">üéØ Why positional encodings matter</h4>
    <ul class="text-sm space-y-1">
      <li>&bull; <strong>Order awareness:</strong> Self-attention is permutation-invariant, so encodings write order into the inputs.</li>
      <li>&bull; <strong>Sequence understanding:</strong> Translation, summarisation, and reasoning collapse without positional cues.</li>
      <li>&bull; <strong>Mathematical elegance:</strong> Sinusoidal encodings allow models to extrapolate to longer sequences.</li>
      <li>&bull; <strong>Efficiency:</strong> Adding encodings is cheaper than rewriting the attention mechanism.</li>
    </ul>
  </div>
</div>
