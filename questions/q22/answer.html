<div class="space-y-4">
  <div class="panel panel-info p-3 space-y-2">
    <h4 class="font-semibold text-heading">ðŸ“š Recommended reading (related topics)</h4>
    <ul class="list-disc ml-5 text-sm text-muted space-y-1">
      <li><a href="#question-21" class="underline">Question 21: Positional encodings</a></li>
      <li><a href="#question-23" class="underline">Question 23: Softmax in attention</a></li>
      <li><a href="#question-24" class="underline">Question 24: Dot product in self-attention</a></li>
      <li><a href="#question-32" class="underline">Question 32: Attention score calculation</a></li>
    </ul>
  </div>

  <div class="panel panel-info panel-emphasis p-4 space-y-2">
    <h4 class="font-semibold text-heading">ðŸŽ¯ What is multi-head attention?</h4>
    <p class="text-body">Multi-head attention is like consulting a room of specialists at once. The model splits the query, key, and value projections into multiple heads so each one can track a different relationshipâ€”syntax, semantics, long-distance context, or entity referencesâ€”before fusing their perspectives back together.</p>
  </div>

  <div class="grid md:grid-cols-3 gap-4">
    <div class="panel panel-success p-4 flex flex-col gap-3 h-full">
      <h5 class="font-medium text-heading">Split into heads</h5>
      <p class="text-sm text-body">Partition the embedding dimension across <em>h</em> heads so each works in a smaller subspace with its own learned projections.</p>
      <div class="mt-auto space-y-2">
        <div class="math-display">$$ d_{model} = h \times d_{head} $$</div>
        <div class="math-display">$$ head_i = A\big(Q W_i^{Q},\; K W_i^{K},\; V W_i^{V}\big) $$</div>
      </div>
    </div>

    <div class="panel panel-accent p-4 flex flex-col gap-3 h-full">
      <h5 class="font-medium text-heading">Process in parallel</h5>
      <p class="text-sm text-body">Each head runs scaled dot-product attention independently, discovering patterns that match its specialization.</p>
      <div class="mt-auto">
        <div class="math-display">$$ A(Q,K,V) = \operatorname{softmax}\left(\frac{QK^{\top}}{\sqrt{d_k}}\right)V $$</div>
      </div>
    </div>

    <div class="panel panel-warning p-4 flex flex-col gap-3 h-full">
      <h5 class="font-medium text-heading">Concatenate &amp; project</h5>
      <p class="text-sm text-body">Combine the head outputs and apply a final linear projection so the model can blend the insights into the next layer.</p>
      <div class="mt-auto">
        <div class="math-display">$$ \operatorname{MHA}(Q,K,V) = \operatorname{Concat}(head_1,\ldots,head_h) W^{O} $$</div>
      </div>
    </div>
  </div>

  <div class="panel panel-warning p-4 space-y-2">
    <h4 class="font-semibold text-heading">ðŸš€ Why multi-head attention enhances LLMs</h4>
    <ul class="text-sm text-body space-y-1">
      <li><strong>Specialized focus:</strong> Different heads home in on syntax, semantics, long-range dependencies, or local patterns.</li>
      <li><strong>Redundancy &amp; robustness:</strong> Multiple perspectives reduce the odds of missing critical relationships.</li>
      <li><strong>Parallel processing:</strong> Independent heads keep computation efficient while widening the modelâ€™s field of view.</li>
      <li><strong>Rich representations:</strong> Fusing diverse attention maps yields nuanced, context-aware embeddings.</li>
    </ul>
  </div>
</div>