<div class="space-y-4">
  <div class="panel panel-info p-3">
    <h4 class="font-semibold mb-1">📚 Recommended reading</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><a href="#question-2" class="underline">Question 2: How does the attention mechanism function in transformer models?</a></li>
      <li><a href="#question-24" class="underline">Question 24: How does the dot product contribute to self-attention?</a></li>
      <li><a href="#question-32" class="underline">Question 32: How are attention scores calculated in transformers?</a></li>
      <li><a href="#question-22" class="underline">Question 22: What is multi-head attention, and how does it enhance LLMs?</a></li>
    </ul>
  </div>

  <div class="panel panel-info p-4 space-y-2">
    <h4 class="font-semibold">🎯 What is Softmax in Attention?</h4>
    <p>The softmax function converts raw attention scores (similarity values) into a probability distribution that sums to 1. Think of it like converting preference ratings into percentages—if you rate 3 restaurants as 8, 6, and 4, softmax converts these to probabilities like 65%, 24%, and 11%.</p>
  </div>

  <div class="panel panel-neutral p-4 space-y-3">
    <h4 class="font-semibold">📐 The Softmax Formula</h4>
    <div id="q23-formula" class="math-display">
      $$
      \begin{aligned}
        p_i &= \frac{e^{x_i}}{\sum_{j} e^{x_j}} \\
        p_i(T) &= \frac{e^{x_i/T}}{\sum_{j} e^{x_j/T}}
      \end{aligned}
      $$
    </div>
    <p class="text-sm text-muted">Here, x<sub>i</sub> are raw attention scores; softmax converts them into a probability distribution that sums to 1.</p>
  </div>

  <div class="grid md:grid-cols-3 gap-4">
    <div class="panel panel-info p-3 flex flex-col gap-2 h-full">
      <h5 class="font-medium">Low Temperature (T &lt; 1.0)</h5>
      <p class="text-sm">Makes probabilities more “peaked” by emphasizing differences between scores.</p>
      <span class="chip chip-info text-xs font-mono mt-auto self-start">[3, 2, 1] → [0.67, 0.24, 0.09]</span>
    </div>

    <div class="panel panel-success p-3 flex flex-col gap-2 h-full">
      <h5 class="font-medium">Normal Temperature (1.0)</h5>
      <p class="text-sm">Standard softmax behavior with a balanced probability distribution.</p>
      <span class="chip chip-success text-xs font-mono mt-auto self-start">[3, 2, 1] → [0.58, 0.31, 0.11]</span>
    </div>

    <div class="panel panel-warning p-3 flex flex-col gap-2 h-full">
      <h5 class="font-medium">High Temperature (T &gt; 1.0)</h5>
      <p class="text-sm">Flattens the probabilities, reducing differences between scores.</p>
      <span class="chip chip-warning text-xs font-mono mt-auto self-start">[3, 2, 1] → [0.42, 0.34, 0.24]</span>
    </div>
  </div>

  <div class="panel panel-warning p-4 space-y-2">
    <h4 class="font-semibold">🎯 Why This Matters</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><strong>Normalization:</strong> Ensures attention weights sum to 1, creating a valid probability distribution.</li>
      <li><strong>Differentiability:</strong> Smooth function that allows gradient-based learning during training.</li>
      <li><strong>Focus Control:</strong> Temperature parameter controls how sharply the model focuses attention.</li>
      <li><strong>Numerical Stability:</strong> Prevents extreme values from dominating the attention mechanism.</li>
      <li><strong>Interpretability:</strong> Outputs can be read as “how much to attend to each position.”</li>
    </ul>
  </div>
</div>
