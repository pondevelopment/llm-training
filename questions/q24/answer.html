<div class="space-y-4">
  <div class="panel panel-info p-3 space-y-2">
    <h4 class="font-semibold">ğŸ“š Recommended reading</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><a href="#question-2" class="underline">Question 2: How does the attention mechanism function in transformer models?</a></li>
      <li><a href="#question-23" class="underline">Question 23: How is the softmax function applied in attention mechanisms?</a></li>
      <li><a href="#question-32" class="underline">Question 32: How are attention scores calculated in transformers?</a></li>
      <li><a href="#question-22" class="underline">Question 22: What is multi-head attention, and how does it enhance LLMs?</a></li>
    </ul>
  </div>

  <div class="panel panel-info p-4 space-y-2">
    <h4 class="font-semibold">ğŸ¯ What is the dot product in self-attention?</h4>
    <p>The dot product in self-attention asks â€œHow similar are these two tokens?â€ When a transformer reads â€œThe cat sat on the matâ€, the token â€œcatâ€ attends strongly to â€œsatâ€ (an action) and weakly to â€œtheâ€ (a determiner). The dot product scores that similarity using each tokenâ€™s embedding.</p>
  </div>

  <div class="panel panel-success p-4 space-y-2">
    <h4 class="font-semibold">ğŸ’¡ Real-world analogy</h4>
    <p>Think of word vectors as personality profiles with traits like <em>formality</em>, <em>emotion</em>, <em>action</em>, <em>concreteness</em>, and <em>time</em>. Dot products check how aligned these traits are.</p>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><strong>"running" Â· "jogging"</strong> â†’ high score (nearly identical activities)</li>
      <li><strong>"happy" Â· "sad"</strong> â†’ negative score (opposite emotions)</li>
      <li><strong>"quickly" Â· "fast"</strong> â†’ high score (shared notion of speed)</li>
      <li><strong>"the" Â· "elephant"</strong> â†’ low score (function word vs concrete noun)</li>
    </ul>
  </div>

  <div class="panel panel-neutral p-4 space-y-2">
    <h4 class="font-semibold">ğŸ“ The attention score formula</h4>
    <div class="math-display" id="q24-formula">$$ s = \frac{Q \cdot K}{\sqrt{d_k}} $$</div>
    <p class="panel-muted text-sm">Q is the query vector, K is the key vector, and d<sub>k</sub> is the key dimension used for scaling.</p>
  </div>

  <div class="grid md:grid-cols-3 gap-4">
    <div class="panel panel-success p-3 space-y-2">
      <h5 class="font-medium">High similarity</h5>
      <p class="text-sm">Semantically related words reinforce each other.</p>
      <span class="chip chip-success font-mono text-xs">"king" â†’ "queen"</span>
      <div class="text-xs panel-muted">Dot product &gt; 0 â†’ aligned traits.</div>
    </div>
    <div class="panel panel-neutral-soft p-3 space-y-2">
      <h5 class="font-medium">Low similarity</h5>
      <p class="text-sm">Unrelated tokens barely interact.</p>
      <span class="chip chip-neutral font-mono text-xs">"the" â†’ "elephant"</span>
      <div class="text-xs panel-muted">Dot product â‰ˆ 0 â†’ minimal overlap.</div>
    </div>
    <div class="panel panel-warning p-3 space-y-2">
      <h5 class="font-medium">Opposite meaning</h5>
      <p class="text-sm">Contradictory words can oppose attention.</p>
      <span class="chip chip-warning font-mono text-xs">"hot" â†’ "cold"</span>
      <div class="text-xs panel-muted">Dot product &lt; 0 â†’ traits negate each other.</div>
    </div>
  </div>

  <div class="panel panel-info p-4 space-y-3">
    <h4 class="font-semibold">ğŸ“ Why scale by âˆšd<sub>k</sub>?</h4>
    <div class="grid md:grid-cols-2 gap-4 text-sm">
      <div class="space-y-1">
        <h6 class="font-medium">Without scaling</h6>
        <ul class="list-disc ml-5 space-y-1">
          <li>Dot products grow with embedding dimension.</li>
          <li>Softmax becomes extremely peaked.</li>
          <li>Gradients shrink and training destabilises.</li>
          <li>Longer vectors overpower shorter ones.</li>
        </ul>
      </div>
      <div class="space-y-1">
        <h6 class="font-medium">With scaling</h6>
        <ul class="list-disc ml-5 space-y-1">
          <li>Normalises scores across dimensions.</li>
          <li>Keeps softmax smooth and interpretable.</li>
          <li>Prevents saturation in high dimensions.</li>
          <li>Maintains healthy gradient flow.</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="panel panel-warning p-4 space-y-2">
    <h4 class="font-semibold">âš¡ Computational complexity</h4>
    <div class="text-sm space-y-1">
      <p><strong>Quadratic cost O(nÂ²):</strong> Every token compares with every other token (n Ã— n dot products).</p>
      <p><strong>Memory impact:</strong> The attention matrix grows with nÂ², limiting very long contexts.</p>
      <p><strong>Scaling challenge:</strong> Doubling sequence length quadruples compute, inspiring sparse and linear attention research.</p>
    </div>
  </div>

  <div class="panel panel-warning p-4 space-y-2">
    <h4 class="font-semibold">ğŸ¯ Why dot-product attention matters</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><strong>Semantic similarity:</strong> Links tokens like "dog" â†” "barked".</li>
      <li><strong>Context disambiguation:</strong> Resolves meanings ("bank" â†” "river").</li>
      <li><strong>Grammatical structure:</strong> Connects subjects, verbs, and modifiers.</li>
      <li><strong>Long-range links:</strong> Keeps distant references coherent.</li>
      <li><strong>Efficient maths:</strong> One operation captures rich structure.</li>
    </ul>
  </div>

  <div class="panel panel-accent p-4 space-y-3">
    <h4 class="font-semibold">ğŸ”„ Alternative similarity functions</h4>
    <div class="grid md:grid-cols-2 gap-4 text-sm">
      <div class="space-y-1">
        <h6 class="font-medium">Additive attention</h6>
        <p>Uses a small feed-forward network on [Q; K] to learn non-linear similarity.</p>
        <p class="text-xs panel-muted">Flexible but slower at inference.</p>
      </div>
      <div class="space-y-1">
        <h6 class="font-medium">Cosine similarity</h6>
        <p>Normalised dot product (QÂ·K)/(âˆ¥Qâˆ¥Â·âˆ¥Kâˆ¥) that focuses on direction.</p>
        <p class="text-xs panel-muted">Ignores vector magnitude; good for normalised embeddings.</p>
      </div>
    </div>
  </div>
</div>
