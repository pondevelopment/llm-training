<div class="space-y-4">
  <div class="panel panel-info p-3 space-y-2">
    <h4 class="font-semibold">📚 Recommended reading</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><a href="#question-2" class="underline">Question 2: How does the attention mechanism function in transformer models?</a></li>
      <li><a href="#question-23" class="underline">Question 23: How is the softmax function applied in attention mechanisms?</a></li>
      <li><a href="#question-32" class="underline">Question 32: How are attention scores calculated in transformers?</a></li>
      <li><a href="#question-22" class="underline">Question 22: What is multi-head attention, and how does it enhance LLMs?</a></li>
    </ul>
  </div>

  <div class="panel panel-info p-4 space-y-2">
    <h4 class="font-semibold">🎯 What is the dot product in self-attention?</h4>
    <p>The dot product in self-attention asks “How similar are these two tokens?” When a transformer reads “The cat sat on the mat”, the token “cat” attends strongly to “sat” (an action) and weakly to “the” (a determiner). The dot product scores that similarity using each token’s embedding.</p>
  </div>

  <div class="panel panel-success p-4 space-y-2">
    <h4 class="font-semibold">💡 Real-world analogy</h4>
    <p>Think of word vectors as personality profiles with traits like <em>formality</em>, <em>emotion</em>, <em>action</em>, <em>concreteness</em>, and <em>time</em>. Dot products check how aligned these traits are.</p>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><strong>"running" · "jogging"</strong> → high score (nearly identical activities)</li>
      <li><strong>"happy" · "sad"</strong> → negative score (opposite emotions)</li>
      <li><strong>"quickly" · "fast"</strong> → high score (shared notion of speed)</li>
      <li><strong>"the" · "elephant"</strong> → low score (function word vs concrete noun)</li>
    </ul>
  </div>

  <div class="panel panel-neutral p-4 space-y-2">
    <h4 class="font-semibold">📐 The attention score formula</h4>
    <div class="math-display" id="q24-formula">$$ s = \frac{Q \cdot K}{\sqrt{d_k}} $$</div>
    <p class="panel-muted text-sm">Q is the query vector, K is the key vector, and d<sub>k</sub> is the key dimension used for scaling.</p>
  </div>

  <div class="grid md:grid-cols-3 gap-4">
    <div class="panel panel-success p-3 space-y-2">
      <h5 class="font-medium">High similarity</h5>
      <p class="text-sm">Semantically related words reinforce each other.</p>
      <span class="chip chip-success font-mono text-xs">"king" → "queen"</span>
      <div class="text-xs panel-muted">Dot product &gt; 0 → aligned traits.</div>
    </div>
    <div class="panel panel-neutral-soft p-3 space-y-2">
      <h5 class="font-medium">Low similarity</h5>
      <p class="text-sm">Unrelated tokens barely interact.</p>
      <span class="chip chip-neutral font-mono text-xs">"the" → "elephant"</span>
      <div class="text-xs panel-muted">Dot product ≈ 0 → minimal overlap.</div>
    </div>
    <div class="panel panel-warning p-3 space-y-2">
      <h5 class="font-medium">Opposite meaning</h5>
      <p class="text-sm">Contradictory words can oppose attention.</p>
      <span class="chip chip-warning font-mono text-xs">"hot" → "cold"</span>
      <div class="text-xs panel-muted">Dot product &lt; 0 → traits negate each other.</div>
    </div>
  </div>

  <div class="panel panel-info p-4 space-y-3">
    <h4 class="font-semibold">📏 Why scale by √d<sub>k</sub>?</h4>
    <div class="grid md:grid-cols-2 gap-4 text-sm">
      <div class="space-y-1">
        <h6 class="font-medium">Without scaling</h6>
        <ul class="list-disc ml-5 space-y-1">
          <li>Dot products grow with embedding dimension.</li>
          <li>Softmax becomes extremely peaked.</li>
          <li>Gradients shrink and training destabilises.</li>
          <li>Longer vectors overpower shorter ones.</li>
        </ul>
      </div>
      <div class="space-y-1">
        <h6 class="font-medium">With scaling</h6>
        <ul class="list-disc ml-5 space-y-1">
          <li>Normalises scores across dimensions.</li>
          <li>Keeps softmax smooth and interpretable.</li>
          <li>Prevents saturation in high dimensions.</li>
          <li>Maintains healthy gradient flow.</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="panel panel-warning p-4 space-y-2">
    <h4 class="font-semibold">⚡ Computational complexity</h4>
    <div class="text-sm space-y-1">
      <p><strong>Quadratic cost O(n²):</strong> Every token compares with every other token (n × n dot products).</p>
      <p><strong>Memory impact:</strong> The attention matrix grows with n², limiting very long contexts.</p>
      <p><strong>Scaling challenge:</strong> Doubling sequence length quadruples compute, inspiring sparse and linear attention research.</p>
    </div>
  </div>

  <div class="panel panel-warning p-4 space-y-2">
    <h4 class="font-semibold">🎯 Why dot-product attention matters</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><strong>Semantic similarity:</strong> Links tokens like "dog" ↔ "barked".</li>
      <li><strong>Context disambiguation:</strong> Resolves meanings ("bank" ↔ "river").</li>
      <li><strong>Grammatical structure:</strong> Connects subjects, verbs, and modifiers.</li>
      <li><strong>Long-range links:</strong> Keeps distant references coherent.</li>
      <li><strong>Efficient maths:</strong> One operation captures rich structure.</li>
    </ul>
  </div>

  <div class="panel panel-accent p-4 space-y-3">
    <h4 class="font-semibold">🔄 Alternative similarity functions</h4>
    <div class="grid md:grid-cols-2 gap-4 text-sm">
      <div class="space-y-1">
        <h6 class="font-medium">Additive attention</h6>
        <p>Uses a small feed-forward network on [Q; K] to learn non-linear similarity.</p>
        <p class="text-xs panel-muted">Flexible but slower at inference.</p>
      </div>
      <div class="space-y-1">
        <h6 class="font-medium">Cosine similarity</h6>
        <p>Normalised dot product (Q·K)/(∥Q∥·∥K∥) that focuses on direction.</p>
        <p class="text-xs panel-muted">Ignores vector magnitude; good for normalised embeddings.</p>
      </div>
    </div>
  </div>
</div>
