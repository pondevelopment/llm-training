<div class="space-y-4">
  <div class="panel panel-info p-3">
    <h4 class="font-semibold mb-1">&#x1F4DA; Recommended reading</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><a href="#question-23" class="underline">Question 23: How is the softmax function applied in attention mechanisms?</a></li>
      <li><a href="#question-29" class="underline">Question 29: What is KL divergence and how is it used?</a></li>
    </ul>
  </div>

  <div class="panel panel-info panel-emphasis p-4 space-y-2">
    <h4 class="font-semibold">&#x1F3AF; What is Cross-Entropy Loss?</h4>
    <p>Cross-entropy loss measures how far the model&apos;s predicted probability distribution is from the true distribution. Think of it as a surprise meter: if the model assigns high probability to the correct word, the loss is low; if it is confidently wrong, the loss spikes.</p>
  </div>

  <div class="panel panel-success panel-emphasis p-4 space-y-2">
    <h4 class="font-semibold">&#x1F4A1; Real-World Analogy</h4>
    <p>Imagine you are a weather forecaster sharing daily odds. Cross-entropy acts like your credibility score.</p>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li>90% chance of rain and it rains &rarr; <strong>Low loss</strong> (great calibration).</li>
      <li>10% chance of rain and it rains &rarr; <strong>High loss</strong> (confidently wrong).</li>
      <li>50% chance of rain &rarr; <strong>Medium loss</strong> (hedging bets).</li>
      <li>The logarithm makes confident mistakes hurt much more than uncertainty.</li>
    </ul>
  </div>

  <div class="panel panel-neutral p-4 space-y-3">
    <h4 class="font-semibold">&#x1F4D0; The Cross-Entropy Formula</h4>
    <div id="q25-formula" class="math-display">
$$\begin{aligned}
L &= -\sum_i y_i\log(\hat{p}_i) \
\text{(one-hot targets)} &\Rightarrow L = -\log(p_{\mathrm{correct}})
\end{aligned}$$
    </div>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><strong>L</strong> &mdash; total cross-entropy loss.</li>
      <li><strong>y<sub>i</sub></strong> &mdash; true probability (1 for the correct token, 0 otherwise).</li>
      <li><strong>&#x005E;p<sub>i</sub></strong> &mdash; model probability for token <em>i</em>.</li>
      <li><strong>&#x03A3;</strong> runs over every token in the vocabulary.</li>
    </ul>
  </div>

  <div class="grid md:grid-cols-3 gap-4">
    <div class="panel panel-success panel-emphasis p-3 space-y-2">
      <h5 class="font-medium">Cross-Entropy Loss</h5>
      <p class="text-sm panel-muted">Measures distance between true and predicted probability distributions.</p>
      <span class="chip chip-success text-xs font-mono">L = -log(p_{correct})</span>
      <ul class="list-disc ml-5 text-xs space-y-1">
        <li>Probabilistically motivated.</li>
        <li>Smooth gradients for training.</li>
        <li>Handles uncertainty gracefully.</li>
      </ul>
    </div>
    <div class="panel panel-accent panel-emphasis p-3 space-y-2">
      <h5 class="font-medium">Mean Squared Error</h5>
      <p class="text-sm panel-muted">Squares the difference between predicted and true values.</p>
      <span class="chip chip-accent text-xs font-mono">L = (y - &#x005E;y)^2</span>
      <ul class="list-disc ml-5 text-xs space-y-1">
        <li>Not probability-aware.</li>
        <li>Weak gradients for rare words.</li>
        <li>Easy to interpret.</li>
      </ul>
    </div>
    <div class="panel panel-warning panel-emphasis p-3 space-y-2">
      <h5 class="font-medium">Hinge Loss</h5>
      <p class="text-sm panel-muted">Maximises the margin between correct and incorrect classes.</p>
      <span class="chip chip-warning text-xs font-mono">L = max(0, 1 - y&#x00B7;&#x005E;y)</span>
      <ul class="list-disc ml-5 text-xs space-y-1">
        <li>No probability output.</li>
        <li>Not differentiable everywhere.</li>
        <li>Popular for binary margin models.</li>
      </ul>
    </div>
  </div>

  <div class="panel panel-info p-4 space-y-4">
    <h4 class="font-semibold">&#x1FAA8; Why Cross-Entropy Works So Well</h4>
    <div class="grid md:grid-cols-2 gap-4 text-sm">
      <div class="space-y-2">
        <h6 class="font-medium">Mathematical Properties</h6>
        <ul class="list-disc ml-5 space-y-1">
          <li><strong>Well-behaved gradients:</strong> strong learning signal when the model is confidently wrong.</li>
          <li><strong>Smooth objective:</strong> continuous derivatives for gradient descent.</li>
          <li><strong>Unbounded:</strong> heavily penalises confident mistakes.</li>
          <li><strong>Probabilistic:</strong> directly maximises likelihood.</li>
        </ul>
      </div>
      <div class="space-y-2">
        <h6 class="font-medium">Training Benefits</h6>
        <ul class="list-disc ml-5 space-y-1">
          <li><strong>Fast learning:</strong> large gradients highlight mistakes.</li>
          <li><strong>Calibrated outputs:</strong> probabilities reflect confidence.</li>
          <li><strong>Resilient to imbalance:</strong> copes with skewed token frequencies.</li>
          <li><strong>Information theoretic:</strong> minimises surprise (entropy).</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="panel panel-warning panel-emphasis p-4 space-y-3">
    <h4 class="font-semibold">&#x1F4DA; In Language Modelling Context</h4>
    <ul class="list-disc ml-5 text-sm space-y-2">
      <li><strong>Next-token prediction:</strong> compare the model&apos;s distribution against a one-hot vector for the actual token.</li>
      <li><strong>Vocabulary size:</strong> scales to 50K+ outputs while rewarding confident correct predictions.</li>
      <li><strong>Autoregressive training:</strong> apply the loss at every timestep so the model learns each continuation.</li>
    </ul>
  </div>

  <div class="panel panel-accent p-4 space-y-4">
    <h4 class="font-semibold">&#x1F3B2; Loss in Action</h4>
    <div class="grid md:grid-cols-2 gap-4 text-sm">
      <div class="space-y-2">
        <h6 class="font-medium">Good prediction (low loss)</h6>
        <div class="panel panel-neutral-soft p-3 font-mono text-xs space-y-1">
          <div>Context: &quot;I love eating&quot;</div>
          <div>True next: &quot;pizza&quot; (index 1542)</div>
          <div>Model predicts:</div>
          <div>&bull; pizza: 0.75 &#x2714;</div>
          <div>&bull; food: 0.15</div>
          <div>&bull; pasta: 0.10</div>
          <div class="text-success">Loss = -log(0.75) = 0.29</div>
        </div>
      </div>
      <div class="space-y-2">
        <h6 class="font-medium">Bad prediction (high loss)</h6>
        <div class="panel panel-neutral-soft p-3 font-mono text-xs space-y-1">
          <div>Context: &quot;I love eating&quot;</div>
          <div>True next: &quot;pizza&quot; (index 1542)</div>
          <div>Model predicts:</div>
          <div>&bull; elephant: 0.60</div>
          <div>&bull; randomly: 0.35</div>
          <div>&bull; pizza: 0.05 &#x2714;</div>
          <div class="text-danger">Loss = -log(0.05) = 3.00</div>
        </div>
      </div>
    </div>
  </div>

  <div class="panel panel-warning p-4 space-y-2">
    <h4 class="font-semibold">&#x1F3AF; Why Not Other Loss Functions?</h4>
    <ul class="list-disc ml-5 text-sm space-y-2">
      <li><strong>Accuracy:</strong> not differentiable, so it cannot guide gradient descent.</li>
      <li><strong>MSE:</strong> treats probability differences linearly and ignores distribution shape.</li>
      <li><strong>KL divergence:</strong> equivalent to cross-entropy plus a constant for one-hot targets.</li>
      <li><strong>Focal loss:</strong> cross-entropy variant that emphasises hard examples.</li>
      <li><strong>Label smoothing:</strong> softens targets to prevent overconfidence while using cross-entropy.</li>
    </ul>
  </div>
</div>


