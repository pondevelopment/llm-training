<div class="space-y-4">
  <div class="panel panel-info p-3 space-y-2">
    <h4 class="font-semibold">ğŸ“š Recommended reading</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><a href="#question-7" class="underline">Question 7: What are embeddings and how do they enable semantic meaning?</a></li>
      <li><a href="#question-25" class="underline">Question 25: Why is cross-entropy loss used in language modeling?</a></li>
      <li><a href="#question-31" class="underline">Question 31: How does backpropagation work, and why is the chain rule critical?</a></li>
      <li><a href="#question-23" class="underline">Question 23: How is the softmax function applied in attention mechanisms?</a></li>
    </ul>
  </div>

  <div class="panel panel-info p-4 space-y-2">
    <h4 class="font-semibold">ğŸ”— What are Embedding Gradients?</h4>
    <p class="text-sm leading-relaxed">Embedding gradients tell us how to adjust each word's vector representation to reduce the model's prediction error. Think of it like tuning a musical instrumentâ€”if a note sounds off, the gradient shows exactly how much to tighten or loosen each string (dimension). The chain rule connects the final loss back to each embedding: âˆ‚L/âˆ‚E = (âˆ‚L/âˆ‚logits) Ã— (âˆ‚logits/âˆ‚E), allowing the model to refine word meanings through training.</p>
  </div>

  <div class="panel panel-success p-4 space-y-2">
    <h4 class="font-semibold">â›“ï¸ The Chain Rule in Action</h4>
    <p class="text-sm">During backpropagation, gradients flow backward through the network layers:</p>
    <ul class="text-sm space-y-1">
      <li>â€¢ <strong>Loss â†’ Output Layer:</strong> How much does changing the final prediction affect loss?</li>
      <li>â€¢ <strong>Output â†’ Hidden Layers:</strong> How do hidden representations impact the output?</li>
      <li>â€¢ <strong>Hidden â†’ Embeddings:</strong> How do embedding changes propagate through the network?</li>
      <li>â€¢ <strong>Chain Rule:</strong> Multiply all these partial derivatives together.</li>
    </ul>
  </div>

  <div class="panel panel-neutral p-4 space-y-3">
    <h4 class="font-semibold">ğŸ“ The Gradient Formula</h4>
    <div id="q26-formula" class="math-display">
      \[ \frac{\partial L}{\partial E} = \frac{\partial L}{\partial \text{logits}} \cdot \frac{\partial \text{logits}}{\partial E} \]
    </div>
    <div class="text-sm space-y-1">
      <p><strong>âˆ‚L/âˆ‚E</strong> = Gradient of loss with respect to embeddings</p>
      <p><strong>âˆ‚L/âˆ‚logits</strong> = How loss changes with output predictions</p>
      <p><strong>âˆ‚logits/âˆ‚E</strong> = How predictions change with embedding values</p>
      <p><strong>Chain Rule</strong> = Connects final loss to initial embeddings</p>
      <p class="small-caption panel-muted">Softmax + cross-entropy gives: \[ \frac{\partial L}{\partial \text{logits}} = \hat{p} - y \]</p>
    </div>
  </div>

  <div class="grid md:grid-cols-3 gap-4">
    <div class="panel panel-success p-3 space-y-2">
      <h5 class="font-medium">Forward Pass</h5>
      <p class="text-sm">Embeddings â†’ Hidden â†’ Logits â†’ Loss</p>
      <code class="text-xs font-mono">E â†’ h â†’ Å· â†’ L</code>
      <div class="small-caption">âœ“ Data flows forward</div>
      <div class="small-caption">âœ“ Computes predictions</div>
      <div class="small-caption">âœ“ Calculates loss</div>
    </div>
    <div class="panel panel-accent p-3 space-y-2">
      <h5 class="font-medium">Backward Pass</h5>
      <p class="text-sm">Loss â†’ Logits â†’ Hidden â†’ Embeddings</p>
      <code class="text-xs font-mono">âˆ‚L/âˆ‚L â†’ âˆ‚L/âˆ‚Å· â†’ âˆ‚L/âˆ‚h â†’ âˆ‚L/âˆ‚E</code>
      <div class="small-caption">âœ“ Gradients flow backward</div>
      <div class="small-caption">âœ“ Uses the chain rule</div>
      <div class="small-caption">âœ“ Updates parameters</div>
    </div>
    <div class="panel panel-warning p-3 space-y-2">
      <h5 class="font-medium">Parameter Update</h5>
      <p class="text-sm">Apply gradients to refine embeddings</p>
      <code class="text-xs font-mono">E<sub>new</sub> = E - Î± Ã— âˆ‚L/âˆ‚E</code>
      <div class="small-caption">âœ“ Learning rate scaling</div>
      <div class="small-caption">âœ“ Gradient descent</div>
      <div class="small-caption">âœ“ Semantic refinement</div>
    </div>
  </div>

  <div class="panel panel-info p-4 space-y-3">
    <h4 class="font-semibold">ğŸ¯ How Embeddings Learn Meaning</h4>
    <div class="grid md:grid-cols-2 gap-4 text-sm">
      <div class="space-y-1">
        <h6 class="font-medium">Gradient Calculation</h6>
        <ul class="space-y-1">
          <li>â€¢ <strong>Error Signal:</strong> Compare prediction to true answer</li>
          <li>â€¢ <strong>Backpropagate:</strong> Send error backward through layers</li>
          <li>â€¢ <strong>Chain Rule:</strong> Compute embedding contribution to error</li>
          <li>â€¢ <strong>Accumulate:</strong> Sum gradients across all examples</li>
        </ul>
      </div>
      <div class="space-y-1">
        <h6 class="font-medium">Semantic Updates</h6>
        <ul class="space-y-1">
          <li>â€¢ <strong>Context Learning:</strong> Words in similar contexts get similar vectors</li>
          <li>â€¢ <strong>Task Alignment:</strong> Embeddings adapt to specific objectives</li>
          <li>â€¢ <strong>Meaning Refinement:</strong> Subtle semantic distinctions emerge</li>
          <li>â€¢ <strong>Relationship Encoding:</strong> Analogies and patterns develop</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="panel panel-success p-4 space-y-3">
    <h4 class="font-semibold">ğŸ” Gradients in Action</h4>
    <div class="grid md:grid-cols-2 gap-4 text-sm">
      <div class="space-y-2">
        <h6 class="font-medium">Example: Predicting â€œkingâ€</h6>
        <div class="panel panel-neutral-soft p-3 font-mono text-xs space-y-1">
          <div>Context: â€œThe __ ruled the kingdomâ€</div>
          <div>True word: â€œkingâ€</div>
          <div>Model predicts: â€œqueenâ€ (wrong!)</div>
          <div class="text-danger">High loss â†’ Large gradients</div>
          <div class="text-info">Update: Make â€œkingâ€ more likely in royal contexts</div>
        </div>
      </div>
      <div class="space-y-2">
        <h6 class="font-medium">Gradient Flow Process</h6>
        <div class="panel panel-neutral-soft p-3 font-mono text-xs space-y-1">
          <div>1. L = CrossEntropy(pred, true)</div>
          <div>2. âˆ‚L/âˆ‚logits = pred âˆ’ true_one_hot</div>
          <div>3. âˆ‚L/âˆ‚hidden = âˆ‚L/âˆ‚logits Ã— W<sub>out</sub></div>
          <div>4. âˆ‚L/âˆ‚embedding = âˆ‚L/âˆ‚hidden Ã— W<sub>hidden</sub></div>
          <div class="text-success">â†’ Update embedding for â€œkingâ€</div>
        </div>
      </div>
    </div>
  </div>

  <div class="panel panel-warning p-4 space-y-2">
    <h4 class="font-semibold">âš ï¸ Gradient Computation Challenges</h4>
    <ul class="text-sm space-y-2">
      <li><strong>Sparse Updates:</strong> Only embeddings of words present in the batch receive gradients. Rare words update slowly, potentially remaining poorly trained.</li>
      <li><strong>Gradient Magnitude:</strong> Words appearing in many contexts may receive very large or very small gradients, requiring careful learning rate scheduling.</li>
      <li><strong>Memory Requirements:</strong> With vocabularies of 50K+ words and 768+ dimensions, embedding gradients can consume significant memory during training.</li>
    </ul>
  </div>

  <div class="panel panel-accent p-4 space-y-3">
    <h4 class="font-semibold">ğŸ“ˆ Training Dynamics</h4>
    <div class="grid md:grid-cols-2 gap-4 text-sm">
      <div class="space-y-1">
        <h6 class="font-medium">Early Training</h6>
        <ul class="space-y-1">
          <li>â€¢ Large gradients reshape embeddings dramatically</li>
          <li>â€¢ Basic word associations form (cat â†’ animal)</li>
          <li>â€¢ Frequent words stabilize first</li>
          <li>â€¢ Syntactic patterns emerge before semantics</li>
        </ul>
      </div>
      <div class="space-y-1">
        <h6 class="font-medium">Late Training</h6>
        <ul class="space-y-1">
          <li>â€¢ Smaller gradients fine-tune relationships</li>
          <li>â€¢ Subtle semantic distinctions develop</li>
          <li>â€¢ Rare words slowly improve</li>
          <li>â€¢ Complex analogies and reasoning patterns</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="panel panel-warning p-4 space-y-2">
    <h4 class="font-semibold">âš¡ Optimization Techniques</h4>
    <ul class="text-sm space-y-2">
      <li>â€¢ <strong>Gradient Clipping:</strong> Prevent exploding gradients from destabilizing training</li>
      <li>â€¢ <strong>Learning Rate Scheduling:</strong> Start high for exploration, decrease for fine-tuning</li>
      <li>â€¢ <strong>Weight Decay:</strong> Regularize embeddings to prevent overfitting to training data</li>
      <li>â€¢ <strong>Gradient Accumulation:</strong> Handle large batches when memory is limited</li>
      <li>â€¢ <strong>Mixed Precision:</strong> Use FP16 for gradients while maintaining FP32 for critical updates</li>
    </ul>
  </div>
</div>
