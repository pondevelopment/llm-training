<div class="space-y-6">
  <div class="panel panel-info p-3 space-y-2">
    <h4 class="font-semibold text-heading">ğŸ“š Recommended reading</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><a href="#question-31" class="underline">Question 31: How does backpropagation work, and why is the chain rule critical?</a></li>
      <li><a href="#question-26" class="underline">Question 26: How are gradients computed for embeddings in LLMs?</a></li>
      <li><a href="#question-24" class="underline">Question 24: How does the dot product contribute to self-attention?</a></li>
      <li><a href="#question-32" class="underline">Question 32: How are attention scores calculated in transformers?</a></li>
    </ul>
  </div>

  <div class="panel panel-info p-6 space-y-5 text-center">
    <div class="space-y-2">
      <h3 class="text-xl font-bold text-heading">ğŸ”— The Jacobian matrix</h3>
      <p class="text-sm md:text-base text-body max-w-3xl mx-auto">
        A mathematical tool that captures how every output changes with respect to every inputâ€”the backbone of efficient backpropagation in transformers.
      </p>
    </div>

    <div class="panel panel-neutral-soft p-5 space-y-4 max-w-3xl mx-auto">
      <p class="text-sm font-medium text-heading uppercase tracking-wide">Core definition</p>
      <div class="math-display">$$J = \frac{\partial f}{\partial x}$$</div>
      <p class="small-caption panel-muted">
        For function $f: \mathbb{R}^n \to \mathbb{R}^m$, the Jacobian $J$ is an $m \times n$ matrix showing how output $f_i$ changes with input $x_j$.
      </p>
    </div>
  </div>

  <div class="panel panel-neutral p-6 space-y-5">
    <div class="flex flex-wrap items-center gap-3 text-heading">
      <span class="chip chip-success text-xs uppercase tracking-wide">Step 1</span>
      <h4 class="text-lg font-semibold">ğŸ“ Matrix structure &amp; components</h4>
    </div>

    <div class="grid gap-4 lg:grid-cols-2">
      <div class="panel panel-success p-5 space-y-4 text-center">
        <h5 class="font-semibold text-heading">Mathematical definition</h5>
        <div class="panel panel-neutral-soft p-4 space-y-3">
          <p class="text-sm font-medium text-heading">General Jacobian matrix</p>
          <div class="math-display">$$J_{ij} = \frac{\partial f_i}{\partial x_j}$$</div>
          <p class="small-caption panel-muted">Each entry shows how output component $f_i$ changes with input $x_j$.</p>
        </div>
        <p class="small-caption panel-muted">Indices run over outputs ($i = 1 \dots m$) and inputs ($j = 1 \dots n$).</p>
      </div>

      <div class="panel panel-info p-5 space-y-3">
        <h5 class="font-semibold text-heading">In transformer context</h5>
        <div class="q27-context-item" data-tone="outputs">
          <span class="q27-context-dot" aria-hidden="true"></span>
          <div>
            <p class="text-sm font-medium text-heading">Rows (outputs)</p>
            <p class="small-caption panel-muted">Attention heads, hidden features</p>
          </div>
        </div>
        <div class="q27-context-item" data-tone="inputs">
          <span class="q27-context-dot" aria-hidden="true"></span>
          <div>
            <p class="text-sm font-medium text-heading">Columns (inputs)</p>
            <p class="small-caption panel-muted">Tokens, embeddings</p>
          </div>
        </div>
        <div class="q27-context-item" data-tone="insight">
          <span class="q27-context-dot" aria-hidden="true"></span>
          <div>
            <p class="text-sm font-medium text-heading">Purpose</p>
            <p class="small-caption panel-muted">Efficient gradient propagation</p>
          </div>
        </div>
      </div>
    </div>
  </div>

  <div class="panel panel-neutral p-6 space-y-5">
    <div class="flex flex-wrap items-center gap-3 text-heading">
      <span class="chip chip-accent text-xs uppercase tracking-wide">Step 2</span>
      <h4 class="text-lg font-semibold">â›“ï¸ Chain rule &amp; function composition</h4>
    </div>

    <p class="text-sm text-body">
      When composing functions in transformer layers, the chain rule becomes a matrix product of Jacobians across the stack.
    </p>

    <div class="grid gap-4 md:grid-cols-3">
      <div class="panel panel-accent p-4 space-y-3 text-center">
        <p class="text-sm font-medium text-heading">Function composition</p>
        <div class="panel panel-neutral-soft p-3">
          <div class="math-display">$$z = h(g(f(x)))$$</div>
        </div>
        <p class="small-caption panel-muted">Multiple transformer layers</p>
      </div>
      <div class="panel panel-accent p-4 space-y-3 text-center">
        <p class="text-sm font-medium text-heading">Chain rule</p>
        <div class="panel panel-neutral-soft p-3">
          <div class="math-display">$$\frac{\partial z}{\partial x} = \frac{\partial z}{\partial y_2} \cdot \frac{\partial y_2}{\partial y_1} \cdot \frac{\partial y_1}{\partial x}$$</div>
        </div>
        <p class="small-caption panel-muted">Gradient computation</p>
      </div>
      <div class="panel panel-accent p-4 space-y-3 text-center">
        <p class="text-sm font-medium text-heading">Jacobian form</p>
        <div class="panel panel-neutral-soft p-3">
          <div class="math-display">$$J_{total} = J_h \cdot J_g \cdot J_f$$</div>
        </div>
        <p class="small-caption panel-muted">Matrix multiplication</p>
      </div>
    </div>

    <div class="panel panel-accent panel-neutral-soft p-4 text-sm text-heading text-center">
      <strong>Key insight:</strong> each <strong>J</strong> represents the Jacobian matrix of the respective function, enabling efficient gradient flow through deep networks.
    </div>
  </div>

  <div class="panel panel-neutral p-6 space-y-4">
    <h4 class="text-lg font-semibold text-heading text-center">ğŸ¯ Key takeaways</h4>
    <div class="grid gap-4 md:grid-cols-3">
      <div class="panel panel-success panel-neutral-soft p-4 space-y-2 text-center">
        <div class="text-2xl">ğŸ“Š</div>
        <h5 class="font-semibold text-heading">Mathematical tool</h5>
        <p class="text-sm text-body">Organizes partial derivatives in matrix form: $J_{ij} = âˆ‚f_i/âˆ‚x_j$.</p>
      </div>
      <div class="panel panel-info panel-neutral-soft p-4 space-y-2 text-center">
        <div class="text-2xl">â›“ï¸</div>
        <h5 class="font-semibold text-heading">Chain rule</h5>
        <p class="text-sm text-body">Enables efficient gradient propagation: $J_{total} = J_h Â· J_g Â· J_f$.</p>
      </div>
      <div class="panel panel-accent panel-neutral-soft p-4 space-y-2 text-center">
        <div class="text-2xl">ğŸš€</div>
        <h5 class="font-semibold text-heading">Optimization</h5>
        <p class="text-sm text-body">Modern techniques use VJP to avoid storing full matrices, enabling large-scale training.</p>
      </div>
    </div>
  </div>
</div>
