<div class="space-y-6">
  <div class="panel panel-info p-4 space-y-2">
    <h4 class="font-semibold text-heading">üìö Recommended reading</h4>
    <ul class="list-disc ml-5 text-sm space-y-1 text-muted">
      <li><a href="#question-7" class="underline">Question 7: What are embeddings and how do they enable semantic meaning?</a></li>
      <li><a href="#question-26" class="underline">Question 26: How are gradients computed for embeddings in LLMs?</a></li>
      <li><a href="#question-31" class="underline">Question 31: How does backpropagation work, and why is the chain rule critical?</a></li>
      <li><a href="#question-32" class="underline">Question 32: How does SVD enable low-rank approximations and compression?</a></li>
    </ul>
  </div>

  <div class="panel panel-info p-6 space-y-6">
    <div class="text-center space-y-3">
      <h3 class="text-2xl font-semibold text-heading">üîç Eigenvalues &amp; Eigenvectors</h3>
      <p class="text-lg text-secondary max-w-3xl mx-auto">
        Mathematical tools that reveal the principal directions of data variance, enabling efficient dimensionality reduction while preserving maximum information.
      </p>
    </div>
    <div class="panel panel-neutral p-6 space-y-4 max-w-3xl mx-auto">
      <p class="text-sm font-medium text-secondary text-center">Core mathematical relationship</p>
      <div class="panel panel-info p-5 space-y-3">
        <div class="math-display">$$\mathbf{A}\mathbf{v} = \lambda\mathbf{v}$$</div>
        <p class="text-sm text-muted text-center">where \( \mathbf{A} \) is the covariance matrix, \( \mathbf{v} \) is an eigenvector, and \( \lambda \) is the eigenvalue</p>
      </div>
      <p class="text-sm text-muted text-center">Eigenvectors point to directions of maximum variance, while eigenvalues quantify the amount of variance in each direction.</p>
    </div>
  </div>

  <div class="panel panel-neutral p-6 space-y-6">
    <div class="flex items-center gap-3">
      <span class="inline-flex h-8 w-8 items-center justify-center rounded-full bg-subtle border border-divider text-heading text-sm font-semibold">1</span>
      <h4 class="text-xl font-semibold text-heading">üìê Mathematical foundation</h4>
    </div>
    <div class="grid gap-5 lg:grid-cols-2">
      <div class="panel panel-info p-5 space-y-4">
        <h5 class="font-semibold text-heading text-center">Eigenvalue problem</h5>
        <div class="panel panel-neutral-soft p-4 space-y-3">
          <p class="text-sm font-medium text-secondary text-center">Characteristic equation</p>
          <div class="math-display">$$\det(\mathbf{A} - \lambda\mathbf{I}) = 0$$</div>
          <p class="text-xs text-muted text-center">Determines eigenvalues \( \lambda_1, \lambda_2, \ldots, \lambda_n \)</p>
          <p class="text-xs text-muted text-center">Each eigenvalue represents the variance captured along its corresponding eigenvector direction.</p>
        </div>
      </div>
      <div class="panel panel-success p-5 space-y-3">
        <h5 class="font-semibold text-heading">PCA application</h5>
        <div class="panel panel-neutral-soft flex items-center gap-3 p-3">
          <span class="inline-flex h-3 w-3 rounded-full" style="background: var(--tone-purple-strong);"></span>
          <div>
            <div class="font-medium text-sm text-heading">Step 1: Covariance matrix</div>
            <p class="text-xs text-muted">Compute \( \mathbf{C} = \frac{1}{n} \mathbf{X}^\top \mathbf{X} \) to capture relationships between features.</p>
          </div>
        </div>
        <div class="panel panel-neutral-soft flex items-center gap-3 p-3">
          <span class="inline-flex h-3 w-3 rounded-full" style="background: var(--tone-sky-strong);"></span>
          <div>
            <div class="font-medium text-sm text-heading">Step 2: Eigendecomposition</div>
            <p class="text-xs text-muted">Solve \( \mathbf{C}\mathbf{v}_i = \lambda_i\mathbf{v}_i \) to find principal directions.</p>
          </div>
        </div>
        <div class="panel panel-neutral-soft flex items-center gap-3 p-3">
          <span class="inline-flex h-3 w-3 rounded-full" style="background: var(--tone-emerald-strong);"></span>
          <div>
            <div class="font-medium text-sm text-heading">Step 3: Select top-k</div>
            <p class="text-xs text-muted">Keep eigenvectors with the largest eigenvalues to maximise retained variance.</p>
          </div>
        </div>
      </div>
    </div>
  </div>

  <div class="panel panel-neutral p-6 space-y-6">
    <div class="flex items-center gap-3">
      <span class="inline-flex h-8 w-8 items-center justify-center rounded-full bg-subtle border border-divider text-heading text-sm font-semibold">2</span>
      <h4 class="text-xl font-semibold text-heading">üéØ Dimensionality reduction process</h4>
    </div>
    <div class="grid gap-4 md:grid-cols-3">
      <div class="panel panel-info p-4 h-full space-y-3 text-center">
        <div class="font-medium text-heading">1. Variance analysis</div>
        <div class="math-display">$$\text{Var}(\mathbf{X}) = \frac{1}{n}\sum_{i=1}^{n}(\mathbf{x}_i - \bar{\mathbf{x}})^2$$</div>
        <p class="text-xs text-muted">Compute the covariance matrix to understand how features co-vary.</p>
      </div>
      <div class="panel panel-info p-4 h-full space-y-3 text-center">
        <div class="font-medium text-heading">2. Principal components</div>
        <div class="math-display">$$\mathbf{W} = [\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_k]$$</div>
        <p class="text-xs text-muted">Select eigenvectors associated with the highest eigenvalues.</p>
      </div>
      <div class="panel panel-info p-4 h-full space-y-3 text-center">
        <div class="font-medium text-heading">3. Projection</div>
        <div class="math-display">$$\mathbf{Y} = \mathbf{X}\mathbf{W}$$</div>
        <p class="text-xs text-muted">Project data into a lower-dimensional space aligned with those directions.</p>
      </div>
    </div>
    <div class="panel panel-info p-4 text-sm text-center text-heading">
      <strong>Key insight:</strong> Selecting eigenvectors with the largest eigenvalues preserves the directions of maximum variance, retaining most information while shrinking dimensionality.
    </div>
  </div>

  <div class="panel panel-neutral p-6 space-y-6">
    <div class="flex items-center gap-3">
      <span class="inline-flex h-8 w-8 items-center justify-center rounded-full bg-subtle border border-divider text-heading text-sm font-semibold">3</span>
      <h4 class="text-xl font-semibold text-heading">üìä Variance preservation &amp; information loss</h4>
    </div>
    <p class="text-secondary">The eigenvalues directly quantify how much variance (information) is preserved when selecting principal components:</p>
    <div class="grid gap-5 md:grid-cols-2">
      <div class="panel panel-info p-4 space-y-3">
        <h5 class="font-semibold text-heading">Variance explained</h5>
        <div class="panel panel-neutral-soft p-4 space-y-3">
          <div class="math-display">$$\text{Variance Explained} = \frac{\sum_{i=1}^{k} \lambda_i}{\sum_{i=1}^{n} \lambda_i}$$</div>
          <p class="text-xs text-muted text-center">Ratio of selected eigenvalues to total eigenvalues.</p>
        </div>
        <p class="text-sm text-muted">Higher eigenvalues = more variance captured = less information loss.</p>
      </div>
      <div class="panel panel-info p-4 space-y-3">
        <h5 class="font-semibold text-heading">Component selection</h5>
        <div class="space-y-3">
          <div class="flex items-center justify-between text-sm text-heading">
            <span>PC1 (\( \lambda_1 = 0.6 \))</span>
            <span class="font-mono">60%</span>
          </div>
          <div class="h-2 w-full rounded-full bg-subtle">
            <div class="h-2 rounded-full" style="background: var(--tone-sky-strong); width: 60%;"></div>
          </div>
          <div class="flex items-center justify-between text-sm text-heading">
            <span>PC1-2 (\( \lambda_2 = 0.25 \))</span>
            <span class="font-mono">85%</span>
          </div>
          <div class="h-2 w-full rounded-full bg-subtle">
            <div class="h-2 rounded-full" style="background: var(--tone-emerald-strong); width: 85%;"></div>
          </div>
          <div class="flex items-center justify-between text-sm text-heading">
            <span>PC1-3 (\( \lambda_3 = 0.1 \))</span>
            <span class="font-mono">95%</span>
          </div>
          <div class="h-2 w-full rounded-full bg-subtle">
            <div class="h-2 rounded-full" style="background: var(--tone-purple-strong); width: 95%;"></div>
          </div>
        </div>
      </div>
    </div>
    <p class="text-sm text-muted text-center">Jump into the interactive below to experiment with different covariance structures and see how these variance trade-offs change in real time.</p>
  </div>

  <div class="panel panel-neutral p-6 space-y-4">
    <h4 class="text-xl font-semibold text-heading text-center">ü§ñ Applications in LLMs</h4>
    <div class="grid gap-4 md:grid-cols-3">
      <div class="panel panel-info p-4 space-y-2 text-center">
        <div class="text-2xl">üéØ</div>
        <h5 class="font-semibold text-heading">Embedding reduction</h5>
        <p class="text-sm text-secondary">Reduce high-dimensional embeddings (e.g., 768D ‚Üí 128D) while preserving semantic relationships.</p>
      </div>
      <div class="panel panel-accent p-4 space-y-2 text-center">
        <div class="text-2xl">‚ö°</div>
        <h5 class="font-semibold text-heading">Computational efficiency</h5>
        <p class="text-sm text-secondary">Lower dimensional representations reduce memory usage and speed up matrix operations.</p>
      </div>
      <div class="panel panel-success p-4 space-y-2 text-center">
        <div class="text-2xl">üîç</div>
        <h5 class="font-semibold text-heading">Feature analysis</h5>
        <p class="text-sm text-secondary">Eigenvectors reveal principal directions in embedding space, helping interpret learned representations.</p>
      </div>
    </div>
  </div>

  <div class="panel panel-warning p-6 space-y-3">
    <h4 class="text-xl font-semibold text-heading">üöÄ Why this matters</h4>
    <ul class="list-disc ml-5 text-sm text-muted space-y-2">
      <li><strong>Deploy smaller, faster models:</strong> Trimming redundant dimensions cuts latency and memory without sacrificing recall.</li>
      <li><strong>Diagnose embedding drift:</strong> Watching eigenvalues fall off alerts you when newer data is under-represented.</li>
      <li><strong>Guide feature engineering:</strong> Dominant eigenvectors highlight which combinations of signals drive model behaviour.</li>
      <li><strong>Support privacy &amp; compliance:</strong> Low-variance components are often easiest to suppress when you need to forget sensitive attributes.</li>
    </ul>
  </div>
</div>
