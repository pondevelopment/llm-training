<div class="space-y-6">
        <!-- Recommended Reading (Top) -->
        <div class="bg-indigo-50 p-3 rounded-lg border border-indigo-200">
            <h4 class="font-semibold text-indigo-900 mb-1">üìö Recommended reading</h4>
            <ul class="list-disc ml-5 text-sm text-indigo-800 space-y-1">
                <li><a href="#question-7" class="text-indigo-700 underline hover:text-indigo-900">Question 7: What are embeddings and how do they enable semantic meaning?</a></li>
                <li><a href="#question-26" class="text-indigo-700 underline hover:text-indigo-900">Question 26: How are gradients computed for embeddings in LLMs?</a></li>
                <li><a href="#question-31" class="text-indigo-700 underline hover:text-indigo-900">Question 31: How does backpropagation work, and why is the chain rule critical?</a></li>
                <li><a href="#question-32" class="text-indigo-700 underline hover:text-indigo-900">Question 32: How does SVD enable low‚Äërank approximations and compression?</a></li>
            </ul>
        </div>
        <!-- Hero Section with Clear Definition -->
        <div class="bg-gradient-to-br from-purple-50 via-indigo-50 to-blue-100 p-6 rounded-xl border border-purple-200 shadow-sm">
            <div class="text-center mb-4">
                <h3 class="text-xl font-bold text-gray-900 mb-2">üîç Eigenvalues & Eigenvectors</h3>
                <p class="text-lg text-gray-700 max-w-3xl mx-auto">
                    Mathematical tools that reveal the principal directions of data variance, enabling efficient dimensionality reduction while preserving maximum information.
                </p>
            </div>
            
            <div class="max-w-3xl mx-auto">
                <div class="bg-white p-6 rounded-lg border shadow-sm">
                    <div class="text-center space-y-4">
                        <p class="text-sm font-medium text-gray-700">Core Mathematical Relationship</p>
                        
                        <div class="bg-purple-50 p-6 rounded-lg border border-purple-200">
                            <div class="space-y-3">
                                <div class="math-display">$$\mathbf{A}\mathbf{v} = \lambda\mathbf{v}$$</div>
                                <div class="text-sm text-gray-600">where \( \mathbf{A} \) is the covariance matrix, \( \mathbf{v} \) is an eigenvector, and \( \lambda \) is the eigenvalue</div>
                            </div>
                        </div>
                        
                        <div class="text-center text-sm text-gray-600 mt-4">
                            Eigenvectors point to directions of maximum variance, while eigenvalues quantify the amount of variance in each direction
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <!-- Mathematical Foundation Section -->
        <div class="bg-white border border-gray-200 rounded-xl p-6 shadow-sm">
            <h4 class="text-lg font-bold text-gray-900 mb-4 flex items-center">
                <span class="bg-blue-100 text-blue-800 w-8 h-8 rounded-full flex items-center justify-center text-sm font-bold mr-3">1</span>
                üìê Mathematical Foundation
            </h4>
            
            <div class="grid lg:grid-cols-2 gap-6">
                <!-- Eigenvalue Equation -->
                <div class="bg-blue-50 p-5 rounded-lg border border-blue-200">
                    <h5 class="font-semibold text-blue-900 mb-4 text-center">Eigenvalue Problem</h5>
                    <div class="bg-white p-5 rounded-lg border">
                        <div class="text-center space-y-4">
                            <div class="text-sm font-medium text-gray-700">Characteristic Equation</div>
                            
                <div class="math-display">
                    $$\det(\mathbf{A} - \lambda\mathbf{I}) = 0$$
                </div>
                <div class="text-xs text-gray-600 text-center">Determines eigenvalues \( \lambda_1, \lambda_2, \ldots, \lambda_n \)</div>
                            
                            <div class="text-xs text-gray-600 max-w-xs mx-auto">
                                Each eigenvalue represents the variance captured along its corresponding eigenvector direction
                            </div>
                        </div>
                    </div>
                </div>

                <!-- PCA Connection -->
                <div class="bg-green-50 p-5 rounded-lg border border-green-200">
                    <h5 class="font-semibold text-green-900 mb-3">PCA Application</h5>
                    <div class="space-y-3">
                        <div class="bg-white p-3 rounded-lg border flex items-center space-x-3">
                            <div class="w-3 h-3 bg-purple-500 rounded-full flex-shrink-0"></div>
                            <div>
                                <div class="font-medium text-sm text-gray-900">Step 1: Covariance Matrix</div>
                                <div class="text-xs text-gray-600">\( \mathbf{C} = \frac{1}{n-1}\mathbf{X}^T\mathbf{X} \)</div>
                            </div>
                        </div>
                        <div class="bg-white p-3 rounded-lg border flex items-center space-x-3">
                            <div class="w-3 h-3 bg-blue-500 rounded-full flex-shrink-0"></div>
                            <div>
                                <div class="font-medium text-sm text-gray-900">Step 2: Eigendecomposition</div>
                                <div class="text-xs text-gray-600">\( \mathbf{C}\mathbf{v}_i = \lambda_i\mathbf{v}_i \)</div>
                            </div>
                        </div>
                        <div class="bg-white p-3 rounded-lg border flex items-center space-x-3">
                            <div class="w-3 h-3 bg-green-500 rounded-full flex-shrink-0"></div>
                            <div>
                                <div class="font-medium text-sm text-gray-900">Step 3: Select Top-k</div>
                                <div class="text-xs text-gray-600">Keep eigenvectors with largest eigenvalues</div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <!-- Dimensionality Reduction Process -->
        <div class="bg-white border border-gray-200 rounded-xl p-6 shadow-sm">
            <h4 class="text-lg font-bold text-gray-900 mb-4 flex items-center">
                <span class="bg-purple-100 text-purple-800 w-8 h-8 rounded-full flex items-center justify-center text-sm font-bold mr-3">2</span>
                üéØ Dimensionality Reduction Process
            </h4>
            
            <div class="grid md:grid-cols-3 gap-4">
                <!-- Step 1 -->
                <div class="text-center">
                    <div class="bg-purple-50 p-4 rounded-lg border border-purple-200 h-full">
                        <div class="font-medium text-purple-900 mb-3">1. Variance Analysis</div>
                        <div class="math-display">$$\text{Var}(\mathbf{X}) = \frac{1}{n}\sum_{i=1}^{n}(\mathbf{x}_i - \bar{\mathbf{x}})^2$$</div>
                        <div class="text-xs text-purple-700">Compute covariance matrix to understand data spread</div>
                    </div>
                </div>

                <!-- Step 2 -->
                <div class="text-center">
                    <div class="bg-purple-50 p-4 rounded-lg border border-purple-200 h-full">
                        <div class="font-medium text-purple-900 mb-3">2. Principal Components</div>
                        <div class="math-display">$$\mathbf{W} = [\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_k]$$</div>
                        <div class="text-xs text-purple-700">Select eigenvectors with highest eigenvalues</div>
                    </div>
                </div>

                <!-- Step 3 -->
                <div class="text-center">
                    <div class="bg-purple-50 p-4 rounded-lg border border-purple-200 h-full">
                        <div class="font-medium text-purple-900 mb-3">3. Projection</div>
                        <div class="math-display">$$\mathbf{Y} = \mathbf{X}\mathbf{W}$$</div>
                        <div class="text-xs text-purple-700">Transform data to lower-dimensional space</div>
                    </div>
                </div>
            </div>
            
            <div class="mt-6 p-4 bg-purple-50 rounded-lg border border-purple-200">
                <div class="text-sm text-purple-800 text-center">
                    <strong>Key Insight:</strong> By selecting eigenvectors with the largest eigenvalues, we preserve the directions of maximum variance, retaining most information while reducing dimensions.
                </div>
            </div>
        </div>
        
        <!-- Variance Preservation -->
        <div class="bg-white border border-gray-200 rounded-xl p-6 shadow-sm">
            <h4 class="text-lg font-bold text-gray-900 mb-4 flex items-center">
                <span class="bg-green-100 text-green-800 w-8 h-8 rounded-full flex items-center justify-center text-sm font-bold mr-3">3</span>
                üìä Variance Preservation & Information Loss
            </h4>
            
            <p class="text-gray-700 mb-6">The eigenvalues directly quantify how much variance (information) is preserved when selecting principal components:</p>
            
            <div class="grid md:grid-cols-2 gap-6">
                <!-- Variance Explained -->
                <div class="bg-white p-4 rounded-lg border">
                    <h5 class="font-semibold text-gray-900 mb-3">Variance Explained</h5>
                    <div class="bg-blue-50 p-4 rounded-lg border border-blue-200 mb-3">
                        <div class="math-display">$$\text{Variance Explained} = \frac{\sum_{i=1}^{k} \lambda_i}{\sum_{i=1}^{n} \lambda_i}$$</div>
                        <div class="text-xs text-gray-600 text-center mt-2">Ratio of selected eigenvalues to total eigenvalues</div>
                    </div>
                    <p class="text-sm text-gray-600">Higher eigenvalues = more variance captured = less information loss</p>
                </div>
                
                <!-- Cumulative Variance -->
                <div class="bg-white p-4 rounded-lg border">
                    <h5 class="font-semibold text-gray-900 mb-3">Component Selection</h5>
                    <div class="space-y-2">
                        <div class="flex justify-between text-sm">
                            <span>PC1 (\( \lambda_1 = 0.6 \))</span>
                            <span class="font-mono">60%</span>
                        </div>
                        <div class="w-full bg-gray-200 rounded-full h-2">
                            <div class="bg-blue-500 h-2 rounded-full" style="width: 60%"></div>
                        </div>
                        
                        <div class="flex justify-between text-sm">
                            <span>PC1-2 (\( \lambda_2 = 0.25 \))</span>
                            <span class="font-mono">85%</span>
                        </div>
                        <div class="w-full bg-gray-200 rounded-full h-2">
                            <div class="bg-green-500 h-2 rounded-full" style="width: 85%"></div>
                        </div>
                        
                        <div class="flex justify-between text-sm">
                            <span>PC1-3 (\( \lambda_3 = 0.1 \))</span>
                            <span class="font-mono">95%</span>
                        </div>
                        <div class="w-full bg-gray-200 rounded-full h-2">
                            <div class="bg-purple-500 h-2 rounded-full" style="width: 95%"></div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        
        <!-- LLM Applications -->
        <div class="bg-gradient-to-r from-blue-50 via-purple-50 to-indigo-50 border border-gray-200 rounded-xl p-6 shadow-sm">
            <h4 class="text-lg font-bold text-gray-900 mb-4 text-center">ü§ñ Applications in LLMs</h4>
            
            <div class="grid md:grid-cols-3 gap-4">
                <div class="bg-white p-4 rounded-lg border shadow-sm text-center">
                    <div class="text-2xl mb-2">üéØ</div>
                    <h5 class="font-semibold text-blue-900 mb-2">Embedding Reduction</h5>
                    <p class="text-sm text-gray-700">
                        Reduce high-dimensional embeddings (e.g., 768D ‚Üí 128D) while preserving semantic relationships
                    </p>
                </div>
                
                <div class="bg-white p-4 rounded-lg border shadow-sm text-center">
                    <div class="text-2xl mb-2">‚ö°</div>
                    <h5 class="font-semibold text-purple-900 mb-2">Computational Efficiency</h5>
                    <p class="text-sm text-gray-700">
                        Lower dimensional representations reduce memory usage and speed up matrix operations
                    </p>
                </div>
                
                <div class="bg-white p-4 rounded-lg border shadow-sm text-center">
                    <div class="text-2xl mb-2">üîç</div>
                    <h5 class="font-semibold text-green-900 mb-2">Feature Analysis</h5>
                    <p class="text-sm text-gray-700">
                        Eigenvectors reveal principal directions in embedding space, helping understand learned representations
                    </p>
                </div>
            </div>
        </div>
    </div>
