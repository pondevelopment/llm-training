<div class="space-y-4">
  <div class="panel panel-info p-3 space-y-2">
    <h4 class="font-semibold">📚 Recommended reading</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><a href="#question-25" class="underline">Question 25: Why is cross-entropy loss used in language modeling?</a></li>
      <li><a href="#question-23" class="underline">Question 23: How is the softmax function applied in attention mechanisms?</a></li>
      <li><a href="#question-31" class="underline">Question 31: How does backpropagation work, and why is the chain rule critical?</a></li>
      <li><a href="#question-28" class="underline">Question 28: How do eigenvalues and eigenvectors relate to dimensionality reduction?</a></li>
    </ul>
  </div>

  <div class="question-card space-y-4">
    <div class="text-center space-y-2">
      <h3 class="text-xl font-bold text-heading">📊 Kullback-Leibler (KL) Divergence</h3>
      <p class="text-body">
        A mathematical measure that quantifies how one probability distribution differs from another—essential for training and aligning language models.
      </p>
    </div>
    <div class="panel panel-info panel-emphasis p-4 space-y-3 text-sm">
      <div class="font-medium text-heading">Core mathematical definition</div>
      <div class="math-display">$$D_{KL}(P||Q) = \sum_{x} P(x) \log \frac{P(x)}{Q(x)}$$</div>
      <p class="panel-muted">where \( P(x) \) is the true distribution and \( Q(x) \) is the approximating distribution</p>
      <p class="small-caption text-muted text-center">Measures the “surprise” when Q stands in for P</p>
    </div>
  </div>

  <div class="question-card space-y-4">
    <h4 class="font-semibold text-heading flex items-center gap-2">
      <span class="chip chip-info text-xs">1</span>
      📐 Mathematical properties
    </h4>
    <div class="grid md:grid-cols-2 gap-4">
      <div class="panel panel-info panel-emphasis p-4 space-y-3">
        <h5 class="font-medium text-heading text-center">Essential properties</h5>
        <div class="panel panel-neutral p-3 space-y-2">
          <div class="text-sm font-medium text-heading">Non-negativity</div>
          <div class="math-display">$$D_{KL}(P||Q) \geq 0$$</div>
          <p class="small-caption text-muted">Zero only when the two distributions match exactly</p>
        </div>
        <div class="panel panel-neutral p-3 space-y-2">
          <div class="text-sm font-medium text-heading">Asymmetry</div>
          <div class="math-display">$$D_{KL}(P||Q) \neq D_{KL}(Q||P)$$</div>
          <p class="small-caption text-muted">Order matters, so it is not a true distance metric</p>
        </div>
      </div>
      <div class="panel panel-success panel-emphasis p-4 space-y-3">
        <h5 class="font-medium text-heading">Information-theory view</h5>
        <ul class="text-sm space-y-2 list-disc list-inside">
          <li><strong>Relative entropy:</strong> extra bits spent when encoding with Q instead of P.</li>
          <li><strong>Cross-entropy link:</strong> \(D_{KL}(P||Q) = H(P,Q) - H(P)\).</li>
          <li><strong>Information gain:</strong> quantifies what is lost when the approximation shifts.</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="question-card space-y-4">
    <h4 class="font-semibold text-heading flex items-center gap-2">
      <span class="chip chip-accent text-xs">2</span>
      🤖 Applications in large language models
    </h4>
    <div class="grid md:grid-cols-3 gap-4 text-sm">
      <div class="panel panel-accent p-4 space-y-3 text-center">
        <div class="font-medium text-heading">Knowledge distillation</div>
        <div class="math-display">$$\mathcal{L} = D_{KL}(P_{teacher}||P_{student})$$</div>
        <p class="panel-muted">Train a compact student to mimic a larger teacher by matching token distributions.</p>
      </div>
      <div class="panel panel-info p-4 space-y-3 text-center">
        <div class="font-medium text-heading">RLHF constraints</div>
        <div class="math-display">$$\mathcal{L} = \mathbb{E}[r(x,y)] - \beta D_{KL}(\pi||\pi_{ref})$$</div>
        <p class="panel-muted">Keeps the aligned policy close to a safe reference while maximising reward.</p>
      </div>
      <div class="panel panel-success p-4 space-y-3 text-center">
        <div class="font-medium text-heading">Regularisation</div>
        <div class="math-display">$$\mathcal{L} = \mathcal{L}_{task} + \lambda D_{KL}(P_{model}||P_{prior})$$</div>
        <p class="panel-muted">Prevents catastrophic drift by anchoring the model to a prior distribution.</p>
      </div>
    </div>
    <div class="panel panel-accent panel-emphasis p-3 text-sm text-center">
      <strong>Key insight:</strong> KL divergence gives a principled knob for how far an LLM may move during training.
    </div>
  </div>

  <div class="question-card space-y-4">
    <h4 class="font-semibold text-heading flex items-center gap-2">
      <span class="chip chip-warning text-xs">3</span>
      ⚙️ Practical implementation considerations
    </h4>
    <div class="grid md:grid-cols-2 gap-4">
      <div class="panel panel-neutral p-4 space-y-3">
        <h5 class="font-medium text-heading">Numerical stability</h5>
        <div class="panel panel-warning panel-emphasis p-3 space-y-2">
          <div class="math-display">$$
\begin{align*}
D_{KL}(P||Q) = & \sum P(x) \left[\log P(x) \right. \\
               & \left. - \log Q(x)\right]
\end{align*}
$$</div>
          <p class="small-caption text-muted">Compute in log-space and guard against zero probabilities.</p>
        </div>
        <ul class="text-sm text-muted space-y-1 list-disc list-inside">
          <li>Use log probabilities to avoid underflow.</li>
          <li>Add a small epsilon before taking the logarithm.</li>
          <li>Leverage <code>torch.nn.functional.kl_div</code> for stable implementations.</li>
        </ul>
      </div>
      <div class="panel panel-neutral p-4 space-y-3">
        <h5 class="font-medium text-heading">Common pitfalls</h5>
        <ul class="text-sm space-y-2 list-disc list-inside">
          <li><strong>Asymmetry confusion:</strong> always state which distribution is the reference.</li>
          <li><strong>Zero-probability issue:</strong> undefined when \(Q(x)=0\) but \(P(x)>0\).</li>
          <li><strong>Tail sensitivity:</strong> small probability mass differences can dominate the metric.</li>
        </ul>
      </div>
    </div>
  </div>

  <div class="question-card space-y-4">
    <h4 class="font-semibold text-heading text-center">🔄 Related divergence measures</h4>
    <div class="grid lg:grid-cols-3 gap-4 text-sm">
      <div class="panel panel-neutral p-4 space-y-3 text-center">
        <div class="text-2xl">↔️</div>
        <h5 class="font-medium text-heading">Jensen–Shannon divergence</h5>
        <div class="math-display">$$
\begin{align*}
JS(P,Q) = & \frac{1}{2}D_{KL}(P||M) \\
          & + \frac{1}{2}D_{KL}(Q||M)
\end{align*}
$$</div>
        <p class="panel-muted">Symmetric and bounded, ideal for comparing distributions for clustering or evaluation.</p>
      </div>
      <div class="panel panel-neutral p-4 space-y-3 text-center">
        <div class="text-2xl">🎯</div>
        <h5 class="font-medium text-heading">Reverse KL</h5>
        <div class="math-display">$$D_{KL}(Q||P)$$</div>
        <p class="panel-muted">Mode-seeking; commonly used in variational inference and GAN objectives.</p>
      </div>
      <div class="panel panel-neutral p-4 space-y-3 text-center">
        <div class="text-2xl">🌊</div>
        <h5 class="font-medium text-heading">Wasserstein distance</h5>
        <div class="math-display">$$W(P,Q) = \inf_{\gamma} \mathbb{E}[d(x,y)]$$</div>
        <p class="panel-muted">Earth-mover distance that behaves better for disjoint supports, stabilising GAN training.</p>
      </div>
    </div>
  </div>
</div>
