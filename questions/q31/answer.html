<div class="space-y-4">
        <!-- Recommended Reading -->
        <div class="bg-indigo-50 p-3 rounded-lg border border-indigo-200">
            <h4 class="font-semibold text-indigo-900 mb-1">📚 Recommended reading</h4>
            <ul class="list-disc ml-5 text-sm text-indigo-800 space-y-1">
                <li><a href="#question-24" class="text-indigo-700 underline hover:text-indigo-900">Question 24: What is gradient descent and how does it optimize neural networks?</a></li>
                <li><a href="#question-26" class="text-indigo-700 underline hover:text-indigo-900">Question 26: How are gradients computed for embeddings in LLMs?</a></li>
                <li><a href="#question-30" class="text-indigo-700 underline hover:text-indigo-900">Question 30: Why is the ReLU derivative so important for deep learning?</a></li>
                <li><a href="#question-32" class="text-indigo-700 underline hover:text-indigo-900">Question 32: How does SVD enable low-rank approximations and compression?</a></li>
            </ul>
        </div>
        <!-- Main Concept Box -->
        <div class="bg-blue-50 p-4 rounded-lg border-l-4 border-blue-400">
            <h4 class="font-semibold text-blue-900 mb-2">🔄 What is Backpropagation?</h4>
            <p class="text-blue-800">Backpropagation is the <strong>gradient computation algorithm</strong> that enables neural networks to learn. It works by propagating error signals backward through the network, using the <strong>chain rule of calculus</strong> to compute how much each parameter contributed to the final error. Think of it as tracing responsibility - when the network makes a mistake, backpropagation figures out which weights to blame and by how much.</p>
        </div>
        
        <!-- Chain Rule Mathematical Foundation -->
        <div class="bg-white p-4 rounded-lg border border-gray-200">
            <h4 class="font-semibold text-gray-900 mb-3">📊 The Chain Rule Foundation</h4>
            <div class="grid md:grid-cols-2 gap-4">
                <div class="text-center">
                    <h5 class="font-medium text-gray-800 mb-2">Basic Chain Rule</h5>
                    <div class="bg-gray-50 p-3 rounded">
                        $$\frac{\partial L}{\partial w} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial z} \cdot \frac{\partial z}{\partial w}$$
                    </div>
                    <p class="text-xs text-gray-600 mt-2">How loss L changes with weight w through intermediate variables</p>
                </div>
                <div class="text-center">
                    <h5 class="font-medium text-gray-800 mb-2">Neural Network Context</h5>
                    <div class="bg-gray-50 p-3 rounded">
                        $$\frac{\partial L}{\partial W^{(1)}} = \frac{\partial L}{\partial a^{(3)}} \cdot \frac{\partial a^{(3)}}{\partial z^{(3)}} \cdot \frac{\partial z^{(3)}}{\partial a^{(2)}} \cdot \frac{\partial a^{(2)}}{\partial z^{(2)}} \cdot \frac{\partial z^{(2)}}{\partial W^{(1)}}$$
                    </div>
                    <p class="text-xs text-gray-600 mt-2">Gradient flows through layers using chain rule</p>
                </div>
            </div>
        </div>
        
        <!-- Three-Step Process -->
        <div class="grid md:grid-cols-3 gap-4">
            <div class="bg-green-50 p-3 rounded border-l-4 border-green-400">
                <h5 class="font-medium text-green-900">1. Forward Pass</h5>
                <p class="text-sm text-green-700 mb-2">Compute predictions and store intermediate values</p>
                <div class="text-xs bg-green-100 px-2 py-1 rounded font-mono">
                    z = Wx + b → a = σ(z) → ŷ = f(a)
                </div>
                <p class="text-xs text-green-600 mt-2">✓ Cache activations for backward pass</p>
            </div>
            
            <div class="bg-purple-50 p-3 rounded border-l-4 border-purple-400">
                <h5 class="font-medium text-purple-900">2. Loss Computation</h5>
                <p class="text-sm text-purple-700 mb-2">Calculate error between prediction and target</p>
                <div class="text-xs bg-purple-100 px-2 py-1 rounded font-mono">
                    L = loss(ŷ, y_true)
                </div>
                <p class="text-xs text-purple-600 mt-2">⚠ Single scalar summarizing network performance</p>
            </div>
            
            <div class="bg-orange-50 p-3 rounded border-l-4 border-orange-400">
                <h5 class="font-medium text-orange-900">3. Backward Pass</h5>
                <p class="text-sm text-orange-700 mb-2">Propagate gradients backward layer by layer</p>
                <div class="text-xs bg-orange-100 px-2 py-1 rounded font-mono">
                    ∂L/∂W = chain_rule(∂L/∂ŷ, ∂ŷ/∂a, ∂a/∂z, ∂z/∂W)
                </div>
                <p class="text-xs text-orange-600 mt-2">🔄 Updates weights using computed gradients</p>
            </div>
        </div>
        
        <!-- Why It Matters Section -->
        <div class="bg-yellow-50 p-4 rounded-lg">
            <h4 class="font-semibold text-yellow-900 mb-2">🎯 Why Backpropagation is Revolutionary</h4>
            <ul class="text-sm text-yellow-800 space-y-1">
                <li>• <strong>Enables Learning:</strong> Without backprop, neural networks couldn't automatically adjust weights to minimize errors</li>
                <li>• <strong>Efficient Computation:</strong> Computes all gradients in one backward pass instead of separate finite differences</li>
                <li>• <strong>Scales to Deep Networks:</strong> Chain rule allows gradient computation through many layers systematically</li>
                <li>• <strong>Automatic Differentiation:</strong> Modern frameworks implement backprop automatically using computational graphs</li>
                <li>• <strong>Foundation of Deep Learning:</strong> Every neural network training relies on this gradient computation method</li>
            </ul>
        </div>
        
        <!-- Computational Graph Perspective -->
        <div class="bg-indigo-50 p-4 rounded-lg border-l-4 border-indigo-400">
            <h4 class="font-semibold text-indigo-900 mb-2">🌐 Computational Graph View</h4>
            <p class="text-sm text-indigo-800">Modern deep learning frameworks represent neural networks as <strong>computational graphs</strong> where nodes are operations and edges carry values. Forward pass flows data through the graph, backward pass flows gradients in reverse. The chain rule becomes automatic graph traversal, making backpropagation systematic and efficient.</p>
            <div class="mt-3 p-2 bg-indigo-100 rounded text-xs font-mono">
                Input → [Linear] → [ReLU] → [Linear] → [Softmax] → Output<br>
                ← ∂L/∂W₂ ← ∂L/∂a₁ ← ∂L/∂z₁ ← ∂L/∂W₁ ← ∂L/∂ŷ
            </div>
        </div>
        
        <!-- Common Challenges -->
        <div class="bg-red-50 p-4 rounded-lg border-l-4 border-red-400">
            <h4 class="font-semibold text-red-900 mb-2">⚠️ Backpropagation Challenges</h4>
            <div class="grid md:grid-cols-2 gap-3 text-sm text-red-800">
                <div>
                    <p><strong>Vanishing Gradients:</strong> Deep networks suffer when gradients become extremely small during backward propagation, making early layers learn very slowly.</p>
                </div>
                <div>
                    <p><strong>Exploding Gradients:</strong> Conversely, gradients can become exponentially large, causing unstable training and divergence.</p>
                </div>
            </div>
        </div>
    </div>
