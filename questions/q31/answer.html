<div class="space-y-4">
  <div class="panel panel-info p-3">
    <h4 class="font-semibold mb-1">📚 Recommended reading</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><a href="#question-24" class="underline">Question 24: What is gradient descent and how does it optimize neural networks?</a></li>
      <li><a href="#question-26" class="underline">Question 26: How are gradients computed for embeddings in LLMs?</a></li>
      <li><a href="#question-30" class="underline">Question 30: Why is the ReLU derivative so important for deep learning?</a></li>
      <li><a href="#question-32" class="underline">Question 32: How does SVD enable low-rank approximations and compression?</a></li>
    </ul>
  </div>

  <div class="panel panel-info panel-emphasis p-4 space-y-2">
    <h4 class="font-semibold">🔧 What is Backpropagation?</h4>
    <p>Backpropagation is the <strong>gradient computation algorithm</strong> that lets neural networks learn. It pushes the output error backward through each layer using the chain rule so every weight knows how much it contributed to the mistake. Think of it as accountability tracing: when a prediction is off, backprop assigns blame and suggests the precise adjustment for every parameter.</p>
  </div>

  <div class="panel panel-neutral p-4 space-y-4">
    <h4 class="font-semibold text-heading">📘 The Chain Rule Foundation</h4>
    <div class="grid md:grid-cols-2 gap-4">
      <div class="panel panel-neutral-soft p-3 space-y-2 text-center">
        <h5 class="font-medium text-heading">Basic Chain Rule</h5>
        <div class="math-display">
          $$\frac{\partial L}{\partial w} = \frac{\partial L}{\partial y} \cdot \frac{\partial y}{\partial z} \cdot \frac{\partial z}{\partial w}$$
        </div>
        <p class="text-xs text-muted">Measures how a parameter influences loss through intermediate variables.</p>
      </div>
      <div class="panel panel-neutral-soft p-3 space-y-2 text-center">
        <h5 class="font-medium text-heading">Neural Network Context</h5>
        <div class="math-display">
          $$\frac{\partial L}{\partial W^{(1)}} = \frac{\partial L}{\partial a^{(3)}} \cdot \frac{\partial a^{(3)}}{\partial z^{(3)}} \cdot \frac{\partial z^{(3)}}{\partial a^{(2)}} \cdot \frac{\partial a^{(2)}}{\partial z^{(2)}} \cdot \frac{\partial z^{(2)}}{\partial W^{(1)}}$$
        </div>
        <p class="text-xs text-muted">Chain rule flows through each layer, multiplying local derivatives.</p>
      </div>
    </div>
  </div>

  <div class="grid md:grid-cols-3 gap-4">
    <div class="panel panel-success panel-emphasis p-3 stacked-card">
      <h5 class="font-medium">1. Forward pass</h5>
      <p class="text-sm">Compute predictions and cache intermediate activations.</p>
      <div class="chip chip-success text-xs font-mono w-fit">z = W·x + b → a = σ(z) → y_hat = f(a)</div>
      <p class="text-xs text-muted">✓ Store activations for reuse in the backward pass.</p>
    </div>
    <div class="panel panel-accent panel-emphasis p-3 stacked-card">
      <h5 class="font-medium">2. Loss computation</h5>
      <p class="text-sm">Compare the prediction with the target to measure error.</p>
      <div class="chip chip-accent text-xs font-mono w-fit">L = loss(y_hat, y_true)</div>
      <p class="text-xs text-muted">⚠️ Produces a single scalar that summarises performance.</p>
    </div>
    <div class="panel panel-warning panel-emphasis p-3 stacked-card">
      <h5 class="font-medium">3. Backward pass</h5>
      <p class="text-sm">Propagate gradients layer by layer using the chain rule.</p>
      <div class="chip chip-warning text-xs font-mono w-fit">∂L/∂W = chain_rule(∂L/∂y, ∂y/∂a, ∂a/∂z, ∂z/∂W)</div>
      <p class="text-xs text-muted">🔧 Use gradients to update weights via gradient descent.</p>
    </div>
  </div>

  <div class="panel panel-warning p-4 space-y-2">
    <h4 class="font-semibold">🎯 Why Backpropagation is Revolutionary</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><strong>Enables learning:</strong> Without backprop, networks cannot automatically tune their parameters.</li>
      <li><strong>Efficient computation:</strong> Computes every gradient in a single backward sweep.</li>
      <li><strong>Scales to depth:</strong> The chain rule systematically handles dozens of stacked layers.</li>
      <li><strong>Powers autodiff:</strong> Modern frameworks implement backprop via computational graphs.</li>
      <li><strong>Foundation of deep learning:</strong> Every training loop depends on these gradients.</li>
    </ul>
  </div>

  <div class="panel panel-info panel-emphasis p-4 space-y-2">
    <h4 class="font-semibold">🌐 Computational graph view</h4>
    <p class="text-sm">Deep learning libraries represent models as <strong>computational graphs</strong>. The forward pass pushes values through the graph, while the backward pass walks the graph in reverse to accumulate gradients automatically.</p>
    <div class="panel panel-neutral-soft p-2 text-xs font-mono">
      Input → [Linear] → [ReLU] → [Linear] → [Softmax] → Output<br>
      ← ∂L/∂W₂ ← ∂L/∂a₁ ← ∂L/∂z₁ ← ∂L/∂W₁ ← ∂L/∂y
    </div>
  </div>

  <div class="panel panel-accent panel-emphasis p-4 space-y-3">
    <h4 class="font-semibold">⚠️ Backpropagation challenges</h4>
    <div class="grid md:grid-cols-2 gap-3 text-sm">
      <p><strong>Vanishing gradients:</strong> In deep stacks, derivatives multiply into extremely small values so early layers barely learn.</p>
      <p><strong>Exploding gradients:</strong> The opposite problem—derivatives blow up—which destabilises training and causes divergence.</p>
    </div>
  </div>
</div>
