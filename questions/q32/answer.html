<div class="space-y-4">
  <div class="panel panel-info p-3" id="q32-recommended-reading">
    <h4 class="font-semibold">üìö Recommended reading</h4>
    <ul class="list-disc ml-5 text-sm panel-muted space-y-1">
      <li><a href="#question-24" class="underline">Question 24: What is softmax and why is it used?</a></li>
      <li><a href="#question-30" class="underline">Question 30: What is the ReLU activation function?</a></li>
      <li><a href="#question-31" class="underline">Question 31: How does backpropagation work?</a></li>
      <li><a href="#question-33" class="underline">Question 33: What is multi-head attention?</a></li>
    </ul>
    <p class="small-caption panel-muted mt-2">These build conceptual grounding for scaling, softmax distribution, gradient flow, and extension to multi-head attention.</p>
  </div>

  <div class="panel panel-info panel-emphasis p-4 space-y-2">
    <h4 class="font-semibold">üéØ What is Transformer Attention?</h4>
    <p>Attention in transformers is a mechanism that allows the model to <strong>focus on different parts of the input sequence</strong> when processing each token. It's computed using three matrices: <strong>Query (Q)</strong>, <strong>Key (K)</strong>, and <strong>Value (V)</strong>. Think of it like a search engine: the Query asks "what am I looking for?", Keys answer "what information do I contain?", and Values provide "the actual information to use".</p>
  </div>

  <div class="panel panel-neutral p-4 space-y-3">
    <h4 class="font-semibold">üìä The Attention Formula</h4>
    <div class="text-center space-y-2">
      <div class="math-display">\[\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\mathbf{V}\]</div>
      <p class="text-sm panel-muted">The scaled dot-product attention mechanism that revolutionized NLP</p>
    </div>
  </div>

  <div class="grid md:grid-cols-4 gap-4">
    <div class="panel panel-info panel-emphasis p-3 space-y-2 stacked-card">
      <h5 class="font-medium">1. Dot Product</h5>
      <p class="text-sm panel-muted">Compute similarity between queries and keys</p>
      <div class="math-display">\[\mathbf{Q}\mathbf{K}^T\]</div>
      <p class="small-caption">üîç Measures relevance: higher values = more similar</p>
    </div>

    <div class="panel panel-accent panel-emphasis p-3 space-y-2 stacked-card">
      <h5 class="font-medium">2. Scale</h5>
      <p class="text-sm panel-muted">Normalize by dimension to prevent saturation</p>
      <div class="math-display">\[\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\]</div>
      <p class="small-caption">‚öñÔ∏è Prevents gradients from vanishing in softmax</p>
    </div>

    <div class="panel panel-warning panel-emphasis p-3 space-y-2 stacked-card">
      <h5 class="font-medium">3. Softmax</h5>
      <p class="text-sm panel-muted">Convert to probability distribution</p>
      <div class="math-display">\[\text{softmax}(\text{scores})\]</div>
      <p class="small-caption">üìä Creates attention weights that sum to 1</p>
    </div>

    <div class="panel panel-success panel-emphasis p-3 space-y-2 stacked-card">
      <h5 class="font-medium">4. Weighted Sum</h5>
      <p class="text-sm panel-muted">Combine values using attention weights</p>
      <div class="math-display">\[\text{weights} \cdot \mathbf{V}\]</div>
      <p class="small-caption">üéØ Final contextualized representation</p>
    </div>
  </div>

  <div class="panel panel-info p-4 space-y-3">
    <h4 class="font-semibold">üî¢ Matrix Dimensions</h4>
    <div class="text-sm space-y-2">
      <p class="panel-muted">Understanding the shapes is crucial to see why the matrix multiplications line up:</p>
      <div class="overflow-x-auto">
        <table class="w-full text-xs md:text-sm border border-divider rounded-lg overflow-hidden">
          <thead class="bg-subtle text-heading">
            <tr class="text-left">
              <th class="p-2 font-medium">Symbol</th>
              <th class="p-2 font-medium">Meaning</th>
              <th class="p-2 font-medium">Shape</th>
            </tr>
          </thead>
          <tbody>
            <tr class="border-t border-divider">
              <td class="p-2 font-mono">Q</td>
              <td class="p-2">Query vectors</td>
              <td class="p-2 font-mono">(seq_len √ó d_k)</td>
            </tr>
            <tr class="border-t border-divider">
              <td class="p-2 font-mono">K</td>
              <td class="p-2">Key vectors</td>
              <td class="p-2 font-mono">(seq_len √ó d_k)</td>
            </tr>
            <tr class="border-t border-divider">
              <td class="p-2 font-mono">V</td>
              <td class="p-2">Value vectors</td>
              <td class="p-2 font-mono">(seq_len √ó d_v)</td>
            </tr>
            <tr class="border-t border-divider bg-subtle">
              <td class="p-2 font-mono">Q K·µÄ</td>
              <td class="p-2">Similarity scores</td>
              <td class="p-2 font-mono">(seq_len √ó seq_len)</td>
            </tr>
            <tr class="border-t border-divider">
              <td class="p-2 font-mono">softmax(Q K·µÄ / ‚àöd_k)</td>
              <td class="p-2">Attention weights</td>
              <td class="p-2 font-mono">(seq_len √ó seq_len)</td>
            </tr>
            <tr class="border-t border-divider bg-subtle">
              <td class="p-2 font-mono">Attn ¬∑ V</td>
              <td class="p-2">Output representations</td>
              <td class="p-2 font-mono">(seq_len √ó d_v)</td>
            </tr>
          </tbody>
        </table>
      </div>
      <p class="small-caption panel-muted">Multiplications are valid because inner dimensions match: (seq_len √ó d_k) ¬∑ (d_k √ó seq_len) ‚Üí (seq_len √ó seq_len), then (seq_len √ó seq_len) ¬∑ (seq_len √ó d_v) ‚Üí (seq_len √ó d_v).</p>
    </div>
  </div>
</div>
