<div class="space-y-4">
  <!-- Recommended Reading (Cross-links) -->
  <div class="panel panel-info p-3 space-y-2">
    <h4 class="font-semibold">ğŸ“š Recommended reading</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><a class="underline" href="#question-15">Question 15: What is catastrophic forgetting?</a></li>
      <li><a class="underline" href="#question-16">Question 16: What is transfer learning in LLMs?</a></li>
      <li><a class="underline" href="#question-24">Question 24: What is parameter-efficient fine-tuning?</a></li>
      <li><a class="underline" href="#question-31">Question 31: How does RLHF shape model behavior?</a></li>
      <li><a class="underline" href="#question-34">Question 34: What types of foundation models exist?</a></li>
    </ul>
    <p class="small-caption panel-muted">These provide grounding for continual learning pressure, transfer dynamics, and alignment signals relevant to PEFT.</p>
  </div>

  <!-- Definition / Core Concept -->
  <div class="panel panel-info panel-emphasis p-4 space-y-3">
    <h4 class="font-semibold">ğŸ§  Core Idea</h4>
    <p class="text-sm"><strong>Parameterâ€‘Efficient Fineâ€‘Tuning (PEFT)</strong> reduces <em>catastrophic forgetting</em> by freezing most pretrained weights (Î¸<sub>base</sub>) and introducing a small, trainable subset (Î”Î¸)â€”adapters, lowâ€‘rank matrices, prefixes, bias terms. Because the original representation space stays largely intact, gradients from new tasks cannot overwrite previously learned generalizations.</p>
    <div class="grid md:grid-cols-4 gap-3 text-xs">
      <div class="panel panel-neutral-soft p-3 space-y-1">
        <span class="font-semibold text-heading">Freeze Majority</span>
        <p class="small-caption panel-muted">Protects core semantics</p>
      </div>
      <div class="panel panel-neutral-soft p-3 space-y-1">
        <span class="font-semibold text-heading">Small Delta</span>
        <p class="small-caption panel-muted">Few params adapt fast</p>
      </div>
      <div class="panel panel-neutral-soft p-3 space-y-1">
        <span class="font-semibold text-heading">Composable</span>
        <p class="small-caption panel-muted">Task adapters stack</p>
      </div>
      <div class="panel panel-neutral-soft p-3 space-y-1">
        <span class="font-semibold text-heading">Memory Efficient</span>
        <p class="small-caption panel-muted">Multiple tasks, one base</p>
      </div>
    </div>
  </div>

  <!-- Mechanism & Math -->
  <div class="panel panel-neutral p-4 space-y-3">
    <h4 class="font-semibold">ğŸ§© Mechanism</h4>
    <p class="text-sm">Instead of updating inline parameters, we add a <em>structured delta</em> to a frozen base (additive decomposition):</p>
    <p class="text-sm panel-muted"><code>f(x; Î¸_base, Î”Î¸) = f(x; Î¸_base + P(Î”Î¸))</code></p>
    <p class="text-sm">Where <code>P(Â·)</code> injects / projects the small trainable structure (adapters, lowâ€‘rank matrices, prefixes). We model forgetting with a <em>qualitative</em> saturating hazard (illustrative only):</p>
    <p class="text-sm panel-muted"><code>ForgettingRisk = 1 âˆ’ e^(âˆ’Î±Â·sÂ·dÂ·t)</code><br><code>Retention = e^(âˆ’Î±Â·sÂ·dÂ·t)</code></p>
    <p class="small-caption panel-muted">Symbols: (s)=trainable fraction; (d)=domain shift factor; (t)=sequential task count; (Î±)=method sensitivity (smaller â‡’ more robust). Pedagogical curveâ€”use empirical continual learning benchmarks for measurement.</p>
    <p class="text-sm">LoRA applies a lowâ€‘rank update to each targeted weight matrix:</p>
    <p class="text-sm panel-muted"><code>Wâ€² = W + BÂ·A; W âˆˆ â„^{d_outÃ—d_in}; A âˆˆ â„^{rÃ—d_in}; B âˆˆ â„^{d_outÃ—r}; r â‰ª d_out, d_in</code></p>
    <p class="small-caption panel-muted">Original weights (W) stay frozen; only the injected components (A,B) train â†’ prior capabilities remain accessible.</p>
  </div>

  <!-- Comparison Cards -->
  <div class="grid lg:grid-cols-4 md:grid-cols-2 gap-4 text-sm">
    <div class="panel panel-success panel-emphasis p-3 space-y-1">
      <h5 class="font-semibold">LoRA / QLoRA</h5>
      <ul class="list-disc ml-4 text-xs space-y-1 panel-muted">
        <li>Lowâ€‘rank residual update</li>
        <li>Quantize base (QLoRA) to cut memory</li>
        <li>Mergeable at inference</li>
      </ul>
    </div>
    <div class="panel panel-accent panel-emphasis p-3 space-y-1">
      <h5 class="font-semibold">Adapters</h5>
      <ul class="list-disc ml-4 text-xs space-y-1 panel-muted">
        <li>Bottleneck MLP blocks</li>
        <li>Composable per task</li>
        <li>Stable gradients</li>
      </ul>
    </div>
    <div class="panel panel-warning panel-emphasis p-3 space-y-1">
      <h5 class="font-semibold">Prefix / Pâ€‘Tuning</h5>
      <ul class="list-disc ml-4 text-xs space-y-1 panel-muted">
        <li>Train virtual tokens</li>
        <li>No weight merge needed</li>
        <li>Fast adaptation</li>
      </ul>
    </div>
    <div class="panel panel-accent panel-emphasis p-3 space-y-1">
      <h5 class="font-semibold">BitFit / Biasâ€‘Only</h5>
      <ul class="list-disc ml-4 text-xs space-y-1 panel-muted">
        <li>Update only biases</li>
        <li>Tiny footprint</li>
        <li>Lower capacity</li>
      </ul>
    </div>
  </div>

  <!-- Why It Matters -->
  <div class="panel panel-warning p-4 space-y-2">
    <h4 class="font-semibold">ğŸ¯ Why This Matters</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><strong>Preserves generalization:</strong> Frozen base prevents destructive drift.</li>
      <li><strong>Operational efficiency:</strong> Store many tasks as small deltas.</li>
      <li><strong>Rapid iteration:</strong> Faster fineâ€‘tunes & lower GPU memory.</li>
      <li><strong>Composable specialization:</strong> Swap/stack adapters per domain.</li>
    </ul>
  </div>
</div>
