<div class="space-y-4">
    <!-- Recommended Reading (Cross-links) -->
    <div class="bg-indigo-50 p-3 rounded-lg border border-indigo-200">
      <h4 class="font-semibold text-indigo-900 mb-1">ðŸ“š Recommended reading</h4>
      <ul class="list-disc ml-5 text-sm text-indigo-800 space-y-1">
        <li><a class="text-indigo-700 underline hover:text-indigo-900" href="#question-15">Question 15: What is catastrophic forgetting?</a></li>
        <li><a class="text-indigo-700 underline hover:text-indigo-900" href="#question-16">Question 16: What is transfer learning in LLMs?</a></li>
        <li><a class="text-indigo-700 underline hover:text-indigo-900" href="#question-24">Question 24: What is parameter-efficient fine-tuning?</a></li>
        <li><a class="text-indigo-700 underline hover:text-indigo-900" href="#question-31">Question 31: How does RLHF shape model behavior?</a></li>
        <li><a class="text-indigo-700 underline hover:text-indigo-900" href="#question-34">Question 34: What types of foundation models exist?</a></li>
      </ul>
  <p class="text-xs text-indigo-700 mt-2">These provide grounding for continual learning pressure, transfer dynamics, and alignment signals relevant to PEFT.</p>
    </div>
    <!-- Definition / Core Concept -->
  <div class="bg-blue-50 p-4 rounded-lg border-l-4 border-blue-400">
      <h4 class="font-semibold text-blue-900 mb-2">ðŸ§  Core Idea</h4>
      <p class="text-sm text-blue-800 leading-relaxed"><strong>Parameterâ€‘Efficient Fineâ€‘Tuning (PEFT)</strong> reduces <em>catastrophic forgetting</em> by freezing most pretrained weights (	heta_{	ext{base}}) and introducing a small, trainable subset (Delta	heta) (adapters, lowâ€‘rank matrices, prefixes, bias terms). Because the original representation space is largely preserved, gradients from new tasks cannot overwrite previously learned generalizations.</p>
      <div class="mt-3 grid md:grid-cols-4 gap-3 text-xs">
        <div class="bg-white rounded p-3 border border-blue-100"><span class="font-semibold">Freeze Majority</span><br>Protects core semantics</div>
        <div class="bg-white rounded p-3 border border-blue-100"><span class="font-semibold">Small Delta</span><br>Few params adapt fast</div>
        <div class="bg-white rounded p-3 border border-blue-100"><span class="font-semibold">Composable</span><br>Task adapters stack</div>
        <div class="bg-white rounded p-3 border border-blue-100"><span class="font-semibold">Memory Efficient</span><br>Multiple tasks, one base</div>
      </div>
    </div>

    <!-- Mechanism & Math -->
  <div class="bg-white p-4 rounded-lg border border-gray-200 shadow-sm">
      <h4 class="font-semibold text-gray-900 mb-3">ðŸ§© Mechanism</h4>
  <p class="text-sm text-gray-700">Instead of updating inline parameters, we add a <em>structured delta</em> to a frozen base (additive decomposition):</p>
  <div class="math-display">$$ f(x; 	heta_{\text{base}}, \Delta\theta) = f\big(x; 	heta_{\text{base}} + P(\Delta\theta)\big) $$</div>
  <p class="text-sm text-gray-700 mt-3">Where <code>P(cdot)</code> injects / projects the small trainable structure (adapters, lowâ€‘rank matrices, prefixes). We model forgetting with a <em>qualitative</em> saturating hazard (illustrative only):</p>
  <div class="math-display">$$\begin{aligned}
    \text{ForgettingRisk} &= 1 - e^{-\alpha s d t},\\
    \text{Retention}      &= e^{-\alpha s d t}.

  \end{aligned}$$</div>
  <p class="text-xs text-gray-600">Symbols: (s)=trainable fraction; (d)=domain shift factor; (t)=sequential task count; (alpha)=method sensitivity (smaller â‡’ more robust). Pedagogical curveâ€”use empirical continual learning benchmarks for measurement.</p>
      <p class="text-sm text-gray-700 mb-2">LoRA applies a lowâ€‘rank update to each targeted weight matrix:</p>
  <div class="math-display">$$ W' = W + B A,; W\in\mathbb{R}^{d_{out}\times d_{in}},; A\in\mathbb{R}^{r\times d_{in}},; B\in\mathbb{R}^{d_{out}\times r},; r \ll d_{out},d_{in} $$</div>
      <p class="text-xs text-gray-600">Original (W) is preserved (frozen); only (A,B) train â†’ prior capabilities remain accessible.</p>
    </div>

    <!-- Comparison Cards -->
    <div class="grid lg:grid-cols-4 md:grid-cols-2 gap-4 text-sm">
      <div class="bg-green-50 p-3 rounded border-l-4 border-green-400">
        <h5 class="font-semibold text-green-800 mb-1">LoRA / QLoRA</h5>
        <ul class="list-disc ml-4 text-xs text-green-700 space-y-1">
          <li>Lowâ€‘rank residual update</li>
          <li>Quantize base (QLoRA) to cut memory</li>
          <li>Mergeable at inference</li>
        </ul>
      </div>
      <div class="bg-purple-50 p-3 rounded border-l-4 border-purple-400">
        <h5 class="font-semibold text-purple-800 mb-1">Adapters</h5>
        <ul class="list-disc ml-4 text-xs text-purple-700 space-y-1">
          <li>Bottleneck MLP blocks</li>
          <li>Composable per task</li>
          <li>Stable gradients</li>
        </ul>
      </div>
      <div class="bg-amber-50 p-3 rounded border-l-4 border-amber-400">
        <h5 class="font-semibold text-amber-800 mb-1">Prefix / Pâ€‘Tuning</h5>
        <ul class="list-disc ml-4 text-xs text-amber-700 space-y-1">
          <li>Train virtual tokens</li>
          <li>No weight merge needed</li>
          <li>Fast adaptation</li>
        </ul>
      </div>
      <div class="bg-rose-50 p-3 rounded border-l-4 border-rose-400">
        <h5 class="font-semibold text-rose-800 mb-1">BitFit / Biasâ€‘Only</h5>
        <ul class="list-disc ml-4 text-xs text-rose-700 space-y-1">
          <li>Update only biases</li>
          <li>Tiny footprint</li>
          <li>Lower capacity</li>
        </ul>
      </div>
    </div>

    <!-- Why It Matters -->
  <div class="bg-yellow-50 p-4 rounded-lg">
      <h4 class="font-semibold text-yellow-900 mb-2">ðŸŽ¯ Why This Matters</h4>
      <ul class="text-sm text-yellow-800 space-y-1">
        <li>â€¢ <strong>Preserves generalization:</strong> Frozen base prevents destructive drift.</li>
        <li>â€¢ <strong>Operational efficiency:</strong> Store many tasks as small deltas.</li>
        <li>â€¢ <strong>Rapid iteration:</strong> Faster fineâ€‘tunes & lower GPU memory.</li>
        <li>â€¢ <strong>Composable specialization:</strong> Swap/stack adapters per domain.</li>
      </ul>
    </div>
  </div>
