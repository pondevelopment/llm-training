<div class="space-y-4">
  <!-- Recommended Reading (Cross-links) -->
  <div class="panel panel-info p-3 space-y-2">
    <h4 class="font-semibold">📚 Recommended reading</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><a class="underline" href="#question-15">Question 15: What is catastrophic forgetting?</a></li>
      <li><a class="underline" href="#question-16">Question 16: What is transfer learning in LLMs?</a></li>
      <li><a class="underline" href="#question-24">Question 24: What is parameter-efficient fine-tuning?</a></li>
      <li><a class="underline" href="#question-31">Question 31: How does RLHF shape model behavior?</a></li>
      <li><a class="underline" href="#question-34">Question 34: What types of foundation models exist?</a></li>
    </ul>
    <p class="small-caption panel-muted">These provide grounding for continual learning pressure, transfer dynamics, and alignment signals relevant to PEFT.</p>
  </div>

  <!-- Definition / Core Concept -->
  <div class="panel panel-info panel-emphasis p-4 space-y-3">
    <h4 class="font-semibold">🧠 Core Idea</h4>
    <p class="text-sm"><strong>Parameter‑Efficient Fine‑Tuning (PEFT)</strong> reduces <em>catastrophic forgetting</em> by freezing most pretrained weights (θ<sub>base</sub>) and introducing a small, trainable subset (Δθ)—adapters, low‑rank matrices, prefixes, bias terms. Because the original representation space stays largely intact, gradients from new tasks cannot overwrite previously learned generalizations.</p>
    <div class="grid md:grid-cols-4 gap-3 text-xs">
      <div class="panel panel-neutral-soft p-3 space-y-1">
        <span class="font-semibold text-heading">Freeze Majority</span>
        <p class="small-caption panel-muted">Protects core semantics</p>
      </div>
      <div class="panel panel-neutral-soft p-3 space-y-1">
        <span class="font-semibold text-heading">Small Delta</span>
        <p class="small-caption panel-muted">Few params adapt fast</p>
      </div>
      <div class="panel panel-neutral-soft p-3 space-y-1">
        <span class="font-semibold text-heading">Composable</span>
        <p class="small-caption panel-muted">Task adapters stack</p>
      </div>
      <div class="panel panel-neutral-soft p-3 space-y-1">
        <span class="font-semibold text-heading">Memory Efficient</span>
        <p class="small-caption panel-muted">Multiple tasks, one base</p>
      </div>
    </div>
  </div>

  <!-- Mechanism & Math -->
  <div class="panel panel-neutral p-4 space-y-3">
    <h4 class="font-semibold">🧩 Mechanism</h4>
    <p class="text-sm">Instead of updating inline parameters, we add a <em>structured delta</em> to a frozen base (additive decomposition):</p>
    <p class="text-sm panel-muted"><code>f(x; θ_base, Δθ) = f(x; θ_base + P(Δθ))</code></p>
    <p class="text-sm">Where <code>P(·)</code> injects / projects the small trainable structure (adapters, low‑rank matrices, prefixes). We model forgetting with a <em>qualitative</em> saturating hazard (illustrative only):</p>
    <p class="text-sm panel-muted"><code>ForgettingRisk = 1 − e^(−α·s·d·t)</code><br><code>Retention = e^(−α·s·d·t)</code></p>
    <p class="small-caption panel-muted">Symbols: (s)=trainable fraction; (d)=domain shift factor; (t)=sequential task count; (α)=method sensitivity (smaller ⇒ more robust). Pedagogical curve—use empirical continual learning benchmarks for measurement.</p>
    <p class="text-sm">LoRA applies a low‑rank update to each targeted weight matrix:</p>
    <p class="text-sm panel-muted"><code>W′ = W + B·A; W ∈ ℝ^{d_out×d_in}; A ∈ ℝ^{r×d_in}; B ∈ ℝ^{d_out×r}; r ≪ d_out, d_in</code></p>
    <p class="small-caption panel-muted">Original weights (W) stay frozen; only the injected components (A,B) train → prior capabilities remain accessible.</p>
  </div>

  <!-- Comparison Cards -->
  <div class="grid lg:grid-cols-4 md:grid-cols-2 gap-4 text-sm">
    <div class="panel panel-success panel-emphasis p-3 space-y-1">
      <h5 class="font-semibold">LoRA / QLoRA</h5>
      <ul class="list-disc ml-4 text-xs space-y-1 panel-muted">
        <li>Low‑rank residual update</li>
        <li>Quantize base (QLoRA) to cut memory</li>
        <li>Mergeable at inference</li>
      </ul>
    </div>
    <div class="panel panel-accent panel-emphasis p-3 space-y-1">
      <h5 class="font-semibold">Adapters</h5>
      <ul class="list-disc ml-4 text-xs space-y-1 panel-muted">
        <li>Bottleneck MLP blocks</li>
        <li>Composable per task</li>
        <li>Stable gradients</li>
      </ul>
    </div>
    <div class="panel panel-warning panel-emphasis p-3 space-y-1">
      <h5 class="font-semibold">Prefix / P‑Tuning</h5>
      <ul class="list-disc ml-4 text-xs space-y-1 panel-muted">
        <li>Train virtual tokens</li>
        <li>No weight merge needed</li>
        <li>Fast adaptation</li>
      </ul>
    </div>
    <div class="panel panel-accent panel-emphasis p-3 space-y-1">
      <h5 class="font-semibold">BitFit / Bias‑Only</h5>
      <ul class="list-disc ml-4 text-xs space-y-1 panel-muted">
        <li>Update only biases</li>
        <li>Tiny footprint</li>
        <li>Lower capacity</li>
      </ul>
    </div>
  </div>

  <!-- Why It Matters -->
  <div class="panel panel-warning p-4 space-y-2">
    <h4 class="font-semibold">🎯 Why This Matters</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><strong>Preserves generalization:</strong> Frozen base prevents destructive drift.</li>
      <li><strong>Operational efficiency:</strong> Store many tasks as small deltas.</li>
      <li><strong>Rapid iteration:</strong> Faster fine‑tunes & lower GPU memory.</li>
      <li><strong>Composable specialization:</strong> Swap/stack adapters per domain.</li>
    </ul>
  </div>
</div>
