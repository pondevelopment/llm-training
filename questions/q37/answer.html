<div class="space-y-4">
    <!-- Recommended Reading -->
  <div class="bg-indigo-50 p-3 rounded-lg border border-indigo-200">
    <h4 class="font-semibold text-indigo-900 mb-1">📚 Recommended reading</h4>
  <ul class="list-disc ml-5 text-sm text-indigo-800 space-y-1">
    <li><a class="text-indigo-700 underline hover:text-indigo-900" href="#question-12">Question 12: How do LLMs scale with parameters?</a></li>
    <li><a class="text-indigo-700 underline hover:text-indigo-900" href="#question-24">Question 24: What is parameter-efficient fine-tuning?</a></li>
    <li><a class="text-indigo-700 underline hover:text-indigo-900" href="#question-32">Question 32: How are attention scores computed?</a></li>
    <li><a class="text-indigo-700 underline hover:text-indigo-900" href="#question-35">Question 35: What is PEFT simulation?</a></li>
    <li><a class="text-indigo-700 underline hover:text-indigo-900" href="#question-46">Question 46: Encoders vs decoders</a></li>
    </ul>
    <p class="text-xs text-indigo-700 mt-2">Context: scaling laws, efficient adaptation, routing math, sparse activation, architecture roles.</p>
  </div>

    <!-- Core Concept -->
  <div class="bg-blue-50 p-4 rounded-lg border-l-4 border-blue-400">
    
      <h4 class="font-semibold text-blue-900 mb-2">🧩 Key Idea</h4>
      <p class="text-sm text-blue-800">Mixture of Experts (MoE) inflates <em>capacity</em> (parameters) while largely preserving <em>per‑token compute</em>: a small gating module chooses the top‑<span class="font-mono">k</span> experts out of <span class="font-mono">E</span> for each token and linearly combines their outputs.</p>
    <div class="math-display text-xs font-mono mt-3">$$\begin{aligned}
  S(x) &= \text{TopK}(\text{softmax}(W_g x)) \\
  y &= \sum_{i \in S(x)} p_i(x) f_i(x)
  \end{aligned}$$</div>
      <p class="text-xs text-blue-800 mt-2"><b>Sparsity ratio:</b> only <span class="font-mono">k/E</span> experts active → compute ∝ k, parameters ∝ E. With E≫k (e.g. 64 experts, top‑2) billions of weights sit behind a light gating decision.</p>
    </div>

    <!-- Approaches / Trade-offs -->
    <div class="grid md:grid-cols-3 gap-4 text-sm">
  <div class="bg-green-50 p-3 rounded border-l-4 border-green-400">
        <h5 class="font-semibold text-green-800 mb-1">Dense (Baseline)</h5>
        <ul class="list-disc ml-4 text-xs text-green-700 space-y-1">
          <li>All weights active</li>
          <li>Stable & simple</li>
          <li>Compute grows with params</li>
        </ul>
      </div>
  <div class="bg-purple-50 p-3 rounded border-l-4 border-purple-400">
        <h5 class="font-semibold text-purple-800 mb-1">Sparse MoE</h5>
        <ul class="list-disc ml-4 text-xs text-purple-700 space-y-1">
          <li>Top‑k routing</li>
          <li>Capacity > compute</li>
          <li>Needs balancing loss</li>
        </ul>
      </div>
  <div class="bg-amber-50 p-3 rounded border-l-4 border-amber-400">
        <h5 class="font-semibold text-amber-800 mb-1">Advanced / Hybrid</h5>
        <ul class="list-disc ml-4 text-xs text-amber-700 space-y-1">
          <li>Routing tricks (Switch, Hash)</li>
          <li>Expert parallel + sharding</li>
          <li>Memory & comms overhead</li>
        </ul>
      </div>
    </div>

    <!-- Why it matters -->
  <div class="bg-yellow-50 p-4 rounded-lg">
    
      <h4 class="font-semibold text-yellow-900 mb-2">🎯 Why This Matters</h4>
      <ul class="text-sm text-yellow-800 space-y-1">
        <li>• <b>Sparse activation</b> keeps per‑token FLOPs ~ proportional to <span class="font-mono">k</span>, not total parameters.</li>
        <li>• <b>Specialization</b> lets experts focus on sub‑domains (code, math, dialogue).</li>
        <li>• <b>Scaling headroom</b> adds experts instead of deeper/wider dense layers.</li>
        <li>• <b>Balancing strategies</b> (entropy / load losses / capacity limits) avoid collapse.</li>
      </ul>
    </div>
  </div>
