<div class="space-y-4">
  <!-- Recommended Reading -->
  <div class="panel panel-info p-3 space-y-2">
    <h4 class="font-semibold mb-1">üìö Recommended reading</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><a class="underline" href="#question-12">Question 12: How do LLMs scale with parameters?</a></li>
      <li><a class="underline" href="#question-24">Question 24: What is parameter-efficient fine-tuning?</a></li>
      <li><a class="underline" href="#question-32">Question 32: How are attention scores computed?</a></li>
      <li><a class="underline" href="#question-35">Question 35: What is PEFT simulation?</a></li>
      <li><a class="underline" href="#question-46">Question 46: Encoders vs decoders</a></li>
    </ul>
    <p class="text-xs panel-muted">Context: scaling laws, efficient adaptation, routing math, sparse activation, architecture roles.</p>
  </div>

  <!-- Core Concept -->
  <div class="panel panel-info panel-emphasis p-4 space-y-3">
    <h4 class="font-semibold">üß© Key Idea</h4>
    <p class="text-sm">Mixture of Experts (MoE) inflates <em>capacity</em> (parameters) while largely preserving <em>per-token compute</em>: a small gating module chooses the top-<span class="font-mono">k</span> experts out of <span class="font-mono">E</span> for each token and linearly combines their outputs.</p>
    <div class="math-display text-xs font-mono" aria-label="MoE gating equations">$$
  \begin{aligned}
  S(x) &= \text{TopK}(\text{softmax}(W_g x)) \
  y &= \sum_{i \in S(x)} p_i(x) f_i(x)
  \end{aligned}
$$</div>
    <p class="text-xs panel-muted"><strong>Sparsity ratio:</strong> only <span class="font-mono">k/E</span> experts active ‚Üí compute ‚àù k, parameters ‚àù E. With E‚â´k (e.g. 64 experts, top-2) billions of weights sit behind a light gating decision.</p>
  </div>

  <!-- Approaches / Trade-offs -->
  <div class="grid md:grid-cols-3 gap-4 text-sm">
    <div class="panel panel-success panel-emphasis p-3 space-y-1">
      <h5 class="font-semibold">Dense (Baseline)</h5>
      <ul class="list-disc ml-4 text-xs space-y-1">
        <li>All weights active</li>
        <li>Stable &amp; simple</li>
        <li>Compute grows with params</li>
      </ul>
    </div>
    <div class="panel panel-accent panel-emphasis p-3 space-y-1">
      <h5 class="font-semibold">Sparse MoE</h5>
      <ul class="list-disc ml-4 text-xs space-y-1">
        <li>Top-k routing</li>
        <li>Capacity &gt; compute</li>
        <li>Needs balancing loss</li>
      </ul>
    </div>
    <div class="panel panel-warning panel-emphasis p-3 space-y-1">
      <h5 class="font-semibold">Advanced / Hybrid</h5>
      <ul class="list-disc ml-4 text-xs space-y-1">
        <li>Routing tricks (Switch, Hash)</li>
        <li>Expert parallel + sharding</li>
        <li>Memory &amp; comms overhead</li>
      </ul>
    </div>
  </div>

  <!-- Why it matters -->
  <div class="panel panel-warning p-4 space-y-2">
    <h4 class="font-semibold">üéØ Why This Matters</h4>
    <ul class="text-sm space-y-1">
      <li>‚Ä¢ <strong>Sparse activation</strong> keeps per-token FLOPs ~ proportional to <span class="font-mono">k</span>, not total parameters.</li>
      <li>‚Ä¢ <strong>Specialization</strong> lets experts focus on sub-domains (code, math, dialogue).</li>
      <li>‚Ä¢ <strong>Scaling headroom</strong> adds experts instead of deeper/wider dense layers.</li>
      <li>‚Ä¢ <strong>Balancing strategies</strong> (entropy / load losses / capacity limits) avoid collapse.</li>
    </ul>
  </div>
</div>
