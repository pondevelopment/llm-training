<div class="space-y-4">
  <div class="panel panel-info p-3">
    <h4 class="font-semibold mb-1">&#x1F4DA; Recommended reading</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><a class="underline" href="#question-12">Question 12: Prompt engineering basics</a></li>
      <li><a class="underline" href="#question-31">Question 31: Temperature &amp; sampling</a></li>
      <li><a class="underline" href="#question-36">Question 36: Retrieval-Augmented Generation</a></li>
      <li><a class="underline" href="#question-38">Question 38: Chain-of-Thought prompting</a></li>
    </ul>
  </div>

  <div class="panel panel-info panel-emphasis p-4 space-y-2">
    <h4 class="font-semibold">&#x1F527; Key idea</h4>
    <p class="text-sm">
      <strong>Discriminative</strong> models learn the decision rule
      \(P(y\mid x)\) to predict labels given features (for example logistic regression or a fine-tuned BERT).
      <strong>Generative</strong> models learn how data is produced, modeling \(P(x)\) or \(P(x,y)\),
      so they can <em>sample</em> new examples (for example GPT).
    </p>
    <div class="math-display">
      $$\begin{aligned}
      \text{Discriminative: } & \; \hat{y} = \arg\max_y P(y\mid x) \
      \text{Generative: } & \; P(x) \text{ or }\; P(x,y) = P(y\mid x)P(x)
      \end{aligned}$$
    </div>
    <p class="small-caption panel-muted">Rule of thumb: Discriminative = best boundaries for labeling; Generative = learn the data distribution to generate or reason with missing parts.</p>
  </div>

  <div class="grid md:grid-cols-3 gap-4">
    <div class="panel panel-success panel-emphasis p-3 space-y-2">
      <h5 class="font-semibold">&#x1F7E2; Discriminative</h5>
      <ul class="list-disc ml-4 text-sm space-y-1">
        <li>Models \(P(y\mid x)\)</li>
        <li>Great for classification and ranking</li>
        <li>Often simpler and faster at inference</li>
        <li>Examples: Logistic regression, SVM, fine-tuned encoder</li>
      </ul>
    </div>
    <div class="panel panel-accent panel-emphasis p-3 space-y-2">
      <h5 class="font-semibold">&#x1F7E3; Generative</h5>
      <ul class="list-disc ml-4 text-sm space-y-1">
        <li>Models \(P(x)\) or \(P(x,y)\)</li>
        <li>Can sample and synthesize new data</li>
        <li>Useful for imputation, simulation, few-shot tasks</li>
        <li>Examples: GPT, diffusion models, variational autoencoders</li>
      </ul>
    </div>
    <div class="panel panel-warning panel-emphasis p-3 space-y-2">
      <h5 class="font-semibold">&#x1F7E0; Hybrid patterns</h5>
      <ul class="list-disc ml-4 text-sm space-y-1">
        <li>Discriminative heads on generative backbones</li>
        <li>Prompting LLMs for classification</li>
        <li>Energy-based and classifier-guided generation</li>
      </ul>
    </div>
  </div>

  <div class="panel panel-warning p-4 space-y-2">
    <h4 class="font-semibold">&#x1F3AF; Why this matters</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li>Choose the right tool: label vs. synthesize vs. reason.</li>
      <li>Understand cost/latency: discriminative is often lighter.</li>
      <li>Generative models enable data augmentation and interactive agents.</li>
      <li>Many modern systems blend both for best results.</li>
    </ul>
  </div>

  <div class="panel panel-neutral p-4 text-xs">
    <div class="grid md:grid-cols-2 gap-4">
      <div>
        <div class="font-semibold mb-1">Discriminative (logistic)</div>
        <code class="block font-mono">p = sigmoid(dot(w, x) + b)  // estimates P(y=1 | x)</code>
      </div>
      <div>
        <div class="font-semibold mb-1">Generative (LM sampling)</div>
        <code class="block font-mono">for t in 1..T: x[t] ~ softmax(logits(x[:t]) / T)</code>
      </div>
    </div>
  </div>
</div>
