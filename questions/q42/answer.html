<div class="space-y-4">
  <div class="panel panel-info p-3 space-y-2">
    <h4 class="font-semibold">📚 Recommended reading (related)</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><a class="underline" href="#question-05">Question 5: Tokenization &amp; vocabulary shaping</a></li>
      <li><a class="underline" href="#question-06">Question 6: Temperature &amp; decoding</a></li>
      <li><a class="underline" href="#question-39">Question 39: Discriminative vs generative heads</a></li>
    </ul>
  </div>
  <div class="panel panel-info panel-emphasis p-4 space-y-2">
    <h4 class="font-semibold">⚡ Key Idea</h4>
    <p class="text-sm leading-relaxed">Adaptive Softmax splits a large vocabulary into a <strong>frequent head</strong> and one or more <strong>rare tails</strong>. At training time it computes the head for <em>all</em> tokens and only the <em>relevant tail</em> for the gold label. Rare tails use <strong>reduced projection dimensions</strong>, cutting multiply-adds and parameters while preserving accuracy on frequent words.</p>
    <div class="text-xs panel-muted space-y-1">
      <span class="font-semibold">Compute model:</span> batch size (B), hidden dim (d), head size (V_h), tails (i=1..C), tail dims (d_i = α_i d), tail sizes (V_i), batch fractions (p_i):
      <div class="math-display">$$
\begin{gather*}
C_{\text{full}} \propto B d V,\\
C_{\text{adaptive}} \propto B d (V_h + C) + B\sum_i p_i d_i V_i,\\
P_{\text{adaptive}} \approx d V_h + \sum_i d_i V_i + d C.
\end{gather*}
$$</div>
    </div>
  </div>
  <div class="grid md:grid-cols-3 gap-4">
    <div class="panel panel-success panel-emphasis p-3 space-y-2">
      <h5 class="font-medium">🔵 Full Softmax</h5>
      <ul class="list-disc ml-4 text-sm space-y-1">
        <li>Compute over the <strong>entire</strong> vocabulary</li>
        <li>Delivers maximum accuracy but is expensive</li>
      </ul>
    </div>
    <div class="panel panel-accent panel-emphasis p-3 space-y-2">
      <h5 class="font-medium">🟣 Adaptive Softmax</h5>
      <ul class="list-disc ml-4 text-sm space-y-1">
        <li>Head plus a small set of tails per example</li>
        <li>Tail projections shrink: d_i = α_i · d</li>
      </ul>
    </div>
    <div class="panel panel-warning panel-emphasis p-3 space-y-2">
      <h5 class="font-medium">🟠 When to use</h5>
      <ul class="list-disc ml-4 text-sm space-y-1">
        <li>Very large vocabularies (50k–1M)</li>
        <li>Zipfian distributions (few frequent, many rare)</li>
      </ul>
    </div>
  </div>
  <div class="panel panel-warning p-4 space-y-2">
    <h4 class="font-semibold">🎯 Why This Matters</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><strong>Speeds up</strong> training by avoiding full-vocab logits</li>
      <li><strong>Reduces parameters</strong> with low-dim tails</li>
      <li><strong>Keeps accuracy high</strong> for frequent words</li>
      <li>Great for large-vocab or resource-limited settings</li>
    </ul>
  </div>
</div>




