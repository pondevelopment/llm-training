<div class="space-y-4">
  <div class="panel panel-info p-3 space-y-2">
    <h4 class="font-semibold text-heading mb-1">ðŸ“š Recommended reading (related)</h4>
    <ul class="list-disc ml-5 text-sm text-body space-y-1">
      <li><a class="underline" href="#question-41">Question 41: Zero-shot learning &amp; baseline capability</a></li>
      <li><a class="underline" href="#question-13">Question 13: Prompt engineering techniques</a></li>
      <li><a class="underline" href="#question-15">Question 15: Model distillation (contrast with inference-time adaptation)</a></li>
      <li><a class="underline" href="#question-05">Question 5: Tokenization (shot token budgeting)</a></li>
    </ul>
    <p class="small-caption text-muted">How baseline skill, prompt design, and token budgeting shape few-shot lift.</p>
  </div>

  <div class="panel panel-info p-4 space-y-3">
    <h4 class="font-semibold text-heading">ðŸ§  Core Idea</h4>
    <p class="text-sm text-body">Few-shot prompting lets an LLM <strong>adapt at inference time</strong> by embedding a handful of <strong>labeled examples</strong> in context&mdash;pattern learned on the fly, no parameter updates.</p>
    <div class="text-xs text-muted space-y-2">
      <div>Saturating benefit intuition:</div>
      <div class="math-display">$$
      p(K) = 1 - (1 - p_0) e^{-\lambda s c K}
      $$</div>
      <ul class="list-disc ml-5 space-y-1">
        <li><code class="font-mono">p_0</code>: zero-shot baseline</li>
        <li><code class="font-mono">K</code>: example count (shots)</li>
        <li><code class="font-mono">s</code>: similarity (0&ndash;1)</li>
        <li><code class="font-mono">c</code>: CoT multiplier (&ge;1 with reasoning)</li>
        <li><code class="font-mono">\lambda</code>: diminishing rate</li>
      </ul>
    </div>
  </div>

  <div class="grid md:grid-cols-3 gap-4">
    <div class="panel panel-neutral p-3 space-y-2">
      <h5 class="font-medium text-heading">&#x1F7E2; Zero-shot</h5>
      <ul class="text-sm text-body space-y-1">
        <li>No examples, lowest token cost</li>
        <li>Good for broadly aligned tasks</li>
      </ul>
    </div>
    <div class="panel panel-success p-3 space-y-2">
      <h5 class="font-medium text-heading">&#x1F7E3; Few-shot (in-context)</h5>
      <ul class="text-sm text-body space-y-1">
        <li>Add task examples at inference</li>
        <li>Rapid adaptation, no retraining</li>
      </ul>
    </div>
    <div class="panel panel-warning p-3 space-y-2">
      <h5 class="font-medium text-heading">&#x1F7E1; Fine-tuning</h5>
      <ul class="text-sm text-body space-y-1">
        <li>Upfront training effort</li>
        <li>Lowest per-call cost for scale</li>
      </ul>
    </div>
  </div>

  <div class="panel panel-warning p-4 space-y-2">
    <h4 class="font-semibold text-heading">ðŸŽ¯ Why This Matters</h4>
    <ul class="list-disc ml-5 text-sm text-body space-y-1">
      <li><strong>Rapid iteration</strong> without retraining</li>
      <li><strong>Data efficiency</strong> with a handful of shots</li>
      <li><strong>Cost control</strong> via prompt token trade-offs</li>
      <li><strong>Edge cases</strong> and niche domain adaptation</li>
    </ul>
  </div>
</div>
