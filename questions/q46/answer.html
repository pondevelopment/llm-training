<div class="space-y-4">
                <!-- Recommended Reading (canonical) -->
                <div class="bg-indigo-50 p-3 rounded-lg border border-indigo-200">
                    <h4 class="font-semibold text-indigo-900 mb-1">ðŸ“š Recommended reading (related)</h4>
                    <ul class="list-disc ml-5 text-sm text-indigo-800 space-y-1">
                        <li><a class="text-indigo-700 underline hover:text-indigo-900" href="#question-02">Question 2: How does attention work?</a></li>
                        <li><a class="text-indigo-700 underline hover:text-indigo-900" href="#question-05">Question 5: Tokenization & subwords</a></li>
                        <li><a class="text-indigo-700 underline hover:text-indigo-900" href="#question-07">Question 7: Masking & causal generation</a></li>
                        <li><a class="text-indigo-700 underline hover:text-indigo-900" href="#question-21">Question 21: Context windows & limits</a></li>
                        <li><a class="text-indigo-700 underline hover:text-indigo-900" href="#question-15">Question 15: Distillation & architecture choices</a></li>
                    </ul>
                    <p class="mt-2 text-xs text-indigo-700">Attention mechanics, masking, context, and architecture trade-offs.</p>
                </div>

                <!-- Key Idea (accent) -->
                <div class="bg-blue-50 p-4 rounded-lg border-l-4 border-blue-400">
                        <h4 class="font-semibold text-blue-900 mb-2">ðŸ§­ Core Idea</h4>
                        <p class="text-blue-800 text-sm">Encoders form <b>bidirectional contextual embeddings</b>. Decoders generate <b>leftâ€‘toâ€‘right</b> under a causal mask. Encoderâ€“decoder stacks add <b>crossâ€‘attention</b> so each target step can consult the full encoded source + its history.</p>
                        <div class="math-display">$$\text{Attention}(\mathbf{Q},\mathbf{K},\mathbf{V}) = \text{softmax}\!\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\mathbf{V}$$</div>
                        <ul class="mt-3 list-disc ml-5 text-xs text-blue-800 space-y-1">
                            <li><code class="font-mono">L</code>: source length</li>
                            <li><code class="font-mono">t</code>: decode step (1â€¦T)</li>
                            <li><b>Causal mask</b>: hides future positions ( > t )</li>
                            <li><b>Crossâ€‘attention</b>: decoder queries encoder states</li>
                        </ul>
                </div>
        
        <!-- Comparison/Options Grid -->
    <div class="grid md:grid-cols-3 gap-4">
            <div class="bg-green-50 p-3 rounded border-l-4 border-green-400">
                <h5 class="font-medium text-green-900">Encoderâ€‘only (e.g., BERT)</h5>
                <p class="text-sm text-green-700">Bidirectional selfâ€‘attention creates rich token representations for classification, retrieval, etc. No autoregressive generation.</p>
                <code class="text-xs bg-green-100 px-1 rounded">input â†’ contextual embeddings â†’ head (CLS)</code>
            </div>
            <div class="bg-purple-50 p-3 rounded border-l-4 border-purple-400">
                <h5 class="font-medium text-purple-900">Decoderâ€‘only (e.g., GPT)</h5>
                <p class="text-sm text-purple-700">Causal selfâ€‘attention: token t sees only tokens 1â€¦tâˆ’1. Suited for nextâ€‘token generation and longâ€‘form synthesis.</p>
                <span class="text-xs bg-purple-100 px-1 rounded font-mono">( y_t sim p(cdot mid y_{ < t }) )</span>
            </div>
            <div class="bg-orange-50 p-3 rounded border-l-4 border-orange-400">
                <h5 class="font-medium text-orange-900">Encoderâ€“decoder (e.g., T5)</h5>
                <p class="text-sm text-orange-700">Decoder uses masked selfâ€‘attention <em>and</em> crossâ€‘attention to encoder states. Great for sequenceâ€‘toâ€‘sequence tasks like translation and summarization.</p>
                <code class="text-xs bg-orange-100 px-1 rounded">x â†’ encoder â†’ decoder (masked SA + crossâ€‘attn) â†’ y</code>
            </div>
        </div>
        
        <!-- Why This Matters (canonical) -->
        <div class="bg-yellow-50 p-4 rounded-lg">
            <h4 class="font-semibold text-yellow-900 mb-2">ðŸŽ¯ Why This Matters</h4>
            <ul class="text-sm text-yellow-800 space-y-1">
                <li>â€¢ <b>Task fit:</b> classification/embedding â†’ encoder; generation â†’ decoder; seq2seq â†’ encoderâ€“decoder</li>
                <li>â€¢ <b>Masking:</b> causal masks enforce leftâ€‘toâ€‘right; encoders see full context</li>
                <li>â€¢ <b>Flow:</b> crossâ€‘attention fuses source memory + partial target</li>
                <li>â€¢ <b>Tuning:</b> choose architecture + heads that match objectives</li>
            </ul>
        </div>
    </div>
