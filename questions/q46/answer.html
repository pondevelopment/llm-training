<div class="space-y-4">
  <div class="panel panel-info p-3 space-y-2">
    <h4 class="font-semibold text-heading">ðŸ“š Recommended reading (related)</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><a class="underline" href="#question-02">Question 2: How does attention work?</a></li>
      <li><a class="underline" href="#question-05">Question 5: Tokenization &amp; subwords</a></li>
      <li><a class="underline" href="#question-07">Question 7: Masking &amp; causal generation</a></li>
      <li><a class="underline" href="#question-21">Question 21: Context windows &amp; limits</a></li>
      <li><a class="underline" href="#question-15">Question 15: Distillation &amp; architecture choices</a></li>
    </ul>
    <p class="text-xs text-muted">Attention mechanics, masking, context, and architecture trade-offs.</p>
  </div>

  <div class="panel panel-info panel-emphasis p-4 space-y-3">
    <h4 class="font-semibold text-heading">ðŸ§­ Core Idea</h4>
    <p class="text-sm">Encoders form <strong>bidirectional contextual embeddings</strong>. Decoders generate <strong>leftâ€‘toâ€‘right</strong> under a causal mask. Encoderâ€“decoder stacks add <strong>cross-attention</strong> so each target step can consult the full encoded source plus its history.</p>
    <div class="math-display">$$\text{Attention}(\mathbf{Q},\mathbf{K},\mathbf{V}) = \text{softmax}\!\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\mathbf{V}$$</div>
    <ul class="list-disc ml-5 text-xs text-muted space-y-1">
      <li><code class="font-mono">L</code>: source length</li>
      <li><code class="font-mono">t</code>: decode step (1â€¦T)</li>
      <li><strong>Causal mask</strong>: hides future positions (&gt; t)</li>
      <li><strong>Cross-attention</strong>: decoder queries encoder states</li>
    </ul>
  </div>

  <div class="grid md:grid-cols-3 gap-4">
    <div class="panel panel-success panel-emphasis p-3 space-y-2">
      <h5 class="font-medium text-heading">Encoderâ€‘only (e.g., BERT)</h5>
      <p class="text-sm text-muted">Bidirectional self-attention creates rich token representations for classification, retrieval, and embedding tasks. No autoregressive generation.</p>
      <span class="chip chip-success text-xs font-mono">input â†’ contextual embeddings â†’ head (CLS)</span>
    </div>
    <div class="panel panel-accent panel-emphasis p-3 space-y-2">
      <h5 class="font-medium text-heading">Decoderâ€‘only (e.g., GPT)</h5>
      <p class="text-sm text-muted">Causal self-attention: token t sees only tokens 1â€¦tâˆ’1. Suited for next-token generation and long-form synthesis.</p>
      <span class="chip chip-accent text-xs font-mono">p(y<sub>t</sub> | y<sub>&lt; t</sub>)</span>
    </div>
    <div class="panel panel-warning panel-emphasis p-3 space-y-2">
      <h5 class="font-medium text-heading">Encoderâ€“decoder (e.g., T5)</h5>
      <p class="text-sm text-muted">Decoder uses masked self-attention <em>and</em> cross-attention to encoder states. Ideal for sequence-to-sequence tasks like translation and summarization.</p>
      <span class="chip chip-warning text-xs font-mono">x â†’ encoder â†’ decoder â†’ y</span>
    </div>
  </div>

  <div class="panel panel-warning p-4 space-y-1">
    <h4 class="font-semibold text-heading">ðŸŽ¯ Why This Matters</h4>
    <ul class="text-sm space-y-1">
      <li>â€¢ <strong>Task fit:</strong> classification/embedding â†’ encoder; generation â†’ decoder; seq2seq â†’ encoderâ€“decoder</li>
      <li>â€¢ <strong>Masking:</strong> causal masks enforce left-to-right; encoders see full context</li>
      <li>â€¢ <strong>Flow:</strong> cross-attention fuses source memory with partial target</li>
      <li>â€¢ <strong>Tuning:</strong> choose architecture and heads that match objectives</li>
    </ul>
  </div>
</div>
