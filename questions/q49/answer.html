<div class="space-y-4">
  <div class="panel panel-info p-3">
    <h4 class="font-semibold text-heading mb-1">ðŸ“š Recommended reading (related)</h4>
    <ul class="list-disc ml-5 text-sm text-body space-y-1">
      <li><a href="#question-01" class="underline text-link">Question 1: Tokenization & why it matters</a></li>
      <li><a href="#question-02" class="underline text-link">Question 2: Attention mechanisms</a></li>
      <li><a href="#question-18" class="underline text-link">Question 18: Overfitting & mitigation</a></li>
      <li><a href="#question-47" class="underline text-link">Question 47: Context & perplexity</a></li>
      <li><a href="#question-48" class="underline text-link">Question 48: Hyperparameters</a></li>
    </ul>
  </div>

  <div class="panel panel-info panel-emphasis p-4 space-y-2">
    <h4 class="font-semibold text-heading">ðŸ§­ Core idea</h4>
    <p class="text-body">Large language models are transformer-based generative models trained on vast corpora to predict the next token. Scale across parameters, data, compute, and carefully chosen objectives unlocks emergent capabilities such as in-context learning and broad task generalisation.</p>
    <div class="math-display">$$\max_{\theta} \sum_t \log p_{\theta}(y_t \mid y_{< t})$$</div>
  </div>

  <div class="grid md:grid-cols-3 gap-4">
    <div class="panel panel-success panel-emphasis p-3 space-y-2">
      <h5 class="font-medium text-heading">Architecture</h5>
      <ul class="list-disc pl-5 text-sm text-body space-y-1">
        <li>Transformer decoder stacks with self-attention</li>
        <li>Decoder-only or encoderâ€“decoder variants</li>
        <li>Rotary/positional encodings and MLP blocks</li>
      </ul>
    </div>
    <div class="panel panel-accent panel-emphasis p-3 space-y-2">
      <h5 class="font-medium text-heading">Scale</h5>
      <ul class="list-disc pl-5 text-sm text-body space-y-1">
        <li>Parameters spanning millions â†’ trillions</li>
        <li>Diverse pre-training data (web, code, books)</li>
        <li>Extended training runs with massive compute</li>
      </ul>
    </div>
    <div class="panel panel-warning panel-emphasis p-3 space-y-2">
      <h5 class="font-medium text-heading">Capabilities</h5>
      <ul class="list-disc pl-5 text-sm text-body space-y-1">
        <li>Generation, reasoning, and coding</li>
        <li>In-context learning and tool use</li>
        <li>Multimodality for certain model families</li>
      </ul>
    </div>
  </div>

  <div class="panel panel-warning p-4 space-y-2">
    <h4 class="font-semibold text-heading">ðŸŽ¯ Why this matters</h4>
    <ul class="list-disc ml-5 text-sm text-body space-y-1">
      <li><strong>Roadmapping:</strong> Balancing parameter, data, and compute growth sets realistic budgets.</li>
      <li><strong>Performance:</strong> Larger models and longer context windows deliver higher qualityâ€”up to scaling-law limits.</li>
      <li><strong>Safety:</strong> Alignment treatments such as instruction tuning and RLHF are essential for deployment.</li>
      <li><strong>Cost & latency:</strong> Bigger is not always better; inference and fine-tuning costs rise superlinearly with size.</li>
    </ul>
  </div>
</div>
