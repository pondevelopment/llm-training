<div class="space-y-4">
  <div class="panel panel-info p-3">
    <h4 class="font-semibold mb-1">ðŸ“š Recommended reading</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><a class="underline" href="#question-07">Question 7: What are embeddings?</a></li>
      <li><a class="underline" href="#question-12">Question 12: How do LLMs scale with parameters?</a></li>
      <li><a class="underline" href="#question-24">Question 24: What is parameter-efficient fine-tuning?</a></li>
      <li><a class="underline" href="#question-17">Question 17: RLHF &amp; preference optimization</a></li>
      <li><a class="underline" href="#question-45">Question 45: Reducing bias &amp; hallucinations</a></li>
    </ul>
    <p class="small-caption panel-muted mt-2">Lifecycle drill-down: embeddings, scaling laws, adaptation efficiency, alignment, and mitigation.</p>
  </div>

  <div class="panel panel-info p-4 space-y-3">
    <h4 class="font-semibold">ðŸ§  What is a Large Language Model (LLM)?</h4>
    <p>An LLM is a transformer-based neural network with billions of parameters trained (self-supervised) to predict the next token given prior context. Scale across data, parameters, and compute lets it internalize the statistical structure of language and generalize via prompting or light adaptation.</p>
    <div class="math-display">$$\begin{aligned}
      \text{Objective:}\quad & \max_{\theta} \; \mathbb{E}_{(x_1,\dots,x_T)\sim \mathcal{D}} \sum_{t=1}^{T} \log p_{\theta}(x_t \mid x_{\le t}) \\
      \text{Tokenization:}\quad & x_t = \text{BPE}(\text{text}) \\
      \text{Context Window:}\quad & (x_1, \dots, x_T),\; T \leq L_{\text{max}} \\
      \text{Inference:}\quad & x_{t+1} \sim p_{\theta}(\cdot \mid x_{\leq t})
    \end{aligned}$$</div>
  </div>

  <div class="grid md:grid-cols-3 gap-4">
    <div class="panel panel-success p-3">
      <h5 class="font-medium">1. Pre-training</h5>
      <p class="text-sm panel-muted">Predict next tokens over massive, diverse corpora (code, web, books) using self-supervision.</p>
      <span class="chip chip-success text-xs">Trillions of tokens â†’ broad priors</span>
    </div>
    <div class="panel panel-accent p-3">
      <h5 class="font-medium">2. Adaptation</h5>
      <p class="text-sm panel-muted">Domain or task specialization via SFT or PEFT (LoRA, adapters, QLoRA) on curated data.</p>
      <span class="chip chip-accent text-xs">Base â†’ domain tuned</span>
    </div>
    <div class="panel panel-warning p-3">
      <h5 class="font-medium">3. Alignment</h5>
      <p class="text-sm panel-muted">Human feedback stagesâ€”preference data, reward models, RLHF/DPOâ€”shape helpful and safe responses.</p>
      <span class="chip chip-warning text-xs">SFT â†’ RLHF / DPO</span>
    </div>
  </div>

  <div class="grid md:grid-cols-3 gap-4">
    <div class="panel panel-success p-3">
      <h5 class="font-medium">4. Optimization</h5>
      <p class="text-sm panel-muted">Quantization, pruning, KV-cache, and speculative decoding cut latency and cost.</p>
      <span class="chip chip-success text-xs">INT4 / sparsity / caching</span>
    </div>
    <div class="panel panel-info p-3">
      <h5 class="font-medium">5. Guardrails</h5>
      <p class="text-sm panel-muted">Safety filters, retrieval grounding, policy checks, and red-teaming reduce harmful or hallucinated outputs.</p>
      <span class="chip chip-info text-xs">RAG + policy filters</span>
    </div>
    <div class="panel panel-neutral p-3">
      <h5 class="font-medium">6. Monitoring</h5>
      <p class="text-sm panel-muted">Usage, drift, bias, latency, and cost metrics feed continuous evaluation loops.</p>
      <span class="chip chip-neutral text-xs">Eval + telemetry</span>
    </div>
  </div>

  <div class="panel panel-warning p-4 space-y-2">
    <h4 class="font-semibold">ðŸŽ¯ Why this matters</h4>
    <ul class="list-disc ml-5 text-sm space-y-1">
      <li><strong>Holistic view:</strong> Consolidates lifecycle stages into one mental model.</li>
      <li><strong>Cost focus:</strong> Most compute sits in pre-training; adaptation and alignment multiply value.</li>
      <li><strong>Risk control:</strong> Guardrails and monitoring mitigate drift, bias, and hallucination.</li>
      <li><strong>Leverage:</strong> Efficient adapters and quantization let teams reuse frontier bases.</li>
    </ul>
  </div>
</div>